<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Manual de Estadística | Aprende con Alf</title>
    <link>/docencia/estadistica/manual/</link>
      <atom:link href="/docencia/estadistica/manual/index.xml" rel="self" type="application/rss+xml" />
    <description>Manual de Estadística</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>es-es</language>
    <image>
      <url>/images/logo_hude38443eeb2faa5fa84365aba7d86a77_3514_300x300_fit_lanczos_3.png</url>
      <title>Manual de Estadística</title>
      <link>/docencia/estadistica/manual/</link>
    </image>
    
    <item>
      <title>Introducción</title>
      <link>/docencia/estadistica/manual/introduccion/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/docencia/estadistica/manual/introduccion/</guid>
      <description>&lt;h2 id=&#34;la-estadística-como-herramienta-científica&#34;&gt;La estadística como herramienta científica&lt;/h2&gt;
&lt;h3 id=&#34;qué-es-la-estadística&#34;&gt;¿Qué es la estadística?&lt;/h3&gt;
&lt;div class=&#34;alert alert-def&#34;&gt;
  &lt;div&gt;
    &lt;strong&gt;Definición - Estadística&lt;/strong&gt;. La &lt;em&gt;estadística&lt;/em&gt; es una rama de las matemáticas que se encarga de la recogida, análisis e interpretación de datos.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;El papel de la Estadística es extraer información de los datos para adquirir el conocimiento necesario para tomar decisiones.&lt;/p&gt;
&lt;img src=&#34;../img/introduccion/proposito_estadistica.svg&#34; alt=&#34;Propósito de la Estadística&#34; width=&#34;600px&#34;&gt;
&lt;p&gt;La estadística es imprescindible en cualquier disciplina científica o técnica donde se manejen datos, especialmente si son grandes volúmenes de datos, como por ejemplo en Física, Química, Medicina, Psicología, Economía o Ciencias Sociales.&lt;/p&gt;
&lt;p&gt;Pero, &lt;em&gt;¿por qué es necesaria la Estadística?&lt;/em&gt;&lt;/p&gt;
&lt;h3 id=&#34;la-variabilidad-de-nuestro-mundo&#34;&gt;La variabilidad de nuestro mundo&lt;/h3&gt;
&lt;p&gt;El científico trata de estudiar el mundo que le rodea; un mundo que está lleno de variaciones que dificultan la determinación del comportamiento de las cosas.&lt;/p&gt;
&lt;p&gt;La estadística actúa como disciplina puente entre la realidad del mundo y los modelos matemáticos que tratan de explicarla, proporcionando una metodología para evaluar las discrepancias entre la realidad y los modelos teóricos.&lt;/p&gt;
&lt;p&gt;Esto la convierte en una herramienta indispensable en las ciencias aplicadas que requieran el análisis de datos y el diseño de experimentos.&lt;/p&gt;
&lt;h2 id=&#34;población-y-muestra&#34;&gt;Población y muestra&lt;/h2&gt;
&lt;h3 id=&#34;población-estadística&#34;&gt;Población estadística&lt;/h3&gt;
&lt;div class=&#34;alert alert-def&#34;&gt;
  &lt;div&gt;
    &lt;strong&gt;Definición - Población&lt;/strong&gt;. Una &lt;em&gt;población&lt;/em&gt; es un conjunto de elementos definido por una o más características que tienen todos los elementos, y sólo ellos. Cada elemento de la población se llama &lt;em&gt;individuo&lt;/em&gt;.
  &lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;alert alert-def&#34;&gt;
  &lt;div&gt;
    &lt;strong&gt;Definición - Tamaño poblacional&lt;/strong&gt;. El número de individuos de una población se conoce como &lt;em&gt;tamaño poblacional&lt;/em&gt; y se representa como $N$.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Ejemplo&lt;/strong&gt;. En unas elecciones generales a la presidencia del gobierno, la población serían todos los individuos del estado con derecho a voto. En el estudio de una enfermedad, la población sería todas las personas que tienen la enfermedad. Y en un proceso de control de calidad en la fabricación de un fármaco, la población estaría formada por todos los fármacos que se producen en la fábrica.&lt;/p&gt;
&lt;p&gt;A veces, no todos los elementos de la población están accesibles para su estudio. Entonces se distingue entre:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Población Teórica&lt;/strong&gt;: Conjunto de elementos a los que se quiere extrapolar los resultados del estudio.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Población Estudiada&lt;/strong&gt;: Conjunto de elementos realmente accesibles en el estudio.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Ejemplo&lt;/strong&gt;. En el caso del estudio de una enfermedad, la población teórica sería todas las personas que contraigan la enfermedad, incluso si aún no han nacido, mientras que la población estudiada se limitaría al número de personas enfermas que realmente podemos estudiar (obsérvese que incluso quedarían fuera las personas enfermas pero de las que no podemos conseguir información).&lt;/p&gt;
&lt;h3 id=&#34;inconvenientes-en-el-estudio-de-la-población&#34;&gt;Inconvenientes en el estudio de la población&lt;/h3&gt;
&lt;p&gt;El científico estudia un determinado fenómeno en una población para comprenderlo, obtener conocimiento sobre el mismo, y así poder controlarlo. Pero, para tener un conocimiento completo de la población &lt;em&gt;es necesario estudiar todos los individuos de la misma&lt;/em&gt;. Sin embargo, esto no siempre es posible por distintos motivos:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;El tamaño de la población es infinito, o bien es finito pero demasiado grande.&lt;/li&gt;
&lt;li&gt;Las pruebas a que se someten los individuos son destructivas.&lt;/li&gt;
&lt;li&gt;El coste, tanto de dinero como de tiempo, que supondría estudiar a todos los individuos es excesivo.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;muestra-estadística&#34;&gt;Muestra estadística&lt;/h3&gt;
&lt;p&gt;Cuando no es posible o conveniente estudiar todos los individuos de la población, se estudia sólo una parte de la misma.&lt;/p&gt;
&lt;div class=&#34;alert alert-def&#34;&gt;
  &lt;div&gt;
    &lt;strong&gt;Definición - Muestra&lt;/strong&gt;. Una &lt;em&gt;muestra&lt;/em&gt; es un subconjunto de la población.
  &lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;alert alert-def&#34;&gt;
  &lt;div&gt;
    &lt;strong&gt;Definición - Tamaño muestral&lt;/strong&gt;. Al número de individuos que componen la muestra se le llama &lt;em&gt;tamaño muestral&lt;/em&gt; y se representa por $n$.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Habitualmente, el estudio de una población se realiza a partir de muestras extraídas de dicha población.&lt;/p&gt;
&lt;p&gt;Generalmente, el estudio de la muestra sólo aporta conocimiento aproximado de la población. Pero en muchos casos es &lt;em&gt;suficiente&lt;/em&gt;.&lt;/p&gt;
&lt;h3 id=&#34;determinación-del-tamaño-muestral&#34;&gt;Determinación del tamaño muestral&lt;/h3&gt;
&lt;p&gt;Una de las preguntas más interesantes que surge inmediatamente es: &lt;em&gt;¿cuántos individuos es necesario tomar en la muestra para tener un conocimiento aproximado pero suficiente de la población?&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;La respuesta depende de varios factores, como la variabilidad de la población o la fiabilidad deseada para las extrapolaciones que se hagan hacia la población.&lt;/p&gt;
&lt;p&gt;Por desgracia no se podrá responder hasta casi el final del curso, pero en general, cuantos más individuos haya en la muestra, más fiables serán las conclusiones sobre la población, pero también será más lento y costoso el estudio.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Ejemplo&lt;/strong&gt;. Para entender a qué nos referimos cuando hablamos de un tamaño muestral suficiente para comprender lo que ocurre en la población, podemos utilizar el siguiente símil en que se trata de comprender el motivo que representa una fotografía.&lt;/p&gt;
&lt;p&gt;Una fotografía digital está formada por multitud de pequeños puntitos llamados pixels que se dispone en una enorme tabla de filas y columnas (cuantas más filas y columnas haya se habla de que la foto tiene más resolución). Aquí la población estaría formada por todos y cada uno de los píxeles que forman la foto. Por otro lado cada pixel tiene un color y es la variedad de colores a lo largo de los pixels la que permite formar la imagen de la fotografía.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;¿Cuántos píxeles debemos tomar en una muestra para averiguar la imagen de la foto?&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;La respuesta depende de la variabilidad de colores en la foto. Si todos los pixels de la foto son del mismo color, entonces un sólo pixel basta para desvelar la imagen. Pero, si la foto tiene mucha variabilidad de colores, necesitaremos muchos más pixels en la muestra para descubrir el motivo de la foto.&lt;/p&gt;
&lt;p&gt;La imagen siguiente contiene una muestra pequeña de píxeles de una foto. ¿Puedes averiguar el motivo de a foto?&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;../img/introduccion/muestra_molinos1.jpg&#34; alt=&#34;Muestra pequeña de píxeles de una foto.&#34; title=&#34;Muestra pequeña de pixels de una foto&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;¡Con una muestra pequeña es difícil averiguar el contenido de la imagen!&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Seguramente no has podido averiguar el motivo de la fotografía, porque en este caso el número de píxeles que hemos tomado en la muestra es insuficiente para comprender toda la variabilidad de colores que hay en la foto.&lt;/p&gt;
&lt;p&gt;La siguiente imagen contiene una muestra mayor de píxeles. ¿Eres capaz de adivinar el motivo de la foto ahora?&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;../img/introduccion/muestra_molinos2.jpg&#34; alt=&#34;Muestra mayor de píxeles de una foto.&#34; title=&#34;Muestra mayor de pixels de una foto.&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;¡Con una muestra mayor es posible desvelar el motivo de la foto!&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Y aquí está la población completa.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;../img/introduccion/muestra_molinos3.jpg&#34; alt=&#34;Población de píxeles de una foto.&#34; title=&#34;Población de pixels de una foto.&#34;&gt;&lt;/p&gt;
&lt;p&gt;Lo importante es que &lt;em&gt;¡No es necesario conocer todos los píxeles para averiguar la imagen!&lt;/em&gt;&lt;/p&gt;
&lt;h3 id=&#34;tipos-de-razonamiento&#34;&gt;Tipos de razonamiento&lt;/h3&gt;
&lt;p&gt;Así pues, habitualmente realizaremos el estudio de la población a partir de muestras y luego trataremos de extrapolar lo observado en la muestra al resto de la población. A este tipo de razonamiento que saca conclusiones desde la muestra hacia la población se le conoce como &lt;em&gt;razonamiento inductivo&lt;/em&gt;.&lt;/p&gt;
&lt;img src=&#34;../img/introduccion/tipos_razonamiento.svg&#34; alt=&#34;Tipos de razonamiento&#34; width=&#34;400px&#34;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Características de la deducción&lt;/em&gt;: Si las premisas son ciertas, garantiza la certeza de las conclusiones (es decir, si algo se cumple en la población, también se cumple en la muestra). Sin embargo, &lt;em&gt;¡no aporta conocimiento nuevo!&lt;/em&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Características de la inducción&lt;/em&gt;: No garantiza la certeza de las conclusiones (si algo se cumple en la muestra, puede que no se cumpla en la población, así que ¡cuidado con las extrapolaciones!), pero &lt;em&gt;¡es la única forma de generar conocimiento nuevo!&lt;/em&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;La estadística se apoya fundamentalmente en el razonamiento inductivo ya que utiliza la información obtenida a partir de muestras para sacar conclusiones sobre las poblaciones. A diferencia del razonamiento deductivo que va de lo general a lo particular, o en nuestro caso de la población a la muestra, el razonamiento inductivo no garantiza la certeza de las conclusiones, por lo que debemos ser cuidadosos a la hora de generalizar sobre la población lo observado en al muestra, ya que si la muestra no es representativa de la población o contiene sesgos, las conclusiones pueden ser erróneas.&lt;/p&gt;
&lt;h2 id=&#34;muestreo&#34;&gt;Muestreo&lt;/h2&gt;
&lt;div class=&#34;alert alert-def&#34;&gt;
  &lt;div&gt;
    &lt;strong&gt;Definición - Muestreo&lt;/strong&gt;. El proceso de selección de los elementos que compondrán una muestra se conoce como &lt;em&gt;muestreo&lt;/em&gt;.
  &lt;/div&gt;
&lt;/div&gt;
&lt;img src=&#34;../img/introduccion/muestreo.svg&#34; alt=&#34;Muestreo&#34; width=&#34;500px&#34;&gt;
&lt;p&gt;Para que una muestra refleje información fidedigna sobre la población global debe ser representativa de la misma, lo que significa que debe reproducir a pequeña escala la variabilidad de la población.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;El objetivo es obtener una muestra representativa de la población.&lt;/em&gt;&lt;/p&gt;
&lt;h3 id=&#34;modalidades-de-muestreo&#34;&gt;Modalidades de muestreo&lt;/h3&gt;
&lt;p&gt;Existen muchas técnicas de muestreo pero se pueden agrupar en dos categorías:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Muestreo Aleatorio&lt;/strong&gt;: Elección aleatoria de los individuos de la muestra. Todos tienen la misma probabilidad de ser elegidos (&lt;em&gt;equiprobabilidad&lt;/em&gt;).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Muestreo No Aleatorio&lt;/strong&gt;: Los individuos se eligen de forma no aleatoria. Algunos individuos tienen más probabilidad de ser seleccionados que otros.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Sólo las técnicas aleatorias evitan el sesgo de selección, y por tanto, garantizan la representatividad de la muestra extraída, y en consecuencia la validez de las conclusiones.&lt;/p&gt;
&lt;p&gt;Las técnicas no aleatorias no sirven para hacer generalizaciones, ya que no garantizan la representatividad de la muestra. Sin embargo, son menos costosas y pueden utilizarse en estudios exploratorios.&lt;/p&gt;
&lt;h3 id=&#34;muestreo-aleatorio-simple&#34;&gt;Muestreo aleatorio simple&lt;/h3&gt;
&lt;p&gt;Dentro de las modalidades de muestreo aleatorio, el tipo más conocido es el &lt;em&gt;muestreo aleatorio simple&lt;/em&gt;, caracterizado por:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Todos los individuos de la población tienen la misma probabilidad de ser elegidos para la muestra.&lt;/li&gt;
&lt;li&gt;La selección de individuos es con reemplazamiento, es decir, cada individuo seleccionado es devuelto a la población antes de seleccionar al siguiente (y por tanto no se altera la población de partida).&lt;/li&gt;
&lt;li&gt;Las sucesivas selecciones de un individuo son independientes.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;La única forma de realizar un muestreo aleatorio es asignar un número a cada individuo de la población (&lt;em&gt;censo&lt;/em&gt;) y realizar un sorteo aleatorio.&lt;/p&gt;
&lt;h3 id=&#34;variables-estadísticas&#34;&gt;Variables estadísticas&lt;/h3&gt;
&lt;p&gt;Todo estudio estadístico comienza por la identificación de las características que interesa estudiar en la población y que se medirán en los individuos de la muestra.&lt;/p&gt;
&lt;div class=&#34;alert alert-def&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;&lt;strong&gt;Definición - Variable estadística&lt;/strong&gt;. Una &lt;em&gt;variable estadística&lt;/em&gt; es una propiedad o característica medida en los individuos de la población.&lt;/p&gt;
&lt;p&gt;Los &lt;em&gt;datos&lt;/em&gt; son los valores observados en las variables estadísticas.&lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;&lt;img src=&#34;../img/introduccion/variables_estadisticas.svg&#34; alt=&#34;&amp;amp;ldquo;Variables estadísticas&amp;amp;rdquo;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Estas características pueden ser de distintos tipos de acuerdo a su naturaleza y su escala:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Variables cualitativas o atributos&lt;/strong&gt;: Miden cualidades no numéricas. Pueden ser:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Nominales&lt;/strong&gt;: No existe un orden entre las categorías.&lt;br&gt;
Ejemplo: El color de pelo o el sexo.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Ordinales&lt;/strong&gt;: Existe un orden entre las categorías.
Ejemplo: El nivel de estudios o la gravedad de una enfermedad.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Variables cuantitativas&lt;/strong&gt;: Miden cantidades numéricas. Pueden ser:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Discretas&lt;/strong&gt;: Toman valores numéricos aislados (habitualmente números enteros).&lt;br&gt;
Ejemplo: El número de hijos o el número de coches en una familia.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Continuas&lt;/strong&gt;: Pueden tomar cualquier valor en un intervalo real.&lt;br&gt;
Ejemplo: El peso o la estatura.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Las variables cualitativas y discretas se conocen también con &lt;em&gt;variables categóricas&lt;/em&gt; y sus valores &lt;em&gt;categorías&lt;/em&gt;.&lt;/p&gt;
&lt;img src=&#34;../img/introduccion/tipos_variables.svg&#34; alt=&#34;Variables estadísticas&#34; width=&#34;800px&#34;&gt;
&lt;h4 id=&#34;elección-del-tipo-de-variable-más-apropiado&#34;&gt;Elección del tipo de variable más apropiado&lt;/h4&gt;
&lt;p&gt;En ocasiones una característica puede medirse mediante variables de distinto tipo.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Ejemplo&lt;/strong&gt;. Si una persona fuma o no podría medirse de diferentes formas:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Fuma: si/no. (Nominal)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Nivel de fumador: No fuma / ocasional / moderado / bastante / empedernido. (Ordinal)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Número de cigarros diarios: 0,1,2,&amp;hellip; (Discreta)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;En estos casos es preferible usar variables cuantitativas a cualitativas. Dentro de las cuantitativas es preferible usar las continuas a las discretas y dentro de las cualitativas es preferible usar ordinales a nominales pues aportan más información.&lt;/p&gt;
&lt;img src=&#34;../img/introduccion/informacion_variables.svg&#34; alt=&#34;Cantidad de información de los tipos de variables estadísticas&#34; width=&#34;600px&#34;&gt;
&lt;p&gt;De acuerdo al papel que juegan en el estudio las variables también pueden clasificarse como:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Variables independientes&lt;/strong&gt;: Variables que supuestamente no dependen de otras variables en el estudio. Habitualmente son las variables manipuladas en el experimento para ver su efecto en las variables dependientes. Se conocen también como &lt;em&gt;variables predictivas&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Variables dependientes&lt;/strong&gt;: Variables que supuestamente dependen de otras variables en el estudio. No son manipuladas en el experimento y también se conocen como &lt;em&gt;variables respuesta&lt;/em&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Ejemplo&lt;/strong&gt;. En un estudio sobre el rendimiento de los alumnos de un curso, la inteligencia de los alumnos y el número de horas de estudio diarias serían variables independientes y la nota del curso sería una variable dependiente.&lt;/p&gt;
&lt;h3 id=&#34;tipos-de-estudios-estadísticos&#34;&gt;Tipos de estudios estadísticos&lt;/h3&gt;
&lt;p&gt;Dependiendo de si se manipulan las variables independientes existen dos tipos de estudios:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Experimentales&lt;/strong&gt;: Cuando las variables independientes son manipuladas para ver el efecto que producen en las variables dependientes.&lt;br&gt;
&lt;strong&gt;Ejemplo&lt;/strong&gt;. En un estudio sobre el rendimiento de los estudiantes en un test, el profesor manipula la metodología de estudio para crear dos o más grupos con metodologías de estudio distintas.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;No experimentales&lt;/strong&gt;: Cuando las variables independientes no son manipuladas. Esto no significa que sea imposible hacerlo, sino que es difícil o poco ético hacerlo.&lt;br&gt;
&lt;strong&gt;Ejemplo&lt;/strong&gt;. En un estudio un investigador puede estar interesado en el efecto de fumar sobre el cáncer de pulmón. Aunque es posible, no sería ético pedirle a los pacientes que fumasen para ver el efecto que tiene sobre sus pulmones. En este caso, el investigador podría estudiar dos grupos de pacientes, uno con cáncer de pulmón y otro sin cáncer, y observar en cada grupo cuántos fuman o no.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Los estudios experimentales permiten identificar causas y efectos entre las variables del estudio, mientras que los no experimentales sólo permiten identificar relaciones de asociación entre las variables.&lt;/p&gt;
&lt;h3 id=&#34;la-tabla-de-datos&#34;&gt;La tabla de datos&lt;/h3&gt;
&lt;p&gt;Las variables a estudiar se medirán en cada uno de los individuos de la muestra, obteniendo un conjunto de datos que suele organizarse en forma de matriz que se conoce como tabla de datos_.&lt;/p&gt;
&lt;p&gt;En esta tabla cada columna contiene la información de una variable y cada fila la información de un individuo.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Ejemplo&lt;/strong&gt; La siguiente tabla contiene información de las variables Nombre, Edad, Sexo, Peso y Altura de una muestra de 6 personas.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Nombre&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Edad&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Sexo&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Peso(Kg)&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Altura(cm)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;José Luis Martínez&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;18&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;H&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;85&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;179&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Rosa Díaz&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;32&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;M&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;65&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;173&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Javier García&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;24&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;H&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;71&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;181&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Carmen López&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;35&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;M&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;65&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;170&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Marisa López&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;46&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;M&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;51&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;158&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Antonio Ruiz&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;68&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;H&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;66&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;174&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;fases-del-análisis-estadístico&#34;&gt;Fases del análisis estadístico&lt;/h3&gt;
&lt;p&gt;Normalmente un estudio estadístico pasa por las siguientes etapas:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;El estudio comienza por el diseño previo del mismo en el que se establezcan los objetivos del mismo, la población, las variables que se medirán y el tamaño muestral requerido.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A continuación se seleccionará una muestra representativa del tamaño establecido y se medirán las variables en los individuos de la muestra obteniendo la tabla de datos. De esto se encarga el &lt;em&gt;Muestreo&lt;/em&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;El siguiente paso consiste en describir y resumir la información que contiene la muestra. De esto se encarga la &lt;em&gt;Estadística Descriptiva&lt;/em&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;La información obtenida es proyectada sobre un modelo matemático que intenta explicar el comportamiento de la población y el modelo se valida. De todo esto se encarga la &lt;em&gt;Estadística Inferencial&lt;/em&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Finalmente, el modelo validado nos permite hacer predicciones y sacar conclusiones sobre la población de partida con cierta confianza.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;el-ciclo-estadístico&#34;&gt;El ciclo estadístico&lt;/h4&gt;
&lt;img src=&#34;../img/introduccion/ciclo_estadistico.svg&#34; alt=&#34;El ciclo estadístico&#34; width=&#34;600px&#34;&gt;
</description>
    </item>
    
    <item>
      <title>Estadística Descriptiva</title>
      <link>/docencia/estadistica/manual/estadistica-descriptiva/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/docencia/estadistica/manual/estadistica-descriptiva/</guid>
      <description>&lt;p&gt;La estadística descriptiva es la parte de la estadística encargada de representar, analizar y resumir la información contenida en la muestra.&lt;/p&gt;
&lt;p&gt;Tras el proceso de muestreo, es la siguiente etapa de todo estudio estadístico y suele consistir en:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Clasificar, agrupar y ordenar los datos de la muestra.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Tabular y representar gráficamente los datos de acuerdo a sus frecuencias.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Calcular medidas que resuman la información que contiene la muestra (&lt;em&gt;estadísticos muestrales&lt;/em&gt;).&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;No tiene poder inferencial por lo que &lt;em&gt;nunca deben sacarse conclusiones sobre la población a partir de las medidas resumen que aporta la Estadística Descriptiva&lt;/em&gt;.&lt;/p&gt;
&lt;h2 id=&#34;distribución-de-frecuencias&#34;&gt;Distribución de frecuencias&lt;/h2&gt;
&lt;p&gt;El estudio de una variable estadística comienza por medir la variable en los individuos de la muestra y clasificar los valores obtenidos.&lt;/p&gt;
&lt;p&gt;Existen dos formas de clasificar estos valores:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Sin agrupar&lt;/strong&gt;: Ordenar todos los valores obtenidos en la muestra de menor a mayor. Se utiliza con atributos y variables discretas con pocos valores diferentes.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Agrupados&lt;/strong&gt;: Agrupar los valores en clases (intervalos) y ordenar dichas clases de menor a mayor. Se utiliza con variables continuas y con variables discretas con muchos valores diferentes.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;clasificación-de-la-muestra&#34;&gt;Clasificación de la muestra&lt;/h3&gt;
&lt;p&gt;Consiste colocar juntos los valores iguales y ordenarlos si existe un orden entre ellos.&lt;/p&gt;
&lt;img src=&#34;../img/descriptiva/clasificacion_muestra.png&#34; alt=&#34;Clasificación de la muestra&#34; width=400px&gt;
&lt;h3 id=&#34;recuento-de-frecuencias&#34;&gt;Recuento de frecuencias&lt;/h3&gt;
&lt;img src=&#34;../img/descriptiva/recuento_frecuencias.png&#34; alt=&#34;Recuento de frecuencias&#34; width=400px&gt;
&lt;h2 id=&#34;frecuencias-muestrales&#34;&gt;Frecuencias muestrales&lt;/h2&gt;
&lt;div class=&#34;alert alert-def&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;&lt;strong&gt;Definición - Frecuencias muestrales&lt;/strong&gt;. Dada una muestra de tamaño $n$ de una variable $X$, para cada valor de la variable $x_i$ observado en la muestra, se define&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Frecuencia Absoluta $n_i$&lt;/strong&gt;: Es el número de veces que el valor $x_i$ aparece en la muestra.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Frecuencia Relativa $f_i$&lt;/strong&gt;: Es la proporción de veces que el valor $x_i$ aparece en la muestra.
$$f_i = \frac{n_i}{n}$$&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Frecuencia Absoluta Acumulada $N_i$&lt;/strong&gt;: Es el número de valores en la muestra menores o iguales que $x_i$.
$$N_i = n_1 + \cdots + n_i = N_{i-1}+n_i$$&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Frecuencia Relativa Acumulada $F_i$&lt;/strong&gt;: Es la proporción de valores en la muestra menores o iguales que $x_i$.
$$F_i = \frac{N_i}{n}$$&lt;/li&gt;
&lt;/ul&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;h3 id=&#34;tabla-de-frecuencias&#34;&gt;Tabla de frecuencias&lt;/h3&gt;
&lt;p&gt;Al conjunto de valores observados en la muestra junto a sus respectivas frecuencias se le denomina &lt;strong&gt;distribución de frecuencias&lt;/strong&gt; y suele representarse mediante una &lt;strong&gt;tabla de frecuencias&lt;/strong&gt;.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Valores de $X$&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Frecuencia Absoluta&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Frecuencia Relativa&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Frecuencia Absoluta Acumulada&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Frecuencia Relativa Acumulada&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$x_1$&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$n_1$&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$f_1$&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$N_1$&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$F_1$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$\vdots$&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$\vdots$&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$\vdots$&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$\vdots$&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$\vdots$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$x_i$&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$n_i$&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$f_i$&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$N_i$&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$F_i$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$\vdots$&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$\vdots$&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$\vdots$&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$\vdots$&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$\vdots$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$x_k$&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$n_k$&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$f_k$&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$N_k$&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$F_k$&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;Ejemplo - Variable cuantitativa y datos no agrupados&lt;/strong&gt;. El número de hijos en 25 familias es:&lt;/p&gt;
&lt;div style=&#34;text-align:center&#34;&gt;
1, 2, 4, 2, 2, 2, 3, 2, 1, 1, 0, 2, 2, 0, 2, 2, 1, 2, 2, 3, 1, 2, 2, 1, 2
&lt;/div&gt;
&lt;p&gt;La tabla de frecuencias del número de hijos en esta muestra es&lt;/p&gt;
&lt;p&gt;$$ \begin{array}{rrrrr}
\hline
x_i &amp;amp; n_i &amp;amp; f_i &amp;amp; N_i &amp;amp; F_i\newline
\hline
0 &amp;amp; 2 &amp;amp; 0.08 &amp;amp; 2 &amp;amp; 0.08\newline
1 &amp;amp; 6 &amp;amp; 0.24 &amp;amp; 8 &amp;amp; 0.32\newline
2 &amp;amp; 14 &amp;amp; 0.56 &amp;amp; 22 &amp;amp; 0.88\newline
3 &amp;amp; 2 &amp;amp; 0.08 &amp;amp; 24 &amp;amp; 0.96\newline
4 &amp;amp; 1 &amp;amp; 0.04 &amp;amp; 25 &amp;amp; 1 \newline
\hline
\sum &amp;amp; 25 &amp;amp; 1 \newline
\hline
\end{array}
$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Ejemplo - Variable cuantitativa y datos agrupados&lt;/strong&gt;. Se ha medido la estatura (en cm) de 30 universitarios obteniendo:&lt;/p&gt;
&lt;div style=&#34;text-align:center&#34;&gt;
179, 173, 181, 170, 158, 174, 172, 166, 194, 185,&lt;br&gt;
162, 187, 198, 177, 178, 165, 154, 188, 166, 171,&lt;br&gt;
175, 182, 167, 169, 172, 186, 172, 176, 168, 187.
&lt;/div&gt;
&lt;p&gt;La tabla de frecuencias de la estatura en a esta muestra es&lt;/p&gt;
&lt;p&gt;$$ \begin{array}{crrrr}
\hline
x_i &amp;amp; n_i &amp;amp; f_i &amp;amp; N_i &amp;amp; F_i\newline
\hline
(150,160] &amp;amp; 2 &amp;amp; 0.07 &amp;amp; 2 &amp;amp; 0.07\newline
(160,170] &amp;amp; 8 &amp;amp; 0.27 &amp;amp; 10 &amp;amp; 0.34\newline
(170,180] &amp;amp; 11 &amp;amp; 0.36 &amp;amp; 21 &amp;amp; 0.70\newline
(180,190] &amp;amp; 7 &amp;amp; 0.23 &amp;amp; 28 &amp;amp; 0.93\newline
(190,200] &amp;amp; 2 &amp;amp; 0.07 &amp;amp; 30 &amp;amp; 1 \newline
\hline
\sum &amp;amp; 30 &amp;amp; 1 \newline
\hline
\end{array}
$$&lt;/p&gt;
&lt;h3 id=&#34;construcción-de-clases&#34;&gt;Construcción de clases&lt;/h3&gt;
&lt;p&gt;Cada intervalo de agrupación de datos se denomina &lt;strong&gt;clase&lt;/strong&gt; y el centro del intervalo se llama &lt;strong&gt;marca de clase&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;A la hora de agrupar los datos en clases hay que tener en cuenta lo siguiente:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;El número de intervalos no debe ser muy grande ni muy pequeño. Una regla orientativa es tomar un número de intervalos próximo a $\sqrt{n}$ o $\log_2(n)$.&lt;/li&gt;
&lt;li&gt;Los intervalos no deben solaparse y deben cubrir todo el rango de valores. Es indiferente si se abren por la izquierda y se cierran por la derecha o al revés.&lt;/li&gt;
&lt;li&gt;El valor más pequeño debe caer dentro del primer intervalo y el más grande dentro del último.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Ejemplo - Variable cualitativa&lt;/strong&gt;. Los grupos sanguíneos de una muestra de 30 personas son:&lt;/p&gt;
&lt;div style=&#34;text-align:center&#34;&gt;
A, B, B, A, AB, 0, 0, A, B, B, A, A, A, A, AB, A, A, A, B, 0, B, B, B, A, A, A, 0, A, AB, 0.
&lt;/div&gt;
&lt;p&gt;La tabla de frecuencias del grupo sanguíneo en esta muestra es&lt;/p&gt;
&lt;p&gt;$$
\begin{array}{crr}
\hline
x_i &amp;amp; n_i &amp;amp; f_i \newline
\hline
\mbox{0} &amp;amp; 5 &amp;amp; 0.16 \newline
\mbox{A} &amp;amp; 14 &amp;amp; 0.47 \newline
\mbox{B} &amp;amp; 8 &amp;amp; 0.27 \newline
\mbox{AB} &amp;amp; 3 &amp;amp; 0.10 \newline
\hline
\sum &amp;amp; 30 &amp;amp; 1 \newline
\hline
\end{array}
$$&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Obsérvese que en este caso las frecuencias acumuladas no tienen sentido al no existir un orden entre los valores de la variable.&lt;/em&gt;&lt;/p&gt;
&lt;h2 id=&#34;representaciones-gráficas&#34;&gt;Representaciones gráficas&lt;/h2&gt;
&lt;p&gt;La tabla de frecuencias también suele representarse gráficamente. Dependiendo del tipo de variable y de si se han agrupado o no los datos, se utilizan distintos tipos de gráficos:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Diagrama de barras&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Histograma&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Diagrama de líneas o polígonos.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Diagrama de sectores.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;diagrama-de-barras&#34;&gt;Diagrama de barras&lt;/h3&gt;
&lt;p&gt;Un &lt;strong&gt;diagrama de barras&lt;/strong&gt; consiste en un conjunto de barras, una para cada valor o categoría de la variable, dibujadas sobre unos ejes cartesianos.&lt;/p&gt;
&lt;p&gt;Habitualmente los valores o categorías de la variable se representan en eje $X$, y las frecuencias en el eje $Y$.
Para cada valor o categoría se dibuja una barra con la altura correspondiente a su frecuencia. La anchura de la barra no es importante pero las barras deben aparecer claramente separadas unas de otras.&lt;/p&gt;
&lt;p&gt;Dependiendo del tipo de frecuencia representada en el eje $Y$ se tienen diferentes tipos de diagramas de barras.&lt;/p&gt;
&lt;p&gt;En ocasiones se dibuja un polígono, conocido como &lt;strong&gt;polígono de frecuencias&lt;/strong&gt;, uniendo mediante segmentos los puntos más altos de cada barra.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Ejemplo&lt;/strong&gt;. El diagrama de barras que aparece a continuación muestra la distribución de frecuencias absolutas del número de hijos en la muestra anterior.&lt;/p&gt;
&lt;div id=&#34;absolute-barchart&#34; class=&#34;plotly&#34; style=&#34;margin: 25px auto; width:80%&#34;&gt;
&lt;/div&gt;
&lt;script type=&#34;application/json&#34; data-for=&#34;absolute-barchart&#34;&gt;
{&#34;x&#34;:{&#34;data&#34;:[{&#34;type&#34;:&#34;bar&#34;,&#34;inherit&#34;:true,&#34;x&#34;:[0,1,2,3,4],&#34;y&#34;:[2,6,14,2,1], &#34;marker&#34;:{&#34;color&#34;:&#34;rgba(137,211,243,1)&#34;}}],&#34;layout&#34;:{&#34;title&#34;:&#34;Distribución de frecuencias absolutas del número de hijos&#34;,&#34;xaxis&#34;:{&#34;title&#34;:&#34;Número de hijos&#34;},&#34;yaxis&#34;:{&#34;title&#34;:&#34;Frecuencia Absoluta&#34;},&#34;autosize&#34;:false, &#34;width&#34;:600, &#34;height&#34;:400, &#34;bargap&#34;:0.5, &#34;margin&#34;:{&#34;b&#34;:40,&#34;l&#34;:60,&#34;t&#34;:25,&#34;r&#34;:10}},&#34;filename&#34;:&#34;Distribución de frecuencias absolutas del número de hijos&#34;},&#34;evals&#34;:[]}
&lt;/script&gt;
&lt;p&gt;El diagrama de barras que aparece a continuación muestra la distribución de frecuencias relativas del número de hijos en la muestra anterior junto al polígono de frecuencias.&lt;/p&gt;
&lt;div id=&#34;relative-barchart&#34; class=&#34;plotly&#34; style=&#34;margin: 25px auto; width:80%&#34;&gt;
&lt;/div&gt;
&lt;script type=&#34;application/json&#34; data-for=&#34;relative-barchart&#34;&gt;
{&#34;x&#34;:{&#34;data&#34;:[{&#34;type&#34;:&#34;bar&#34;,&#34;inherit&#34;:true,&#34;x&#34;:[0,1,2,3,4],&#34;y&#34;:[0.08,0.24,0.56,0.08,0.04],&#34;name&#34;:&#34;barra&#34;, &#34;marker&#34;:{&#34;color&#34;:&#34;rgba(137,211,243,1)&#34;}},{&#34;type&#34;:&#34;scatter&#34;,&#34;inherit&#34;:true,&#34;x&#34;:[0,1,2,3,4],&#34;y&#34;:[0.08,0.24,0.56,0.08,0.04],&#34;name&#34;:&#34;polígono&#34;, &#34;marker&#34;:{&#34;color&#34;:&#34;rgba(238,50,36,1)&#34;}}],&#34;layout&#34;:{&#34;title&#34;:&#34;Distribución de frecuencias relativas del número de hijos&#34;,&#34;xaxis&#34;:{&#34;title&#34;:&#34;Número de hijos&#34;},&#34;yaxis&#34;:{&#34;title&#34;:&#34;Frecuencia Relativa&#34;},&#34;autosize&#34;:false,&#34;width&#34;:600,&#34;height&#34;:400,&#34;bargap&#34;:0.5,&#34;showlegend&#34;:false,&#34;margin&#34;:{&#34;b&#34;:40,&#34;l&#34;:60,&#34;t&#34;:25,&#34;r&#34;:10}}, &#34;filename&#34;:&#34;Distribución de frecuencias relativas del número de hijos&#34;},&#34;evals&#34;:[]}
&lt;/script&gt;
&lt;p&gt;El diagrama de barras que aparece a continuación muestra la distribución de frecuencias absolutas acumuladas del número de hijos en la muestra anterior.&lt;/p&gt;
&lt;div id=&#34;cumulative-absolute-barchart&#34; class=&#34;plotly&#34; style=&#34;margin: 25px auto; width:80%&#34;&gt;
&lt;/div&gt;
&lt;script type=&#34;application/json&#34; data-for=&#34;cumulative-absolute-barchart&#34;&gt;
{&#34;x&#34;:{&#34;data&#34;:[{&#34;type&#34;:&#34;bar&#34;,&#34;inherit&#34;:true,&#34;x&#34;:[0,1,2,3,4],&#34;y&#34;:[2,8,22,24,25], &#34;marker&#34;:{&#34;color&#34;:&#34;rgba(137,211,243,1)&#34;}}],&#34;layout&#34;:{&#34;title&#34;:&#34;Distribución de frecuencias absolutas acumuladas del número de hijos&#34;,&#34;xaxis&#34;:{&#34;title&#34;:&#34;Número de hijos&#34;},&#34;yaxis&#34;:{&#34;title&#34;:&#34;Frecuencia Absoluta Acumulada&#34;},&#34;autosize&#34;:false, &#34;width&#34;:600, &#34;height&#34;:400,&#34;barsgap&#34;:0.5,&#34;bargap&#34;:0.5,&#34;margin&#34;:{&#34;b&#34;:40,&#34;l&#34;:60,&#34;t&#34;:25,&#34;r&#34;:10}},&#34;filename&#34;:&#34;Distribución de frecuencias absolutas acumuladas del número de hijos&#34;},&#34;evals&#34;:[]}
&lt;/script&gt;
&lt;p&gt;Y el diagrama de barras que aparece a continuación muestra la distribución de frecuencias relativas acumuladas del número de hijos en la muestra anterior junto al polígono de frecuencias.&lt;/p&gt;
&lt;div id=&#34;cumulative-relative-barchart&#34; class=&#34;plotly&#34; style=&#34;margin: 25px auto; width:80%&#34;&gt;
&lt;/div&gt;
&lt;script type=&#34;application/json&#34; data-for=&#34;cumulative-relative-barchart&#34;&gt;
{&#34;x&#34;:{&#34;data&#34;:[{&#34;type&#34;:&#34;bar&#34;,&#34;inherit&#34;:true,&#34;x&#34;:[0,0,1,2,3,4],&#34;y&#34;:[0,0.08,0.32,0.88,0.96,1],&#34;name&#34;:&#34;barra&#34;, &#34;marker&#34;:{&#34;color&#34;:&#34;rgba(137,211,243,1)&#34;}},{&#34;type&#34;:&#34;scatter&#34;,&#34;inherit&#34;:true,&#34;x&#34;:[0,0,1,2,3,4],&#34;y&#34;:[0,0.08,0.32,0.88,0.96,1],&#34;name&#34;:&#34;polígono&#34;,&#34;line&#34;:{&#34;shape&#34;:&#34;hv&#34;}, &#34;marker&#34;:{&#34;color&#34;:&#34;rgba(238,50,36,1)&#34;}}],&#34;layout&#34;:{&#34;title&#34;:&#34;Distribución de frecuencias relativas acumuladas del número de hijos&#34;,&#34;xaxis&#34;:{&#34;title&#34;:&#34;Número de hijos&#34;},&#34;yaxis&#34;:{&#34;title&#34;:&#34;Frecuencia Relativa Acumulada&#34;},&#34;autosize&#34;:false,&#34;width&#34;:600,&#34;height&#34;:400,&#34;bargap&#34;:0.5,&#34;showlegend&#34;:false,&#34;margin&#34;:{&#34;b&#34;:40,&#34;l&#34;:60,&#34;t&#34;:25,&#34;r&#34;:10}}, &#34;filename&#34;:&#34;Distribución de frecuencias relativas acumuladas del número de hijos&#34;},&#34;evals&#34;:[]}
&lt;/script&gt;
&lt;h3 id=&#34;histograma&#34;&gt;Histograma&lt;/h3&gt;
&lt;p&gt;Un &lt;em&gt;histograma&lt;/em&gt; es similar a un diagrama de barras pero para datos agrupados.&lt;/p&gt;
&lt;p&gt;Habitualmente las clases o intervalos de agrupación se representan en el
eje $X$, y las frecuencias en el eje $Y$.
Para cada clase se dibuja una barra de altura la correspondiente
frecuencia. A diferencia del diagrama de barras, la anchura del la barra
coincide con la anchura de las clases y no hay separación entre dos
barras consecutivas.&lt;/p&gt;
&lt;img src=&#34;../img/descriptiva/histogram_creation.gif&#34; alt=&#34;Sample classification&#34; width=&#34;400&#34;&gt;
&lt;p&gt;Dependiendo del tipo de frecuencia representada en el eje $Y$ existen
distintos tipos de histogramas.&lt;/p&gt;
&lt;p&gt;Al igual que con el diagrama de barras, se puede dibujar un &lt;em&gt;polígono de frecuencias&lt;/em&gt; uniendo los puntos centrales más altos de cada barra con segmentos.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Ejemplo&lt;/strong&gt;. El siguiente histograma muestra la distribución de frecuencias absolutas de las estaturas.&lt;/p&gt;
&lt;div id=&#34;absolute-histogram&#34; class=&#34;plotly&#34; style=&#34;margin: auto; width:80%&#34;&gt;&lt;/div&gt;
&lt;script type=&#34;application/json&#34; data-for=&#34;absolute-histogram&#34;&gt;{&#34;x&#34;:{&#34;data&#34;:[{&#34;type&#34;:&#34;histogram&#34;,&#34;inherit&#34;:true,&#34;x&#34;:[179,173,181,169,158,174,172,166,194,185,162,187,198,177,178,165,154,188,166,171,175,182,167,169,172,186,172,176,168,187],&#34;marker&#34;:{&#34;line&#34;:{&#34;width&#34;:1},&#34;color&#34;:&#34;rgba(137,211,243,1)&#34;}}],&#34;layout&#34;:{&#34;title&#34;:&#34;Distribución de frecuencias absolutas de estaturas&#34;,&#34;xaxis&#34;:{&#34;title&#34;:&#34;Estatura&#34;},&#34;yaxis&#34;:{&#34;title&#34;:&#34;Frecuencia absoluta&#34;},&#34;autosize&#34;:false,&#34;width&#34;:600,&#34;height&#34;:400,&#34;margin&#34;:{&#34;b&#34;:40,&#34;l&#34;:60,&#34;t&#34;:25,&#34;r&#34;:10}},&#34;url&#34;:null,&#34;width&#34;:null,&#34;height&#34;:null,&#34;base_url&#34;:&#34;https://plot.ly&#34;,&#34;layout.1&#34;:{&#34;title&#34;:&#34;Distribución de frecuencias absolutas de estaturas&#34;,&#34;xaxis&#34;:{&#34;title&#34;:&#34;Estatura&#34;},&#34;yaxis&#34;:{&#34;title&#34;:&#34;Frecuencia absoluta&#34;},&#34;autosize&#34;:false,&#34;width&#34;:600,&#34;height&#34;:400},&#34;filename&#34;:&#34;Distribución de frecuencias absolutas de estaturas&#34;},&#34;evals&#34;:[]}&lt;/script&gt;
&lt;p&gt;El siguiente histograma muestra la distribución de frecuencias relativas con el polígono de frecuencias.&lt;/p&gt;
&lt;div id=&#34;relative-histogram&#34; class=&#34;plotly&#34; style=&#34;margin: auto; width:80%&#34;&gt;&lt;/div&gt;
&lt;script type=&#34;application/json&#34; data-for=&#34;relative-histogram&#34;&gt;{&#34;x&#34;:{&#34;data&#34;:[{&#34;type&#34;:&#34;histogram&#34;,&#34;inherit&#34;:true,&#34;x&#34;:[179,173,181,169,158,174,172,166,194,185,162,187,198,177,178,165,154,188,166,171,175,182,167,169,172,186,172,176,168,187],&#34;marker&#34;:{&#34;line&#34;:{&#34;width&#34;:1},&#34;color&#34;:&#34;rgba(137,211,243,1)&#34;},&#34;histnorm&#34;:&#34;probability&#34;,&#34;name&#34;:&#34;bar&#34;},{&#34;type&#34;:&#34;scatter&#34;,&#34;inherit&#34;:true,&#34;x&#34;:[155,165,175,185,195],&#34;marker&#34;:{&#34;line&#34;:{&#34;width&#34;:1},&#34;color&#34;:&#34;rgba(238,50,36,1)&#34;},&#34;histnorm&#34;:&#34;probability&#34;,&#34;name&#34;:&#34;polygon&#34;,&#34;y&#34;:[0.0667,0.2667,0.3667,0.2333,0.0667]}],&#34;layout&#34;:{&#34;title&#34;:&#34;Distribución de frequencias relativas de estaturas&#34;,&#34;xaxis&#34;:{&#34;title&#34;:&#34;Estatura&#34;},&#34;yaxis&#34;:{&#34;title&#34;:&#34;Frecuencia relativa&#34;},&#34;autosize&#34;:false,&#34;width&#34;:600,&#34;height&#34;:400,&#34;showlegend&#34;:false,&#34;margin&#34;:{&#34;b&#34;:40,&#34;l&#34;:60,&#34;t&#34;:25,&#34;r&#34;:10}},&#34;url&#34;:null,&#34;width&#34;:null,&#34;height&#34;:null,&#34;base_url&#34;:&#34;https://plot.ly&#34;,&#34;layout.1&#34;:{&#34;title&#34;:&#34;Distribución de frequencias relativas de estaturas&#34;,&#34;xaxis&#34;:{&#34;title&#34;:&#34;Estatura&#34;},&#34;yaxis&#34;:{&#34;title&#34;:&#34;Frecuencia relativa&#34;},&#34;autosize&#34;:false,&#34;width&#34;:600,&#34;height&#34;:400,&#34;showlegend&#34;:false},&#34;filename&#34;:&#34;Distribución de frequencias relativas de estaturas&#34;},&#34;evals&#34;:[]}&lt;/script&gt;
&lt;p&gt;El polígono de frecuencias acumuladas (absolutas o relativas) se conoce como &lt;strong&gt;ojiva&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;. El histograma y la ojiva siguientes muestran la distribución de frecuencias relativas acumuladas de estaturas.&lt;/p&gt;
&lt;div id=&#34;ogive&#34; class=&#34;plotly&#34; style=&#34;margin: auto; width:80%&#34;&gt;&lt;/div&gt;
&lt;script type=&#34;application/json&#34; data-for=&#34;ogive&#34;&gt;{&#34;x&#34;:{&#34;data&#34;:[{&#34;type&#34;:&#34;bar&#34;,&#34;inherit&#34;:true,&#34;x&#34;:[155,165,175,185,195],&#34;y&#34;:[0.0666666666666667,0.333333333333333,0.7,0.933333333333333,1],&#34;marker&#34;:{&#34;line&#34;:{&#34;width&#34;:1},&#34;color&#34;:&#34;rgba(137,211,243,1)&#34;},&#34;name&#34;:&#34;bar&#34;},{&#34;type&#34;:&#34;scatter&#34;,&#34;inherit&#34;:true,&#34;x&#34;:[150,160,170,180,190,200],&#34;y&#34;:[0,0.0666666666666667,0.333333333333333,0.7,0.933333333333333,1],&#34;marker&#34;:{&#34;line&#34;:{&#34;width&#34;:1},&#34;color&#34;:&#34;rgba(238,50,36,1)&#34;},&#34;name&#34;:&#34;ogive&#34;}],&#34;layout&#34;:{&#34;title&#34;:&#34;Distribución de frecuencias relativas acumuladas de estaturas&#34;,&#34;xaxis&#34;:{&#34;title&#34;:&#34;Estatura&#34;},&#34;yaxis&#34;:{&#34;title&#34;:&#34;Frecuencia relativa acumulada&#34;},&#34;autosize&#34;:false,&#34;width&#34;:600,&#34;height&#34;:400,&#34;bargap&#34;:0,&#34;showlegend&#34;:false,&#34;margin&#34;:{&#34;b&#34;:40,&#34;l&#34;:60,&#34;t&#34;:25,&#34;r&#34;:10}},&#34;url&#34;:null,&#34;width&#34;:null,&#34;height&#34;:null,&#34;base_url&#34;:&#34;https://plot.ly&#34;,&#34;layout.1&#34;:{&#34;title&#34;:&#34;Distribución de frecuencias relativas acumuladas de estaturas&#34;,&#34;xaxis&#34;:{&#34;title&#34;:&#34;Estatura&#34;},&#34;yaxis&#34;:{&#34;title&#34;:&#34;Frecuencia relativa acumulada&#34;},&#34;autosize&#34;:false,&#34;width&#34;:600,&#34;height&#34;:400,&#34;bargap&#34;:0,&#34;showlegend&#34;:false},&#34;filename&#34;:&#34;Distribución de frecuencias relativas acumuladas de estaturas&#34;},&#34;evals&#34;:[]}&lt;/script&gt;
&lt;p&gt;Obsérvese que en la ojiva se unen los vértices superiores derechos de cada barra con segmentos, en lugar de los puntos centrales, ya que no se consigue alcanzar la frecuencia acumulada correspondiente a la clase hasta que no se alcanza el final del intervalo.&lt;/p&gt;
&lt;h3 id=&#34;diagrama-de-sectores&#34;&gt;Diagrama de sectores&lt;/h3&gt;
&lt;p&gt;Un &lt;em&gt;diagrama de sectores&lt;/em&gt; consiste en un círculo divido en porciones, uno por cada valor o categoría de la variable.
Cada porción se conoce como &lt;em&gt;sector&lt;/em&gt; y su ángulo o área es proporcional a la correspondiente frecuencia del valor o categoría.&lt;/p&gt;
&lt;p&gt;Los diagramas de sectores pueden representar frecuencias absolutas o relativas, pero no pueden representar frecuencias acumuladas, y se utilizan sobre todo con atributos nominales.
Para atributos ordinales o variables cuantitativas es mejor utilizar diagramas de barras, ya es más fácil percibir las diferencias en una dimensión (altura de las barras) que en dos dimensiones (áreas de los sectores).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;. El diagrama de sectores siguiente muestra la distribución de frecuencias relativas de los grupos sanguíneos.&lt;/p&gt;
&lt;div id=&#34;piechart&#34; class=&#34;plotly&#34; style=&#34;margin: auto; width:80%&#34;&gt;&lt;/div&gt;
&lt;script type=&#34;application/json&#34; data-for=&#34;piechart&#34;&gt;{&#34;x&#34;:{&#34;data&#34;:[{&#34;type&#34;:&#34;pie&#34;,&#34;inherit&#34;:true,&#34;labels&#34;:[&#34;0&#34;,&#34;A&#34;,&#34;AB&#34;,&#34;B&#34;],&#34;values&#34;:[5,14,3,8]}],&#34;layout&#34;:{&#34;title&#34;:&#34;Distribución de frecuencias relativas de los grupos sanguíneos&#34;,&#34;autosize&#34;:false,&#34;width&#34;:600,&#34;height&#34;:400,&#34;margin&#34;:{&#34;b&#34;:40,&#34;l&#34;:60,&#34;t&#34;:25,&#34;r&#34;:10}},&#34;url&#34;:null,&#34;width&#34;:null,&#34;height&#34;:null,&#34;base_url&#34;:&#34;https://plot.ly&#34;,&#34;layout.1&#34;:{&#34;title&#34;:&#34;Distribución de frecuencias relativas de los grupos sanguíneos&#34;,&#34;autosize&#34;:false,&#34;width&#34;:600,&#34;height&#34;:400},&#34;filename&#34;:&#34;Distribución de frecuencias relativas de los grupos sanguíneos&#34;},&#34;evals&#34;:[]}&lt;/script&gt;
&lt;h3 id=&#34;la-distribución-normal&#34;&gt;La distribución Normal&lt;/h3&gt;
&lt;p&gt;Las distribuciones con diferentes propiedades presentan formas distintas.&lt;/p&gt;
&lt;!-- TODO: Insertar histogramas con diferentes formas --&gt;
&lt;h2 id=&#34;datos-atípicos&#34;&gt;Datos atípicos&lt;/h2&gt;
&lt;p&gt;Uno de los principales problemas de las muestras son los &lt;strong&gt;datos atípicos&lt;/strong&gt;, que son valores de la variable que se diferencian mucho del resto de los valores en la muestra.&lt;/p&gt;
&lt;img src=&#34;../img/descriptiva/dato_atipico.png&#34; alt=&#34;Dato atípico&#34; width=&#34;400&#34;&gt;
&lt;p&gt;Es muy importante detectar los datos atípicos antes de realizar cualquier análisis de los datos, pues suelen distorsionar los resultados.&lt;/p&gt;
&lt;p&gt;Aparecen siempre en los extremos de la distribución, y pueden detectarse con un diagrama de caja y bigotes (tal y como veremos más adelante).&lt;/p&gt;
&lt;h3 id=&#34;tratamiento-de-los-datos-atípicos&#34;&gt;Tratamiento de los datos atípicos&lt;/h3&gt;
&lt;p&gt;Cuando trabajemos con muestras grandes, los datos atípicos tienen menor influencia y pueden dejarse en la muestra.&lt;/p&gt;
&lt;p&gt;Cuando trabajemos con muestras pequeñas tenemos varias opciones:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Eliminar el dato atípico si se trata de un error.&lt;/li&gt;
&lt;li&gt;Sustituir el dato atípico por el menor o el mayor valor de la distribución que no es atípico si no se trata de un error y el dato atípico no concuerda con la distribución teórica.&lt;/li&gt;
&lt;li&gt;Dejar el dato atípico si no es un error, y cambiar el modelo de distribución teórico para adecuarlo a los datos atípicos.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;estadísticos-muestrales&#34;&gt;Estadísticos muestrales&lt;/h2&gt;
&lt;p&gt;La tabla de frecuencias sintetiza la información de la distribución de valores de la variable estudiada en la muestra, pero en muchas ocasiones es insuficiente para describir determinados aspectos de la distribución, como por ejemplo, cuáles son los valores más representativos de la muestra, cómo es la variabilidad de los datos, qué datos pueden considerarse atípicos, o cómo es la simetría de la distribución.&lt;/p&gt;
&lt;p&gt;Para describir esos aspectos de la distribución muestral se utilizan unas medidas resumen llamadas &lt;strong&gt;estadísticos muestrales&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;De acuerdo al aspecto de las distribución que miden, existen diferentes tipos de estadísticos:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Estadísticos de Posición&lt;/strong&gt;: Miden los valores en torno a los que se agrupan los datos o que dividen la distribución en partes iguales.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Estadísticos de Dispersión&lt;/strong&gt;: Miden la heterogeneidad de los datos.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Estadísticos de Forma&lt;/strong&gt;: Miden aspectos de la forma que tiene la distribución de los datos, como la simetría o el apuntamiento.&lt;/p&gt;
&lt;h2 id=&#34;estadísticos-de-posición&#34;&gt;Estadísticos de posición&lt;/h2&gt;
&lt;p&gt;Pueden ser de dos tipos:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Estadísticos de Tendencia Central&lt;/strong&gt;: Determinan valores alrededor de los cuales se concentran los datos, habitualmente en el centro de la distribución. Estas medidas suelen utilizarse como valores representativos de la muestra. Las más importantes son:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Media aritmética&lt;/li&gt;
&lt;li&gt;Mediana&lt;/li&gt;
&lt;li&gt;Moda&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Estadísticos de Posición no centrales&lt;/strong&gt;: Dividen la distribución en partes con el mismo número de datos. Las más importantes son:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Cuartiles.&lt;/li&gt;
&lt;li&gt;Deciles.&lt;/li&gt;
&lt;li&gt;Percentiles.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;media-aritmética&#34;&gt;Media aritmética&lt;/h3&gt;
&lt;div class=&#34;alert alert-def&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;&lt;strong&gt;Definición - Media aritmética muestral $\bar{x}$&lt;/strong&gt;. La &lt;em&gt;media aritmética muestral&lt;/em&gt; de una variable $X$ es la suma de los valores observados en la muestra dividida por el tamaño muestral&lt;/p&gt;
&lt;p&gt;$$\bar{x} = \frac{\sum x_i}{n}$$&lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;A partir de la tabla de frecuencias puede calcularse con la fórmula&lt;/p&gt;
&lt;p&gt;$$\bar{x} = \frac{\sum x_in_i}{n} = \sum x_i f_i$$&lt;/p&gt;
&lt;p&gt;En la mayoría de los casos, la media aritmética es la medida que mejor representa a la muestra.&lt;/p&gt;
&lt;div class=&#34;alert alert-warning&#34;&gt;
  &lt;div&gt;
    No puede calcularse para variables cualitativas.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Ejemplo - Datos no agrupados&lt;/strong&gt;. Utilizando los datos de la muestra del número de hijos en las familias, la media aritmética es&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
\bar{x} &amp;amp;= \frac{1+2+4+2+2+2+3+2+1+1+0+2+2}{25}+\newline
&amp;amp;+\frac{0+2+2+1+2+2+3+1+2+2+1+2}{25} = \frac{44}{25} = 1.76 \mbox{ hijos}.
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;o bien, desde la tabla de frecuencias&lt;/p&gt;
&lt;p&gt;$$
\begin{array}{rrrrr}
\hline
x_i &amp;amp; n_i &amp;amp; f_i &amp;amp; x_in_i &amp;amp; x_if_i\newline
\hline
0 &amp;amp; 2 &amp;amp; 0.08 &amp;amp; 0 &amp;amp; 0\newline
1 &amp;amp; 6 &amp;amp; 0.24 &amp;amp; 6 &amp;amp; 0.24\newline
2 &amp;amp; 14 &amp;amp; 0.56 &amp;amp; 28 &amp;amp; 1.12\newline
3 &amp;amp; 2  &amp;amp; 0.08 &amp;amp; 6 &amp;amp; 0.24\newline
4 &amp;amp; 1 &amp;amp; 0.04 &amp;amp; 4 &amp;amp; 0.16 \newline
\hline
\sum &amp;amp; 25 &amp;amp; 1 &amp;amp; 44 &amp;amp; 1.76 \newline
\hline
\end{array}
$$&lt;/p&gt;
&lt;p&gt;$$
\bar{x} = \frac{\sum x_in_i}{n} = \frac{44}{25}= 1.76 \mbox{ hijos}\qquad \bar{x}=\sum{x_if_i} = 1.76 \mbox{ hijos}.
$$&lt;/p&gt;
&lt;p&gt;Esto significa que el valor que mejor representa el número de hijos en las familias de la muestra es 1.76 hijos.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Ejemplo - Datos agrupados&lt;/strong&gt;. Utilizando los datos de la muestra de estaturas, la media es&lt;/p&gt;
&lt;p&gt;$$
\bar{x} = \frac{179+173+\cdots+187}{30} = 175.07 \mbox{ cm}.
$$&lt;/p&gt;
&lt;p&gt;o bien, desde la tabla de frecuencias utilizando las marcas de clase $x_i$:&lt;/p&gt;
&lt;p&gt;$$
\begin{array}{crrrrr}
\hline
X &amp;amp; x_i &amp;amp; n_i &amp;amp; f_i &amp;amp; x_in_i &amp;amp; x_if_i\newline
\hline
(150,160] &amp;amp; 155 &amp;amp; 2 &amp;amp; 0.07 &amp;amp; 310 &amp;amp; 10.33\newline
(160,170] &amp;amp; 165 &amp;amp; 8 &amp;amp; 0.27 &amp;amp; 1320 &amp;amp; 44.00\newline
(170,180] &amp;amp; 175 &amp;amp; 11 &amp;amp; 0.36 &amp;amp; 1925 &amp;amp; 64.17\newline
(180,190] &amp;amp; 185 &amp;amp; 7 &amp;amp; 0.23 &amp;amp; 1295 &amp;amp; 43.17\newline
(190,200] &amp;amp; 195 &amp;amp; 2 &amp;amp; 0.07 &amp;amp; 390 &amp;amp; 13 \newline
\hline
\sum &amp;amp;  &amp;amp; 30 &amp;amp; 1 &amp;amp; 5240 &amp;amp; 174.67 \newline
\hline
\end{array}
$$&lt;/p&gt;
&lt;p&gt;$$
\bar{x} = \frac{\sum x_in_i}{n} = \frac{5240}{30}= 174.67 \mbox{ cm} \qquad \bar{x}=\sum{x_if_i} = 174.67 \mbox{ cm}.
$$&lt;/p&gt;
&lt;p&gt;Obsérvese que al calcular la media desde la tabla de frecuencias el resultado difiere ligeramente del valor real obtenido directamente desde la muestra, ya que los valores usados en los cálculos no son los datos reales sino las marcas de clase.&lt;/p&gt;
&lt;h4 id=&#34;media-ponderada&#34;&gt;Media ponderada&lt;/h4&gt;
&lt;p&gt;En algunos casos, los valores de la muestra no tienen la misma importancia. En este caso la importancia o &lt;em&gt;peso&lt;/em&gt; de cada valor de la muestra debe tenerse en cuenta al calcular la media.&lt;/p&gt;
&lt;div class=&#34;alert alert-def&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;&lt;strong&gt;Definición - Media ponderada muestral $\bar x_p$&lt;/strong&gt;. Dada una muestra de valores $x_1,\ldots, x_n$ donde cada valor $x_i$ tiene asociado un peso $p_i$, la &lt;em&gt;media ponderada muestral&lt;/em&gt; de la variable $X$ es la suma de los productos de cada valor observado en la muestra por su peso, dividida por la suma de todos los pesos&lt;/p&gt;
&lt;p&gt;$$\bar{x}_p = \frac{\sum x_ip_i}{\sum p_i}$$&lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;A partir de la tabla de frecuencias puede calcularse con la fórmula&lt;/p&gt;
&lt;p&gt;$$\bar{x}_p = \frac{\sum x_ip_in_i}{\sum p_i}$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Ejemplo&lt;/strong&gt;. Supóngase que un estudiante quiere calcular una medida que represente su rendimiento en el curso. La nota obtenida en cada asignatura y sus créditos son&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Asignatura&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Créditos&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Nota&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Matemáticas&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;6&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;5&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Economía&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;4&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Química&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;8&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;6&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;La media aritmética vale&lt;/p&gt;
&lt;p&gt;$$\bar{x} = \frac{\sum x_i}{n} = \frac{5+3+6}{3}= 4.67 \text{ puntos}.$$&lt;/p&gt;
&lt;p&gt;Sin embargo, esta nota no representa bien el rendimiento académico del alumno ya que no todas las asignaturas tienen la misma importancia ni requieren el mismo esfuerzo para aprobar. Las asignaturas con más créditos requieren más trabajo y deben tener más peso en el cálculo de la media.&lt;/p&gt;
&lt;p&gt;Es más lógico usar la media ponderada como medida del rendimiento del estudiante, tomando como pesos los créditos de cada asignatura&lt;/p&gt;
&lt;p&gt;$$
\bar{x}_p = \frac{\sum x_ip_i}{\sum p_i} = \frac{5\cdot 6+3\cdot 4+6\cdot 8}{6+4+8}= \frac{90}{18} = 5 \text{ puntos}.
$$&lt;/p&gt;
&lt;h3 id=&#34;mediana&#34;&gt;Mediana&lt;/h3&gt;
&lt;div class=&#34;alert alert-def&#34;&gt;
  &lt;div&gt;
    &lt;strong&gt;Definición - Mediana muestral $Me$&lt;/strong&gt;. La &lt;em&gt;mediana muestral&lt;/em&gt; de una variable $X$ es el valor de la variable que está en el medio de la muestra ordenada.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;La mediana divide la distribución de la muestra en dos partes iguales, es decir, hay el mismo número de valores por debajo y por encima de la mediana. Por tanto, tiene frecuencias acumuladas $N_{Me}= n/2$ y $F_{Me}= 0.5$.&lt;/p&gt;
&lt;div class=&#34;alert alert-warning&#34;&gt;
  &lt;div&gt;
    No puede calcularse para variables nominales.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Con datos no agrupados pueden darse varios casos:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Tamaño muestral impar: La mediana es el valor que ocupa la posición $\frac{n+1}{2}$.&lt;/li&gt;
&lt;li&gt;Tamaño muestral par: La mediana es la media de los valores que ocupan las posiciones $\frac{n}{2}$ y $\frac{n}{2}+1$.&lt;/li&gt;
&lt;/ul&gt;
&lt;img src=&#34;../img/descriptiva/mediana.svg&#34; alt=&#34;Cálculo de la mediana con datos no agrupados&#34; width=&#34;700&#34;&gt;
&lt;p&gt;&lt;strong&gt;Ejemplo&lt;/strong&gt;. Utilizando los datos del número de hijos de las familias, el tamaño muestral es 25, que es impar, y la mediana es el valor que ocupa la posición $\frac{25+1}{2} = 13$ de la muestra ordenada.&lt;/p&gt;
&lt;p&gt;$$0,0,1,1,1,1,1,1,2,2,2,2,\fbox{2},2,2,2,2,2,2,2,2,2,3,3,4$$&lt;/p&gt;
&lt;p&gt;y la mediana es 2 hijos.&lt;/p&gt;
&lt;p&gt;Si se trabaja con la tabla de frecuencias, la mediana es el valor más pequeño con una frecuencia acumulada mayor o igual a $13$, o con una frecuencia relativa acumulada mayor o igual que $0.5$.&lt;/p&gt;
&lt;p&gt;$$
\begin{array}{rrrrr}
\hline
x_i &amp;amp; n_i &amp;amp; f_i &amp;amp; N_i &amp;amp; F_i\newline
\hline
0 &amp;amp; 2 &amp;amp; 0.08 &amp;amp; 2 &amp;amp; 0.08\newline
1 &amp;amp; 6 &amp;amp; 0.24 &amp;amp; 8 &amp;amp; 0.32\newline
\color{red}2 &amp;amp; 14 &amp;amp; 0.56 &amp;amp; 22 &amp;amp; 0.88\newline
3 &amp;amp; 2  &amp;amp; 0.08 &amp;amp; 24 &amp;amp; 0.96\newline
4 &amp;amp; 1 &amp;amp; 0.04 &amp;amp; 25 &amp;amp; 1 \newline
\hline
\sum &amp;amp; 25 &amp;amp; 1 \newline
\hline
\end{array}
$$&lt;/p&gt;
&lt;h4 id=&#34;cálculo-de-la-mediana-con-datos-agrupados&#34;&gt;Cálculo de la mediana con datos agrupados&lt;/h4&gt;
&lt;p&gt;Con datos agrupados la mediana se calcula interpolando en el polígono de frecuencias relativas acumuladas para el valor 0.5.&lt;/p&gt;
&lt;img src=&#34;../img/descriptiva/interpolacion.svg&#34; alt=&#34;Cálculo de la mediana con datos agrupados&#34; width=&#34;600&#34;&gt;
&lt;p&gt;Ambas expresiones son iguales ya que el ángulo $\alpha$ es el mismo, y resolviendo la ecuación se tiene la siguiente fórmula para calcular la mediana&lt;/p&gt;
&lt;p&gt;$$
Me=l_i+\frac{0.5-F_{i-1}}{F_i-F_{i-1}}(l_i-l_{i-1})=l_i+\frac{0.5-F_{i-1}}{f_i}a_i
$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Ejemplo - Datos agrupados&lt;/strong&gt;. Utilizando los datos de la muestra de las estaturas de estudiantes, la mediana cae en la clase (170,180].&lt;/p&gt;
&lt;img src=&#34;../img/descriptiva/interpolacion_ejemplo_1.svg&#34; alt=&#34;Ejemplo de cálculo de la mediana con datos agrupados&#34; width=&#34;600&#34;&gt;
&lt;p&gt;Interpolando en el intervalo (170,180] se tiene&lt;/p&gt;
&lt;img src=&#34;../img/descriptiva/interpolacion_ejemplo_2.svg&#34; alt=&#34;Ejemplo de cálculo de la mediana con datos agrupados&#34; width=&#34;600&#34;&gt;
&lt;p&gt;Igualando ambas expresiones y resolviendo la ecuación se obtiene&lt;/p&gt;
&lt;p&gt;$$
Me= 170+\frac{0.5-0.34}{0.7-0.34}(180-170)=170+\frac{0.16}{0.36}10=174.54 \mbox{ cm}.
$$&lt;/p&gt;
&lt;p&gt;Esto significa que la mitad de los estudiantes tienen estaturas menores o iguales que 174.54 cm y la otra mitad mayores o iguales.&lt;/p&gt;
&lt;h3 id=&#34;moda&#34;&gt;Moda&lt;/h3&gt;
&lt;div class=&#34;alert alert-def&#34;&gt;
  &lt;div&gt;
    &lt;strong&gt;Definición - Moda muestral $Mo$&lt;/strong&gt;. La &lt;em&gt;moda muestral&lt;/em&gt; de una variable $X$ es el valor de la variable más frecuente en la muestra.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Con datos agrupados la &lt;em&gt;clase modal&lt;/em&gt; es la clase con mayor frecuencia en la muestra.&lt;/p&gt;
&lt;p&gt;Puede calcularse para todos los tipos de variables (cuantitativas y cualitativas).&lt;/p&gt;
&lt;p&gt;Las distribuciones pueden tener más de una moda.&lt;/p&gt;
&lt;img src=&#34;../img/descriptiva/moda.png&#34; alt=&#34;Cálculo de la moda&#34; width=&#34;600&#34;&gt;
&lt;p&gt;&lt;strong&gt;Ejemplo&lt;/strong&gt;. Utilizando los datos de la muestra del número de hijos en las familias, el valor con mayor frecuencia es 2, y por tanto la moda es $Mo=2$.&lt;/p&gt;
&lt;p&gt;$$
\begin{array}{rr}
\hline
x_i &amp;amp; n_i \newline
\hline
0 &amp;amp; 2 \newline
1 &amp;amp; 6 \newline
\color{red} 2 &amp;amp; 14 \newline
3 &amp;amp; 2  \newline
4 &amp;amp; 1 \newline
\hline
\end{array}
$$&lt;/p&gt;
&lt;p&gt;Utilizando los datos de la muestra de estaturas de estudiantes, la clase con la mayor frecuencia es $(170,180]$, que es la clase modal $Mo=(170,180]$.&lt;/p&gt;
&lt;p&gt;$$
\begin{array}{cr}
\hline
X &amp;amp; n_i \newline
\hline
(150,160] &amp;amp; 2 \newline
(160,170] &amp;amp; 8 \newline
\color{red}{(170,180]} &amp;amp; 11 \newline
(180,190] &amp;amp; 7 \newline
(190,200] &amp;amp; 2 \newline
\hline
\end{array}
$$&lt;/p&gt;
&lt;h3 id=&#34;qué-estadístico-de-tendencia-central-usar&#34;&gt;¿Qué estadístico de tendencia central usar?&lt;/h3&gt;
&lt;p&gt;En general, siempre que puedan calcularse los estadísticos de tendencia central, es recomendable utilizarlos como valores representativos en el siguiente orden:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Media. La media utiliza más información que el resto ya que para calcularla se tiene en cuenta la magnitud de los datos.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Mediana. La mediana utiliza menos información que la media, pero más que la moda, ya que para calcularla se tiene en cuenta el orden de los datos.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Moda. La moda es la que menos información utiliza ya que para calcularla sólo se tienen en cuenta las frecuencias absolutas.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Pero, &lt;em&gt;hay que tener cuidado con los datos atípicos&lt;/em&gt;, ya que la media puede distorsionarse cuando hay datos atípicos. En tal caso es mejor utilizar la mediana como valor más representativo.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Ejemplo&lt;/strong&gt;. Si una muestra de número de hijos de 7 familias es&lt;/p&gt;
&lt;div style=&#34;text-align:center&#34;&gt;
0, 0, 1, 1, 2, 2, 15,
&lt;/div&gt;
&lt;p&gt;entonces, $\bar{x}=3$ hijos y $Me=1$ hijo.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;¿Qué medida representa mejor el número de hijos en la muestra?&lt;/em&gt;&lt;/p&gt;
&lt;h3 id=&#34;medidas-de-posición-no-centrales&#34;&gt;Medidas de posición no centrales&lt;/h3&gt;
&lt;p&gt;Las medidas de posición no centrales o &lt;em&gt;cuantiles&lt;/em&gt; dividen la distribución en partes iguales.&lt;/p&gt;
&lt;p&gt;Los más utilizados son:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Cuartiles&lt;/strong&gt;: Dividen la distribución en 4 partes iguales. Hay 3 cuartiles: $C_1$ (25% acumulado), $C_2$ (50% acumulado), $C_3$ (75% acumulado).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Deciles&lt;/strong&gt;: Dividen la distribución en 10 partes iguales. Hay 9 deciles: $D_1$ (10% acumulado),&amp;hellip;, $D_9$ (90% acumulado).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Percentiles&lt;/strong&gt;: Dividen la distribución en 100 partes iguales. Hay 99 percentiles: $P_1$ (1% acumulado),&amp;hellip;, $P_{99}$ (99% acumulado).&lt;/p&gt;
&lt;img src=&#34;../img/descriptiva/cuantiles.svg&#34; alt=&#34;Cuartiles, deciles y percentiles&#34; width=&#34;600&#34;&gt;
&lt;p&gt;Obsérvese que hay una correspondencia entre los cuartiles, los deciles y los percentiles. Por ejemplo, el primer cuartil coincide con el percentil 25, y el cuarto decil coincide con el percentil 40.&lt;/p&gt;
&lt;p&gt;Los cuantiles se calculan de forma similar a la mediana. La única diferencia es la frecuencia relativa acumulada que corresponde a cada cuantil.&lt;/p&gt;
&lt;img src=&#34;../img/descriptiva/cuantiles_calculo.svg&#34; alt=&#34;Cálculo de los cuartiles, deciles y percentiles&#34; width=&#34;600&#34;&gt;
&lt;p&gt;&lt;strong&gt;Ejemplo&lt;/strong&gt;. Utilizando los datos de la muestra del número de hijos de las familias, la frecuencia relativa acumulada era&lt;/p&gt;
&lt;p&gt;$$
\begin{array}{rr}
\hline
x_i &amp;amp; F_i \newline
\hline
0 &amp;amp; 0.08\newline
1 &amp;amp; 0.32\newline
2 &amp;amp; 0.88\newline
3 &amp;amp; 0.96\newline
4 &amp;amp; 1\newline
\hline
\end{array}
$$&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
F_{C_1}=0.25 &amp;amp;\Rightarrow Q_1 = 1 \text{ hijos},\newline
F_{C_2}=0.5 &amp;amp;\Rightarrow Q_2 = 2 \text{ hijos},\newline
F_{C_3}=0.75 &amp;amp;\Rightarrow Q_3 = 2 \text{ hijos},\newline
F_{D_4}=0.4 &amp;amp;\Rightarrow D_4 = 2 \text{ hijos},\newline
F_{P_{92}}=0.92 &amp;amp;\Rightarrow P_{92} = 3 \text{ hijos}.
\end{aligned}$$&lt;/p&gt;
&lt;h2 id=&#34;estadísticos-de-dispersión&#34;&gt;Estadísticos de dispersión&lt;/h2&gt;
&lt;p&gt;La &lt;em&gt;dispersión&lt;/em&gt; se refiere a la heterogeneidad o variabilidad de los datos. Así pues, los estadísticos de dispersión mide la variabilidad global de los datos, o con respecto a una medida de tendencia central.&lt;/p&gt;
&lt;p&gt;Para las variables cuantitativas, las más empleadas son:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Recorrido.&lt;/li&gt;
&lt;li&gt;Rango Intercuartílico.&lt;/li&gt;
&lt;li&gt;Varianza.&lt;/li&gt;
&lt;li&gt;Desviación Típica.&lt;/li&gt;
&lt;li&gt;Coeficiente de Variación.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;recorrido&#34;&gt;Recorrido&lt;/h3&gt;
&lt;div class=&#34;alert alert-def&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;&lt;strong&gt;Recorrido muestral $Re$&lt;/strong&gt;. El &lt;em&gt;recorrido muestral&lt;/em&gt; de una variable $X$ se define como la diferencia entre el máximo y el mínimo de los valores en la muestra.&lt;/p&gt;
&lt;p&gt;$$Re = \max_{x_i} -\min_{x_i}$$&lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;img src=&#34;../img/descriptiva/rango.svg&#34; alt=&#34;Rango&#34; width=&#34;600&#34;&gt;
&lt;p&gt;El recorrido mide la máxima variación que hay entre los datos muestrales. No obstante, es muy sensible a datos atípicos ya que suelen aparecer justo en los extremos de la distribución, por lo que no se suele utilizar mucho.&lt;/p&gt;
&lt;h3 id=&#34;rango-intercuartílico&#34;&gt;Rango intercuartílico&lt;/h3&gt;
&lt;p&gt;Para evitar el problema de los datos atípicos en el recorrido, se puede utilizar el primer y tercer cuartil en lugar del mínimo y el máximo.&lt;/p&gt;
&lt;div class=&#34;alert alert-def&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;&lt;strong&gt;Definición - Rango intercuartílico muestral $RI$&lt;/strong&gt;. El &lt;em&gt;rango intercuartílico muestral&lt;/em&gt; de una variable $X$ se define como la diferencia entre el tercer y el primer cuartil de la muestra.&lt;/p&gt;
&lt;p&gt;$$RI = C_3 -C_1$$&lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;img src=&#34;../img/descriptiva/rango_intercuartilico.svg&#34; alt=&#34;Rango intercuartílico&#34; width=&#34;600&#34;&gt;
&lt;p&gt;El rango intercuartílico mide la dispersión del 50% de los datos centrales.&lt;/p&gt;
&lt;h3 id=&#34;diagrama-de-caja-y-bigotes&#34;&gt;Diagrama de caja y bigotes&lt;/h3&gt;
&lt;p&gt;La dispersión de una variable suele representarse gráficamente mediante un &lt;em&gt;diagrama de caja y bigotes&lt;/em&gt;, que representa cinco estadísticos descriptivos (mínimo, cuartiles y máximo) conocidos como los &lt;em&gt;cinco números&lt;/em&gt;. Consiste en una caja, dibujada desde el primer al tercer cuartil, que representa el rango intercuartílico, y dos segmentos, conocidos como &lt;em&gt;bigotes&lt;/em&gt; inferior y superior. A menudo la caja se divide en dos por la mediana.&lt;/p&gt;
&lt;p&gt;Este diagrama es muy útil y se utiliza para muchos propósitos:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Sirve para medir la dispersión de los datos ya que representa el rango y el rango intercuartílico.&lt;/li&gt;
&lt;li&gt;Sirve para detectar datos atípicos, que son los valores que quedan fuera del intervalo definido por los bigotes.&lt;/li&gt;
&lt;li&gt;Sirve para medir la simetría de la distribución, comparando la longitud de las cajas y de los bigotes por encima y por debajo de la mediana.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Ejemplo&lt;/strong&gt;. El diagrama siguiente muestra el diagrama de caja y bigotes del peso de una muestra de recién nacidos.&lt;/p&gt;
&lt;img src=&#34;../img/descriptiva/diagrama_caja.png&#34; alt=&#34;Diagrama de caja y bigotes del peso de recien nacidos&#34; width=&#34;600&#34;&gt;
&lt;p&gt;Para construir el diagrama de caja y bigotes hay que seguir los siguientes pasos:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Calcular los cuartiles.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Dibujar una caja de manera que el extremo inferior caiga sobre el primer cuartil y el extremo superior sobre el tercer cuartil.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Dividir la caja con una línea que caiga sobre el segundo cuartil.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Para los bigotes inicialmente se calculan dos valores llamados &lt;em&gt;vallas&lt;/em&gt; $v_1$ y $v_2$. La valla inferior es el primer cuartil menos una vez y media el rango intercuartílico, y la valla superior es el tercer cuartil más una vez y media el rango intercuartílico.&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
v_1&amp;amp;=Q_1-1.5,\text{IQR}\newline
v_2&amp;amp;=Q_3+1.5,\text{IQR}
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;Las vallas definen el intervalo donde los datos se consideran normales. Cualquier valor fuera de ese intervalo se considera un dato atípico.&lt;br&gt;
El bigote superior se dibuja desde el borde inferior de la caja hasta el menor valor de la muestra que es mayor o igual a la valla inferior, y el bigote superior se dibuja desde el borde superior de la caja hasta el mayor valor de la muestra que es menor o igual a la valla superior.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&#34;alert alert-warning&#34;&gt;
  &lt;div&gt;
    Los bigotes no son las vallas.
  &lt;/div&gt;
&lt;/div&gt;
&lt;ol start=&#34;5&#34;&gt;
&lt;li&gt;Finalmente, si en la muestra hay algún dato atípico, se dibuja un punto para cada uno de ellos.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Ejemplo&lt;/strong&gt;. El diagrama de caja y bigotes de la muestra del número de hijos de las familias se muestra a continuación.&lt;/p&gt;
&lt;img src=&#34;../img/descriptiva/diagrama_caja_hijos.png&#34; alt=&#34;Diagrama de caja y bigotes del número de hijos&#34; width=&#34;600&#34;&gt;
&lt;h4 id=&#34;desviaciones-respecto-de-la-media&#34;&gt;Desviaciones respecto de la media&lt;/h4&gt;
&lt;p&gt;Otra forma de medir la variabilidad de una variable es estudiar la concentración de los valores en torno a algún estadístico de tendencia central como por ejemplo la media.&lt;/p&gt;
&lt;p&gt;Para ello se suele medir la distancia de cada valor a la media. A ese valor se le llama &lt;strong&gt;desviación de la media&lt;/strong&gt;.&lt;/p&gt;
&lt;img src=&#34;../img/descriptiva/desviaciones.svg&#34; alt=&#34;Desviaciones de la media&#34; width=&#34;400&#34;&gt;
&lt;p&gt;Si las desviaciones son grandes la media no será tan representativa como cuando la desviaciones sean pequeñas.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;. La siguiente tabla contiene las notas de 3 estudiantes en un curso con las asignaturas $A$, $B$ y $C$.&lt;/p&gt;
&lt;p&gt;$$
\begin{array}{cccc}
\hline
A &amp;amp; B &amp;amp; C &amp;amp; \bar x \newline
0 &amp;amp; 5 &amp;amp; 10 &amp;amp; 5 \newline
4 &amp;amp; 5 &amp;amp; 6 &amp;amp; 5 \newline
5 &amp;amp; 5 &amp;amp; 5 &amp;amp; 5 \newline
\hline
\end{array}
$$&lt;/p&gt;
&lt;p&gt;Todos los estudiantes tienen la misma media, pero, en qué caso la media representa mejor el rendimiento en el curso?&lt;/p&gt;
&lt;h3 id=&#34;varianza-y-desviación-típica&#34;&gt;Varianza y desviación típica&lt;/h3&gt;
&lt;div class=&#34;alert alert-def&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;&lt;strong&gt;Definición - Varianza $s^2$&lt;/strong&gt;. La &lt;em&gt;varianza muestral&lt;/em&gt; de una variable $X$ se define como el promedio del cuadrado de las desviaciones de los valores de la muestra respecto de la media muestral.&lt;/p&gt;
&lt;p&gt;$$s^2 = \frac{\sum (x_i-\bar x)^2n_i}{n} = \sum (x_i-\bar x)^2f_i$$&lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;También puede calcularse de manera más sencilla mediante la fórmula&lt;/p&gt;
&lt;p&gt;$$s^2 = \frac{\sum x_i^2n_i}{n} -\bar x^2= \sum x_i^2f_i-\bar x^2$$&lt;/p&gt;
&lt;p&gt;La varianza tiene las unidades de la variable al cuadrado, por lo que para facilitar su interpretación se suele utilizar su raíz cuadrada.&lt;/p&gt;
&lt;div class=&#34;alert alert-def&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;&lt;strong&gt;Definición - Desviación típica $s$&lt;/strong&gt;. La &lt;em&gt;desviación típica muestral&lt;/em&gt; de una variable $X$ se define como la raíz cuadrada positiva de su varianza muestral.&lt;/p&gt;
&lt;p&gt;$$s = +\sqrt{s^2}$$&lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;alert alert-int&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;Tanto la varianza como la desviación típica sirven para cuantificar la dispersión de los datos en torno a la media. Cuando la varianza o la desviación típica son pequeñas, los datos de la muestra están concentrados en torno a la media, y la media es una buena medida de representatividad. Por contra, cuando la varianza o la desviación típica son grandes, los datos de la muestra están alejados de la media, y la media ya no representa tan bien.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Desviación típica pequeña&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$\Rightarrow$&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Media representativa&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Desviación típica grande&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$\Rightarrow$&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Media no representativa&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Ejemplo&lt;/strong&gt;. Las siguientes muestras contienen las notas de dos estudiantes en dos asignaturas.&lt;/p&gt;
&lt;img src=&#34;../img/descriptiva/interpretacion_desviacion_tipica.svg&#34; alt=&#34;Interpretación de la desviación típica&#34; width=&#34;400&#34;&gt;
&lt;p&gt;&lt;em&gt;¿Qué media es más representativa?&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Ejemplo - Datos no agrupados&lt;/strong&gt;. Utilizando los datos de la muestra del número de hijos de las familias, con una media $\bar x=1.76$ hijos, y añadiendo una nueva columna a la tabla de frecuencias con los cuadrados de los valores,&lt;/p&gt;
&lt;p&gt;$$
\begin{array}{rrr}
\hline
x_i &amp;amp; n_i &amp;amp; x_i^2n_i \newline
\hline
0 &amp;amp; 2 &amp;amp; 0 \newline
1 &amp;amp; 6 &amp;amp; 6 \newline
2 &amp;amp; 14 &amp;amp; 56\newline
3 &amp;amp; 2  &amp;amp; 18\newline
4 &amp;amp; 1 &amp;amp; 16 \newline
\hline
\sum &amp;amp; 25 &amp;amp; 96 \newline
\hline
\end{array}$$&lt;/p&gt;
&lt;p&gt;$$s^2 = \frac{\sum x_i^2n_i}{n}-\bar x^2 = \frac{96}{25}-1.76^2= 0.7424 \mbox{ hijos}^2.$$&lt;/p&gt;
&lt;p&gt;y la desviación típica es $s=\sqrt{0.7424} = 0.8616$ hijos.&lt;/p&gt;
&lt;p&gt;Comparado este valor con el recorrido, que va de 0 a 4 hijos se observa que no es demasiado grande por lo que se puede concluir que no hay mucha dispersión y en consecuencia la media de $1.76$ hijos representa bien el número de hijos de las familias de la muestra.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Ejemplo - Datos agrupados&lt;/strong&gt;. Utilizando los datos de la muestra de estaturas de los estudiantes y agrupando las estaturas en clases, se obtenía una media $\bar x = 174.67$ cm. El cálculo de la varianza se realiza igual que antes pero tomando como valores de la variable las marcas de clase.&lt;/p&gt;
&lt;p&gt;$$
\begin{array}{crrr}
\hline
X &amp;amp; x_i &amp;amp; n_i &amp;amp; x_i^2n_i \newline
\hline
(150,160] &amp;amp; 155 &amp;amp; 2 &amp;amp; 48050\newline
(160,170] &amp;amp; 165 &amp;amp; 8 &amp;amp; 217800\newline
(170,180] &amp;amp; 175 &amp;amp; 11 &amp;amp; 336875\newline
(180,190] &amp;amp; 185 &amp;amp; 7 &amp;amp; 239575\newline
(190,200] &amp;amp; 195 &amp;amp; 2 &amp;amp; 76050\newline
\hline
\sum &amp;amp;  &amp;amp; 30 &amp;amp; 918350 \newline
\hline
\end{array}
$$&lt;/p&gt;
&lt;p&gt;$$s^2 = \frac{\sum x_i^2n_i}{n}-\bar x^2 = \frac{918350}{30}-174.67^2= 102.06 \mbox{ cm}^2,$$&lt;/p&gt;
&lt;p&gt;y la desviación típica es $s=\sqrt{102.06} = 10.1$ cm.&lt;/p&gt;
&lt;p&gt;Este valor es bastante pequeño, comparado con el recorrido de la variable, que va de 150 a 200 cm, por lo que la variable tiene poca dispersión y en consecuencia su media es muy representativa.&lt;/p&gt;
&lt;h3 id=&#34;coeficiente-de-variación&#34;&gt;Coeficiente de variación&lt;/h3&gt;
&lt;p&gt;Tanto la varianza como la desviación típica tienen unidades y eso dificulta a veces su interpretación, especialmente cuando se compara la dispersión de variables con diferentes unidades.&lt;/p&gt;
&lt;p&gt;Por este motivo, es también común utilizar la siguiente medida de dispersión que no tiene unidades.&lt;/p&gt;
&lt;div class=&#34;alert alert-def&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;&lt;strong&gt;Definición - Coeficiente de variación muestral $cv$&lt;/strong&gt;. El &lt;em&gt;coeficiente de variación muestral&lt;/em&gt; de una variable $X$ se define como el cociente entre su desviación típica muestral y el valor absoluto de su media muestral.&lt;/p&gt;
&lt;p&gt;$$cv = \frac{s}{|\bar x|}$$&lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;alert alert-int&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;El coeficiente de variación muestral mide la dispersión relativa de los valores de la muestra en torno a la media muestral.&lt;/p&gt;
&lt;p&gt;Como no tiene unidades, es muy sencillo de interpretar: Cuanto mayor sea, mayor será la dispersión relativa con respecto a la media y menos representativa será la media.&lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;El coeficiente de variación es muy útil para comparar la dispersión de distribuciones de variables diferentes, incluso si las variables tienen unidades diferentes.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Ejemplo&lt;/strong&gt;. En la muestra del número de hijos, donde la media era $\bar x=1.76$ hijos y la desviación típica $s=0.8616$ hijos, el coeficiente de variación vale&lt;/p&gt;
&lt;p&gt;$$cv = \frac{s}{|\bar x|} = \frac{0.8616}{|1.76|} = 0.49.$$&lt;/p&gt;
&lt;p&gt;En la muestra de las estaturas, donde la media era $\bar x=174.67$ cm y la desviación típica $s=10.1$ cm, el coeficiente de variación vale&lt;/p&gt;
&lt;p&gt;$$cv = \frac{s}{|\bar x|} = \frac{10.1}{|174.67|} = 0.06.$$&lt;/p&gt;
&lt;p&gt;Esto significa que la dispersión relativa en la muestra de estaturas es mucho menor que en la del número de hijos, por lo que la media de las estaturas será más representativa que la media del número de hijos.&lt;/p&gt;
&lt;h2 id=&#34;estadísticos-de-forma&#34;&gt;Estadísticos de forma&lt;/h2&gt;
&lt;p&gt;Son medidas que describen la forma de la distribución.&lt;/p&gt;
&lt;p&gt;Los aspectos más relevantes son:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Simetría&lt;/strong&gt; Mide la simetría de la distribución de frecuencias en torno a la media. El estadístico más utilizado es el &lt;em&gt;Coeficiente de Asimetría de Fisher&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Apuntamiento&lt;/strong&gt; Mide el apuntamiento o el grado de concentración de valores en torno a la media de la distribución de frecuencias. El estadístico más utilizado es el &lt;em&gt;Coeficiente de Apuntamiento o Curtosis&lt;/em&gt;.&lt;/p&gt;
&lt;h3 id=&#34;coeficiente-de-asimetría&#34;&gt;Coeficiente de asimetría&lt;/h3&gt;
&lt;div class=&#34;alert alert-def&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;&lt;strong&gt;Definición - Coeficiente de asimetría muestral $g_1$&lt;/strong&gt;. El &lt;em&gt;coeficiente de asimetría muestral&lt;/em&gt; de una variable $X$ es el promedio de las desviaciones de los valores de la muestra respecto de la media muestral, elevadas al cubo, dividido por la desviación típica al cubo.&lt;/p&gt;
&lt;p&gt;$$g_1 = \frac{\sum (x_i-\bar x)^3 n_i/n}{s^3} = \frac{\sum (x_i-\bar x)^3 f_i}{s^3}$$&lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;alert alert-int&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;Mide el grado de simetría de los valores de la muestra con respecto a la media muestra, es decir, cuantos valores de la muestra están por encima o por debajo de la media y cómo de alejados de esta.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$g_1=0$ indica que hay el mismo número de valores por encima y por debajo de la media e igualmente alejados de ella (simétrica).&lt;/li&gt;
&lt;/ul&gt;
&lt;img src=&#34;../img/descriptiva/distribucion_simetrica.svg&#34; alt=&#34;Distribución simétrica&#34; width=&#34;600&#34;&gt;
&lt;ul&gt;
&lt;li&gt;$g_1&amp;lt;0$ indica que la mayoría de los valores son mayores que la media, pero los valores menores están más alejados de ella (asimétrica a la izquierda).&lt;/li&gt;
&lt;/ul&gt;
&lt;img src=&#34;../img/descriptiva/distribucion_asimetrica_izquierda.svg&#34; alt=&#34;Distribución simétrica a la izquierda&#34; width=&#34;600&#34;&gt;
&lt;ul&gt;
&lt;li&gt;$g_1&amp;gt;0$ indica que la mayoría de los valores son menores que la media, pero los valores mayores están más alejados de ella (asimétrica a la derecha).&lt;/li&gt;
&lt;/ul&gt;
&lt;img src=&#34;../img/descriptiva/distribucion_asimetrica_derecha.svg&#34; alt=&#34;Distribución simétrica a la derecha&#34; width=&#34;600&#34;&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Ejemplo - Datos agrupados&lt;/strong&gt;. Utilizando la tabla de frecuencias de la muestra de estaturas y añadiendo una nueva columna con las desviaciones de la media $\bar x = 174.67$ cm al cubo, se tiene&lt;/p&gt;
&lt;p&gt;$$
\begin{array}{crrrr}
\hline
X &amp;amp; x_i &amp;amp; n_i &amp;amp; x_i-\bar x &amp;amp; (x_i-\bar x)^3 n_i \newline
\hline
(150,160] &amp;amp; 155 &amp;amp; 2 &amp;amp; -19.67 &amp;amp; -15221.00\newline
(160,170] &amp;amp; 165 &amp;amp; 8 &amp;amp; -9.67 &amp;amp; -7233.85\newline
(170,180] &amp;amp; 175 &amp;amp; 11 &amp;amp; 0.33 &amp;amp; 0.40\newline
(180,190] &amp;amp; 185 &amp;amp; 7 &amp;amp; 10.33 &amp;amp; 7716.12\newline
(190,200] &amp;amp; 195 &amp;amp; 2 &amp;amp; 20.33 &amp;amp; 16805.14\newline
\hline
\sum &amp;amp;  &amp;amp; 30 &amp;amp; &amp;amp; 2066.81 \newline
\hline
\end{array}
$$&lt;/p&gt;
&lt;p&gt;$$g_1 = \frac{\sum (x_i-\bar x)^3n_i/n}{s^3} = \frac{2066.81/30}{10.1^3} = 0.07.$$&lt;/p&gt;
&lt;p&gt;Como está cerca de 0, eso significa que la distribución de las estaturas es casi simétrica.&lt;/p&gt;
&lt;h3 id=&#34;coeficiente-de-apuntamiento-o-curtosis&#34;&gt;Coeficiente de apuntamiento o curtosis&lt;/h3&gt;
&lt;div class=&#34;alert alert-def&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;&lt;strong&gt;Definición - Coeficiente de apuntamiento muestral $g_2$&lt;/strong&gt;. El &lt;em&gt;coeficiente de apuntamiento muestral&lt;/em&gt; de una variable $X$ es el promedio de las desviaciones de
los valores de la muestra respecto de la media muestral, elevadas a la cuarta, dividido por la desviación típica a la
cuarta y al resultado se le resta 3.&lt;/p&gt;
&lt;p&gt;$$g_2 = \frac{\sum (x_i-\bar x)^4 n_i/n}{s^4}-3 = \frac{\sum (x_i-\bar x)^4 f_i}{s^4}-3$$&lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;alert alert-int&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;El coeficiente de apuntamiento mide la concentración de valores en torno a la media y la longitud de las colas de la distribución.
Se toma como referencia la distribución normal (campana de Gauss).&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$g_2=0$ indica que la distribución tienen un apuntamiento normal, es decir, la concentración de valores en torno a la media es similar al de una campana de Gauss (&lt;em&gt;mesocúrtica&lt;/em&gt;).&lt;/li&gt;
&lt;/ul&gt;
&lt;img src=&#34;../img/descriptiva/distribucion_mesocurtica.svg&#34; alt=&#34;Distribución mesocúrtica&#34; width=&#34;600&#34;&gt;
&lt;ul&gt;
&lt;li&gt;$g_2&amp;lt;0$ indica que la distribución tiene menos apuntamiento de lo normal, es decir, la concentración de valores en torno a la media es menor que en una campana de Gauss (&lt;em&gt;platicúrtica&lt;/em&gt;).&lt;/li&gt;
&lt;/ul&gt;
&lt;img src=&#34;../img/descriptiva/distribucion_platicurtica.svg&#34; alt=&#34;Distribución platicúrtica&#34; width=&#34;600&#34;&gt;
&lt;ul&gt;
&lt;li&gt;$g_2&amp;gt;0$ indica que la distribución tiene más apuntamiento de lo normal, es decir, la concentración de valores en torno a la media es menor que en una campana de Gauss (&lt;em&gt;leptocúrtica&lt;/em&gt;).&lt;/li&gt;
&lt;/ul&gt;
&lt;img src=&#34;../img/descriptiva/distribucion_leptocurtica.svg&#34; alt=&#34;Distribución leptocúrtica&#34; width=&#34;600&#34;&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Ejemplo - Datos agrupados&lt;/strong&gt;. Utilizando la tabla de frecuencias de la muestra de estaturas y añadiendo una nueva columna con las desviaciones de la media $\bar x = 174.67$ cm a la cuarta, se tiene&lt;/p&gt;
&lt;p&gt;$$
\begin{array}{rrrrr}
\hline
X &amp;amp; x_i &amp;amp; n_i &amp;amp; x_i-\bar x &amp;amp; (x_i-\bar x)^4 n_i\newline
\hline
(150,160] &amp;amp; 155 &amp;amp; 2 &amp;amp; -19.67 &amp;amp; 299396.99\newline
(160,170] &amp;amp; 165 &amp;amp; 8 &amp;amp; -9.67 &amp;amp; 69951.31\newline
(170,180] &amp;amp; 175 &amp;amp; 11 &amp;amp; 0.33 &amp;amp; 0.13\newline
(180,190] &amp;amp; 185 &amp;amp; 7 &amp;amp; 10.33 &amp;amp; 79707.53\newline
(190,200] &amp;amp; 195 &amp;amp; 2 &amp;amp; 20.33 &amp;amp; 341648.49\newline
\hline
\sum &amp;amp;  &amp;amp; 30 &amp;amp; &amp;amp; 790704.45 \newline
\hline
\end{array}
$$&lt;/p&gt;
&lt;p&gt;$$g_2 = \frac{\sum (x_i-\bar x)^4n_i/n}{s^4} - 3 = \frac{790704.45/30}{10.1^4}-3 = -0.47.$$&lt;/p&gt;
&lt;p&gt;Como se trata de un valor negativo, aunque cercano a 0, podemos decir que la distribución es ligeramente platicúrtica.&lt;/p&gt;
&lt;p&gt;Como se verá más adelante en la parte de inferencia, muchas de las pruebas estadísticas solo pueden aplicarse a poblaciones normales.&lt;/p&gt;
&lt;p&gt;Las poblaciones normales se caracterizan por ser simétricas y mesocúrticas, de manera que, tanto el coeficiente de asimetría como el de apuntamiento pueden utilizarse para contrastar si los datos de la muestra provienen de una población normal.&lt;/p&gt;
&lt;div class=&#34;alert alert-int&#34;&gt;
  &lt;div&gt;
    En general, se suele rechazar la hipótesis de normalidad de la población cuando $g_1$ o $g_2$ estén fuera del intervalo $[-2,2]$.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;En tal caso, lo habitual es aplicar alguna transformación a la variable para corregir la anormalidad.&lt;/p&gt;
&lt;h3 id=&#34;distribuciones-no-normales&#34;&gt;Distribuciones no normales&lt;/h3&gt;
&lt;h4 id=&#34;distribución-asimétrica-a-la-derecha-no-normal&#34;&gt;Distribución asimétrica a la derecha no normal&lt;/h4&gt;
&lt;p&gt;Un ejemplo de distribución asimétrica a la derecha es el ingreso de las familias.&lt;/p&gt;
&lt;img src=&#34;../img/descriptiva/ejemplo_distribucion_asimetrica_derecha.svg&#34; alt=&#34;Distribucion de los ingresos familiares de EEUU&#34; width=&#34;600&#34;&gt;
&lt;h4 id=&#34;distribución-asimétrica-a-la-izquierda-no-normal&#34;&gt;Distribución asimétrica a la izquierda no normal&lt;/h4&gt;
&lt;p&gt;Un ejemplo de distribución asimétrica a la izquierda es la edad de fallecimiento.&lt;/p&gt;
&lt;img src=&#34;../img/descriptiva/ejemplo_distribucion_asimetrica_izquierda.svg&#34; alt=&#34;Distribucion de la esperanza de vida&#34; width=&#34;600&#34;&gt;
&lt;h4 id=&#34;distribución-bimodal-no-normal&#34;&gt;Distribución bimodal no normal&lt;/h4&gt;
&lt;p&gt;Un ejemplo de distribución bimodal es la hora de llegada de los clientes de un restaurante.&lt;/p&gt;
&lt;img src=&#34;../img/descriptiva/ejemplo_distribucion_bimodal.svg&#34; alt=&#34;Distribucion de la hora de llegada de los clientes de un restaurante&#34; width=&#34;600&#34;&gt;
&lt;h2 id=&#34;transformaciones-de-variables&#34;&gt;Transformaciones de variables&lt;/h2&gt;
&lt;p&gt;En muchas ocasiones se suelen transformar los datos brutos para corregir alguna anormalidad de la distribución o simplemente para trabajar con unas unidades más cómodas.&lt;/p&gt;
&lt;p&gt;Por ejemplo, si estamos trabajando con estaturas medidas en metros y tenemos los siguientes valores:&lt;/p&gt;
&lt;p&gt;$$
1.75 \mbox{ m}, 1.65 \mbox{ m}, 1.80 \mbox{ m},
$$&lt;/p&gt;
&lt;p&gt;podemos evitar los decimales multiplicando por 100, es decir, pasando de metros a centímetros:&lt;/p&gt;
&lt;p&gt;$$
175 \mbox{ cm}, 165 \mbox{ cm}, 180 \mbox{ cm},
$$&lt;/p&gt;
&lt;p&gt;Y si queremos reducir la magnitud de los datos podemos restarles a todos el menor de ellos, en este caso, 165cm:&lt;/p&gt;
&lt;p&gt;$$10\mbox{cm}, 0\mbox{cm}, 15\mbox{cm},$$&lt;/p&gt;
&lt;p&gt;Está claro que este conjunto de datos es mucho más sencillo que el original. En el fondo lo que se ha hecho es aplicar a los datos la transformación:&lt;/p&gt;
&lt;p&gt;$$Y= 100X-165$$&lt;/p&gt;
&lt;h3 id=&#34;transformaciones-lineales&#34;&gt;Transformaciones lineales&lt;/h3&gt;
&lt;p&gt;Una de las transformaciones más habituales es la &lt;em&gt;transformación lineal&lt;/em&gt;:&lt;/p&gt;
&lt;p&gt;$$Y=a+bX.$$&lt;/p&gt;
&lt;p&gt;Se puede comprobar fácilmente que la media y la desviación típica de la variable resultante cumplen:&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
\bar y &amp;amp;= a+ b\bar x,\newline
s_{y} &amp;amp;= |b|s_{x}
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;Además, el coeficiente de curtosis no se altera y el de asimetría sólo cambia de signo si $b$ es negativo.&lt;/p&gt;
&lt;h3 id=&#34;transformación-de-tipificación-y-puntuaciones-típicas&#34;&gt;Transformación de tipificación y puntuaciones típicas&lt;/h3&gt;
&lt;p&gt;Una de las transformaciones lineales más habituales es la &lt;em&gt;tipificación&lt;/em&gt;:&lt;/p&gt;
&lt;div class=&#34;alert alert-def&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;&lt;strong&gt;Definición - Variable tipificada&lt;/strong&gt;. La &lt;em&gt;variable tipificada&lt;/em&gt; de una variable estadística $X$ es la variable que resulta de restarle su media y dividir por su desviación típica.&lt;/p&gt;
&lt;p&gt;$$Z=\frac{X-\bar x}{s_{x}}$$&lt;/p&gt;
&lt;p&gt;Para cada valor $x_i$ de la muestra, la &lt;em&gt;puntuación típica&lt;/em&gt; es el valor que resulta de aplicarle la transformación de tipificación&lt;/p&gt;
&lt;p&gt;$$z_i=\frac{x_i-\bar x}{s_{x}}.$$&lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;alert alert-int&#34;&gt;
  &lt;div&gt;
    La puntuación típica es el número de desviaciones típicas que un valor está por encima o por debajo de la media, y es útil para evitar la dependencia de una variable respecto de las unidades de medida empleadas. Esto es útil, por ejemplo, para comparar valores de variables o muestras distintas.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;La variable tipificada siempre tiene media 0 y desviación típica 1.&lt;/p&gt;
&lt;p&gt;$$\bar z = 0 \qquad s_{z} = 1$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Ejemplo&lt;/strong&gt;. Las notas de 5 alumnos en dos asignaturas $X$ e $Y$ son&lt;/p&gt;
&lt;p&gt;$$
\begin{array}{rccccccccc}
\mbox{Alumno:} &amp;amp; 1 &amp;amp; 2 &amp;amp; 3 &amp;amp; 4 &amp;amp; 5\newline
\hline
X: &amp;amp; 2 &amp;amp; 5 &amp;amp; 4 &amp;amp; \color{red} 8 &amp;amp; 6 &amp;amp; \qquad &amp;amp; \bar x = 5 &amp;amp; \quad s_x = 2\newline
Y: &amp;amp; 1 &amp;amp; 9 &amp;amp; \color{red} 8 &amp;amp; 5 &amp;amp; 2 &amp;amp; \qquad &amp;amp; \bar y = 5 &amp;amp; \quad s_y = 3.16\newline
\hline
\end{array}
$$&lt;/p&gt;
&lt;p&gt;&lt;em&gt;¿Ha tenido el mismo rendimiento el cuarto alumno en la asignatura $X$ que el tercero en la asignatura $Y$?&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Podría parecer que ambos alumnos han tenido el mismo rendimiento puesto que tienen la misma nota, pero si queremos ver el rendimiento relativo al resto del grupo, tendríamos que tener en cuenta la dispersión de cada muestra y medir sus puntuaciones típicas:&lt;/p&gt;
&lt;p&gt;$$
\begin{array}{cccccc}
\mbox{Alumno:} &amp;amp; 1 &amp;amp; 2 &amp;amp; 3 &amp;amp; 4 &amp;amp; 5\newline
\hline
X: &amp;amp; -1.50 &amp;amp; 0.00 &amp;amp; -0.50 &amp;amp; \color{red}{1.50} &amp;amp; 0.50 \newline
Y: &amp;amp; -1.26 &amp;amp; 1.26 &amp;amp; \color{red}{0.95} &amp;amp; 0.00 &amp;amp; -0.95\newline
\hline
\end{array}
$$&lt;/p&gt;
&lt;p&gt;Es decir, el alumno que tiene un 8 en $X$ está $1.5$ veces la desviación típica por encima de la media de $X$, mientras que el alumno que tiene un 8 en $Y$ sólo está $0.95$ desviaciones típicas por encima de la media de $Y$. Así pues, el primer alumno tuvo un rendimiento superior al segundo.&lt;/p&gt;
&lt;p&gt;Siguiendo con el ejemplo anterior y considerando ambas asignaturas, &lt;em&gt;¿cuál es el mejor alumno?&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Si simplemente se suman las puntuaciones de cada asignatura se tiene:&lt;/p&gt;
&lt;p&gt;$$\begin{array}{rccccc}
\mbox{Alumno:} &amp;amp; 1 &amp;amp; 2 &amp;amp; 3 &amp;amp; 4 &amp;amp; 5\newline
\hline
X: &amp;amp; 2 &amp;amp; 5 &amp;amp; 4 &amp;amp; 8 &amp;amp; 6 \newline
Y: &amp;amp; 1 &amp;amp; 9 &amp;amp; 8 &amp;amp; 5 &amp;amp; 2 \newline
\hline
\sum &amp;amp; 3 &amp;amp; \color{red}{14} &amp;amp; 12 &amp;amp; 13 &amp;amp; 8
\end{array}
$$&lt;/p&gt;
&lt;p&gt;El mejor alumno sería el segundo.&lt;/p&gt;
&lt;p&gt;Pero si se considera el rendimiento relativo tomando las puntuaciones típicas se tiene&lt;/p&gt;
&lt;p&gt;$$
\begin{array}{rccccc}
\mbox{Alumno:} &amp;amp; 1 &amp;amp; 2 &amp;amp; 3 &amp;amp; 4 &amp;amp; 5\newline
\hline
X: &amp;amp; -1.50 &amp;amp; 0.00 &amp;amp; -0.50 &amp;amp; 1.50 &amp;amp; 0.50 \newline
Y: &amp;amp; -1.26 &amp;amp; 1.26 &amp;amp; 0.95 &amp;amp; 0.00 &amp;amp; -0.95\newline
\hline
\sum &amp;amp; -2.76 &amp;amp; 1.26 &amp;amp; 0.45 &amp;amp; \color{red}{1.5} &amp;amp; -0.45
\end{array}
$$&lt;/p&gt;
&lt;p&gt;Y el mejor alumno sería el cuarto.&lt;/p&gt;
&lt;h4 id=&#34;transformaciones-no-lineales&#34;&gt;Transformaciones no lineales&lt;/h4&gt;
&lt;p&gt;Las transformaciones no lineales son también habituales para corregir la anormalidad de las distribuciones.&lt;/p&gt;
&lt;p&gt;La transformación $Y=X^2$ comprime la escala para valores pequeños y la expande para valores altos, de manera que es muy útil para corregir asimetrías hacia la izquierda.&lt;/p&gt;
&lt;img src=&#34;../img/descriptiva/transformacion_cuadratica.svg&#34; alt=&#34;Transformación cuadrática&#34; width=&#34;600&#34;&gt;
&lt;p&gt;Las transformaciones $Y=\sqrt x$, $Y= \log X$ y $Y=1/X$ comprimen la escala para valores altos y la expanden para valores pequeños, de manera que son útiles para corregir asimetrías hacia la derecha.&lt;/p&gt;
&lt;img src=&#34;../img/descriptiva/transformacion_logaritmica.svg&#34; alt=&#34;Trasformación logarítmica&#34; width=&#34;600&#34;&gt;
&lt;h3 id=&#34;variables-clasificadoras-o-factores&#34;&gt;Variables clasificadoras o factores&lt;/h3&gt;
&lt;p&gt;En ocasiones interesa describir el comportamiento de una variable, no para toda la muestra, sino para distintos grupos de individuos correspondientes a las categorías de otra variable conocida como &lt;strong&gt;variable clasificadora&lt;/strong&gt; o &lt;strong&gt;factor&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Ejemplo&lt;/strong&gt;. Dividiendo la muestra de estaturas según el sexo se obtienen dos submuestras:&lt;/p&gt;
&lt;p&gt;$$
\begin{array}{lll}
\hline
\mbox{Mujeres} &amp;amp; &amp;amp; 173, 158, 174, 166, 162, 177, 165, 154, 166, 182, 169, 172, 170, 168. \newline
\mbox{Hombres} &amp;amp; &amp;amp; 179, 181, 172, 194, 185, 187, 198, 178, 188, 171, 175, 167, 186, 172, 176, 187. \newline
\hline
\end{array}
$$&lt;/p&gt;
&lt;h5 id=&#34;comparación-de-distribuciones-según-los-niveles-de-un-factor&#34;&gt;Comparación de distribuciones según los niveles de un factor&lt;/h5&gt;
&lt;p&gt;Habitualmente los factores se usan para comparar la distribución de la variable principal para cada categoría del factor.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Ejemplo&lt;/strong&gt; Los siguientes diagramas permiten comparar la distribución de estaturas según el sexo.&lt;/p&gt;
&lt;img src=&#34;../img/descriptiva/histograma_estatura_sexo.svg&#34; alt=&#34;Histograma de estaturas por sexo&#34; width=&#34;500&#34;&gt;
&lt;img src=&#34;../img/descriptiva/diagrama_caja_estatura_sexo.svg&#34; alt=&#34;Diagrama de cajas de estaturas por sexo&#34; width=&#34;500&#34;&gt;
</description>
    </item>
    
    <item>
      <title>Regresión</title>
      <link>/docencia/estadistica/manual/regresion/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/docencia/estadistica/manual/regresion/</guid>
      <description>&lt;p&gt;Hasta ahora se ha visto como describir el comportamiento de una variable, pero en los fenómenos naturales normalmente aparecen más de una variable que suelen estar relacionadas. Por ejemplo, en un estudio sobre el peso de las personas, deberíamos incluir todas las variables con las que podría tener relación: altura, edad, sexo, dieta, tabaco,
ejercicio físico, etc.&lt;/p&gt;
&lt;p&gt;Para comprender el fenómeno no basta con estudiar cada variable por separado y es preciso un estudio conjunto de todas las variables para ver cómo interactúan y qué relaciones se dan entre ellas. El objetivo de la estadística en este caso es dar medidas del grado y del tipo de relación entre dichas variables.&lt;/p&gt;
&lt;p&gt;Generalmente, en un &lt;em&gt;estudio de dependencia&lt;/em&gt; se considera una &lt;strong&gt;variable dependiente&lt;/strong&gt; $Y$ que se supone relacionada con otras variables $X_1,\ldots,X_n$ llamadas &lt;strong&gt;variables independientes&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;El caso más simple es el de una sola variable independiente, y en tal caso se habla de &lt;em&gt;estudio de dependencia simple&lt;/em&gt;. Para más de una
variable independiente se habla de &lt;em&gt;estudio de dependencia múltiple&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;En este capítulo se verán los estudios de dependencia simple que son más sencillos.&lt;/p&gt;
&lt;h2 id=&#34;distribución-de-frecuencias-conjunta&#34;&gt;Distribución de frecuencias conjunta&lt;/h2&gt;
&lt;h3 id=&#34;frecuencias-conjuntas&#34;&gt;Frecuencias conjuntas&lt;/h3&gt;
&lt;p&gt;Al estudiar la dependencia simple entre dos variables $X$ e $Y$, no se pueden estudiar sus distribuciones por separado, sino que hay que estudiar la distribución conjunta de la &lt;strong&gt;variable bidimensional&lt;/strong&gt; $(X,Y)$, cuyos valores son los pares $(x_i,y_j)$ donde el primer elemento es un valor $X$ y el segundo uno de $Y$.&lt;/p&gt;
&lt;div class=&#34;alert alert-def&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;&lt;strong&gt;Definición - Frecuencias muestrales conjuntas&lt;/strong&gt;. Dada una muestra de tamaño $n$ de una variable bidimensional $(X,Y)$, para cada valor de la variable $(x_i,y_j)$ observado en la muestra se define&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Frecuencia absoluta&lt;/strong&gt; $n_{ij}$: Es el número de veces que el par $(x_i,y_j)$ aparece en la muestra.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Frecuencia relativa&lt;/strong&gt; $f_{ij}$: Es la proporción de veces que el par $(x_i,y_j)$ aparece en la muestra.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$f_{ij}=\frac{n_{ij}}{n}$$&lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;alert alert-warning&#34;&gt;
  &lt;div&gt;
    Para las variables bidimensionales no tienen sentido las frecuencias acumuladas.
  &lt;/div&gt;
&lt;/div&gt;
&lt;h3 id=&#34;distribución-de-frecuencias-bidimensional&#34;&gt;Distribución de frecuencias bidimensional&lt;/h3&gt;
&lt;p&gt;Al conjunto de valores de la variable bidimensional y sus respectivas frecuencias muestrales se le denomina &lt;strong&gt;distribución de frecuencias bidimensional&lt;/strong&gt;, y se representa mediante una &lt;strong&gt;tabla de frecuencias bidimensional&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;$$\begin{array}{|c|ccccc|}
\hline
X\backslash Y &amp;amp; y_1 &amp;amp; \cdots &amp;amp; y_j &amp;amp; \cdots &amp;amp; y_q\newline
\hline
x_1 &amp;amp; n_{11} &amp;amp; \cdots &amp;amp; n_{1j} &amp;amp; \cdots &amp;amp; n_{1q}\newline
\vdots &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \vdots\newline
x_i &amp;amp; n_{i1} &amp;amp; \cdots &amp;amp; n_{ij} &amp;amp; \cdots &amp;amp; n_{iq}\newline
\vdots &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \vdots\newline
x_p &amp;amp; n_{p1} &amp;amp; \cdots &amp;amp; n_{pj} &amp;amp; \cdots &amp;amp; n_{pq}\newline
\hline
\end{array}$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Ejemplo (datos agrupados)&lt;/strong&gt;. La estatura (en cm) y el peso (en Kg) de una muestra de 30 estudiantes es:&lt;/p&gt;
&lt;div style=&#34;text-align:center&#34;&gt;
(179,85), (173,65), (181,71), (170,65), (158,51), (174,66),&lt;br/&gt;
(172,62), (166,60), (194,90), (185,75), (162,55), (187,78),&lt;br/&gt;
(198,109), (177,61), (178,70), (165,58), (154,50), (183,93),&lt;br/&gt;
(166,51), (171,65), (175,70), (182,60), (167,59), (169,62),&lt;br/&gt;
(172,70), (186,71), (172,54), (176,68),(168,67), (187,80).
&lt;/div&gt;
&lt;p&gt;La tabla de frecuencias bidimensional es&lt;/p&gt;
&lt;p&gt;$$\begin{array}{|c||c|c|c|c|c|c|}
\hline
X/Y &amp;amp; [50,60) &amp;amp; [60,70) &amp;amp; [70,80) &amp;amp; [80,90) &amp;amp; [90,100) &amp;amp; [100,110) \newline
\hline\hline
(150,160] &amp;amp; 2 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 \newline
\hline
(160,170] &amp;amp; 4 &amp;amp; 4 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 \newline
\hline
(170,180] &amp;amp; 1 &amp;amp; 6 &amp;amp; 3 &amp;amp; 1 &amp;amp; 0 &amp;amp; 0 \newline
\hline
(180,190] &amp;amp; 0 &amp;amp; 1 &amp;amp; 4 &amp;amp; 1 &amp;amp; 1 &amp;amp; 0 \newline
\hline
(190,200] &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 1 &amp;amp; 1 \newline
\hline
\end{array}$$&lt;/p&gt;
&lt;h3 id=&#34;diagrama-de-dispersión&#34;&gt;Diagrama de dispersión&lt;/h3&gt;
&lt;p&gt;La distribución de frecuencias conjunta de una variable bidimensional puede representarse gráficamente mediante un &lt;strong&gt;diagrama de dispersión&lt;/strong&gt;, donde los datos se representan como una colección de puntos en un plano cartesiano.&lt;/p&gt;
&lt;p&gt;Habitualmente la variable independiente se representa en el eje $X$ y la variable dependiente en el eje $Y$. Por cada par de valores $(x_i,y_j)$ en la muestra se dibuja un punto en el plano con esas coordenadas.&lt;/p&gt;
&lt;img src=&#34;../img/regresion/diagrama_dispersion.svg&#34; alt=&#34;Diagrama de dispersión&#34; width=&#34;300&#34;&gt;
&lt;p&gt;El resultado es un conjunto de puntos que se conoce como &lt;em&gt;nube de puntos&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Ejemplo&lt;/strong&gt;. El siguiente diagrama de dispersión representa la distribución conjunta de estaturas y pesos de la muestra anterior.&lt;/p&gt;
&lt;img src=&#34;../img/regresion/diagrama_dispersion_estatura_peso.svg&#34; alt=&#34;Diagrama de dispersión de estaturas y pesos&#34; width=&#34;600&#34;&gt;
&lt;div class=&#34;alert alert-int&#34;&gt;
  &lt;div&gt;
    El diagrama de dispersión da información visual sobre el tipo de relación entre las variables.&lt;/p&gt;
&lt;img src=&#34;../img/regresion/diagrama_dispersion_tipos_relaciones.svg&#34; alt=&#34;Diagramas de dispersión de diferentes tipos de relaciones&#34; width=&#34;700&#34;&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;h3 id=&#34;distribuciones-marginales&#34;&gt;Distribuciones marginales&lt;/h3&gt;
&lt;p&gt;A cada una de las distribuciones de las variables que conforman la
variable bidimensional se les llama .&lt;/p&gt;
&lt;p&gt;Las distribuciones marginales se pueden obtener a partir de la tabla de
frecuencias bidimensional, sumando las frecuencias por filas y columnas.&lt;/p&gt;
&lt;p&gt;$$
\begin{array}{|c|ccccc|c|}
\hline
X\backslash Y &amp;amp; y_1 &amp;amp; \cdots &amp;amp; y_j &amp;amp; \cdots &amp;amp; y_q &amp;amp; \color{red}{n_x}\newline
\hline
x_1 &amp;amp; n_{11} &amp;amp; \cdots &amp;amp; n_{1j} &amp;amp; \cdots &amp;amp; n_{1q} &amp;amp; \color{red}{n_{x_1}}\newline
\vdots &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \downarrow + &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \color{red}{\vdots} \newline
x_i &amp;amp; n_{i1} &amp;amp; \stackrel{+}{\rightarrow} &amp;amp; n_{ij} &amp;amp; \stackrel{+}{\rightarrow} &amp;amp; n_{iq} &amp;amp; \color{red}{n_{x_i}}\newline
\vdots &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \downarrow +  &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \color{red}{\vdots}\newline
x_p &amp;amp; n_{p1} &amp;amp; \cdots &amp;amp; n_{pj} &amp;amp; \cdots &amp;amp; n_{pq} &amp;amp; \color{red}{n_{x_p}} \newline
\hline
\color{red}{n_y} &amp;amp; \color{red}{n_{y_1}} &amp;amp; \color{red}{\cdots} &amp;amp; \color{red}{n_{y_j}} &amp;amp; \color{red}{\cdots} &amp;amp; \color{red}{n_{y_q}} &amp;amp; n\newline
\hline
\end{array}
$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Ejemplo&lt;/strong&gt;. En el ejemplo anterior de las estaturas y los pesos, las distribuciones marginales son&lt;/p&gt;
&lt;p&gt;$$
\begin{array}{|c||c|c|c|c|c|c|c|}
\hline
X/Y &amp;amp; [50,60) &amp;amp; [60,70) &amp;amp; [70,80) &amp;amp; [80,90) &amp;amp; [90,100) &amp;amp; [100,110) &amp;amp; \color{red}{n_x}\newline
\hline\hline
(150,160] &amp;amp; 2 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; \color{red}{2}\newline
\hline
(160,170] &amp;amp; 4 &amp;amp; 4 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; \color{red}{8}\newline
\hline
(170,180] &amp;amp; 1 &amp;amp; 6 &amp;amp; 3 &amp;amp; 1 &amp;amp; 0 &amp;amp; 0 &amp;amp; \color{red}{11} \newline
\hline
(180,190] &amp;amp; 0 &amp;amp; 1 &amp;amp; 4 &amp;amp; 1 &amp;amp; 1 &amp;amp; 0 &amp;amp; \color{red}{7} \newline
\hline
(190,200] &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 1 &amp;amp; 1 &amp;amp; \color{red}{2}\newline
\hline
\color{red}{n_y} &amp;amp; \color{red}{7} &amp;amp; \color{red}{11} &amp;amp; \color{red}{7} &amp;amp; \color{red}{2} &amp;amp; \color{red}{2} &amp;amp; \color{red}{1} &amp;amp; 30\newline
\hline
\end{array}
$$&lt;/p&gt;
&lt;p&gt;y los estadísticos correspondientes son&lt;/p&gt;
&lt;p&gt;$$
\begin{array}{lllll}
\bar x = 174.67 \mbox{ cm} &amp;amp; \quad &amp;amp; s^2_x = 102.06 \mbox{ cm}^2 &amp;amp; \quad &amp;amp; s_x = 10.1 \mbox{ cm}\newline
\bar y = 69.67 \mbox{ Kg} &amp;amp; &amp;amp; s^2_y = 164.42 \mbox{ Kg}^2 &amp;amp; &amp;amp; s_y = 12.82 \mbox{ Kg}
\end{array}
$$&lt;/p&gt;
&lt;h2 id=&#34;covarianza&#34;&gt;Covarianza&lt;/h2&gt;
&lt;p&gt;Para analizar la relación entre dos variables cuantitativas es importante hacer un estudio conjunto de las desviaciones respecto de la media de cada variable.&lt;/p&gt;
&lt;img src=&#34;../img/regresion/desviaciones_media.svg&#34; alt=&#34;Desviaciones de las medias en un diagrama de dispersión&#34; width=&#34;600&#34;&gt;
&lt;p&gt;Si dividimos la nube de puntos del diagrama de dispersión en 4 cuadrantes centrados en el punto de medias $(\bar x, \bar y)$, el signo de las desviaciones será:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;&lt;strong&gt;Cuadrante&lt;/strong&gt;&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;$(x_i-\bar x)$&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;$(y_j-\bar y)$&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;$(x_i-\bar x)(y_j-\bar y)$&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;1&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$+$&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$+$&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$+$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;2&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$-$&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$+$&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$-$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;3&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$-$&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$-$&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$+$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;4&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$+$&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$-$&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$-$&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;img src=&#34;../img/regresion/cuadrantes_diagrama_dispersion.svg&#34; alt=&#34;Cuadrantes de un diagrama de dispersión&#34; width=&#34;400&#34;&gt;
&lt;p&gt;Si la relación entre las variables es &lt;em&gt;lineal y creciente&lt;/em&gt;, entonces la mayor parte de los puntos estarán en los cuadrantes 1 y 3 y la suma de los productos de desviaciones será positiva.&lt;/p&gt;
&lt;p&gt;$$\sum(x_i-\bar x)(y_j-\bar y) &amp;gt; 0$$&lt;/p&gt;
&lt;img src=&#34;../img/regresion/diagrama_dispersion_lineal_creciente.svg&#34; alt=&#34;Diagrama de dispersión de una relación lineal creciente&#34; width=&#34;500&#34;&gt;
&lt;p&gt;Si la relación entre las variables es &lt;em&gt;lineal y decreciente&lt;/em&gt;, entonces la mayor parte de los puntos estarán en los cuadrantes 2 y 4 y la suma de los productos de desviaciones será negativa.&lt;/p&gt;
&lt;p&gt;$$\sum(x_i-\bar x)(y_j-\bar y) = -$$&lt;/p&gt;
&lt;img src=&#34;../img/regresion/diagrama_dispersion_lineal_decreciente.svg&#34; alt=&#34;Diagrama de dispersión de una relación lineal decreciente&#34; width=&#34;500&#34;&gt;
&lt;p&gt;Usando el producto de las desviaciones respecto de las medias surge el
siguiente estadístico.&lt;/p&gt;
&lt;div class=&#34;alert alert-def&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;&lt;strong&gt;Definición - Covarianza muestral&lt;/strong&gt;. La &lt;em&gt;covarianza muestral&lt;/em&gt; de una variable aleatoria bidimensional $(X,Y)$ se define como el promedio de los productos de las respectivas desviaciones respecto de las medias de $X$ e $Y$.&lt;/p&gt;
&lt;p&gt;$$s_{xy}=\frac{\sum (x_i-\bar x)(y_j-\bar y)n_{ij}}{n}$$&lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;También puede calcularse de manera más sencilla mediante la fórmula&lt;/p&gt;
&lt;p&gt;$$s_{xy}=\frac{\sum x_iy_jn_{ij}}{n}-\bar x\bar y.$$&lt;/p&gt;
&lt;div class=&#34;alert alert-int&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;La covarianza sirve para estudiar la relación lineal entre dos variables:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Si $s_{xy}&amp;gt;0$ existe una relación lineal creciente.&lt;/li&gt;
&lt;li&gt;Si $s_{xy}&amp;lt;0$ existe una relación lineal decreciente.&lt;/li&gt;
&lt;li&gt;Si $s_{xy}=0$ no existe relación lineal.&lt;/li&gt;
&lt;/ul&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Ejemplo&lt;/strong&gt;. Utilizando la tabla de frecuencias bidimensional de la muestra de estaturas y pesos&lt;/p&gt;
&lt;p&gt;$$
\begin{array}{|c||c|c|c|c|c|c|c|}
\hline
X/Y &amp;amp; [50,60) &amp;amp; [60,70) &amp;amp; [70,80) &amp;amp; [80,90) &amp;amp; [90,100) &amp;amp; [100,110) &amp;amp; n_x\newline
\hline\hline
(150,160] &amp;amp; 2 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 2\newline
\hline
(160,170] &amp;amp; 4 &amp;amp; 4 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 8\newline
\hline
(170,180] &amp;amp; 1 &amp;amp; 6 &amp;amp; 3 &amp;amp; 1 &amp;amp; 0 &amp;amp; 0 &amp;amp; 11 \newline
\hline
(180,190] &amp;amp; 0 &amp;amp; 1 &amp;amp; 4 &amp;amp; 1 &amp;amp; 1 &amp;amp; 0 &amp;amp; 7 \newline
\hline
(190,200] &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 1 &amp;amp; 1 &amp;amp; 2\newline
\hline
n_y &amp;amp; 7 &amp;amp; 11 &amp;amp; 7 &amp;amp; 2 &amp;amp; 2 &amp;amp; 1 &amp;amp; 30\newline
\hline
\end{array}
$$&lt;/p&gt;
&lt;p&gt;$$\bar x = 174.67 \mbox{ cm} \qquad \bar y = 69.67 \mbox{ Kg}$$&lt;/p&gt;
&lt;p&gt;la covarianza vale&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
s_{xy} &amp;amp;=\frac{\sum x_iy_jn_{ij}}{n}-\bar x\bar y =  \frac{155\cdot 55\cdot 2 + 165\cdot 55\cdot 4 + \cdots + 195\cdot 105\cdot 1}{30}-174.67\cdot 69.67 =\newline
&amp;amp; = \frac{368200}{30}-12169.26 = 104.07 \mbox{ cm$\cdot$ Kg}.
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;Esto indica que existe una relación lineal creciente entre la estatura y el peso.&lt;/p&gt;
&lt;h2 id=&#34;regresión&#34;&gt;Regresión&lt;/h2&gt;
&lt;p&gt;En muchos casos el objetivo de un estudio no es solo detectar una relación entre dos variables, sino explicarla mediante alguna función matemática $$y=f(x)$$ que permita predecir la variable dependiente para cada valor de la independiente.&lt;/p&gt;
&lt;p&gt;La &lt;strong&gt;regresión&lt;/strong&gt; es la parte de la Estadística encargada de construir esta función, que se conoce como &lt;strong&gt;función de regresión&lt;/strong&gt; o &lt;strong&gt;modelo de regresión&lt;/strong&gt;.&lt;/p&gt;
&lt;h3 id=&#34;modelos-de-regresión-simple&#34;&gt;Modelos de regresión simple&lt;/h3&gt;
&lt;p&gt;Dependiendo de la forma de función de regresión, existen muchos tipos de
regresión simple. Los más habituales son los que aparecen en la
siguiente tabla:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;&lt;strong&gt;Modelo&lt;/strong&gt;&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;&lt;strong&gt;Ecuación&lt;/strong&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Lineal&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$y=a+bx$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Cuadrático&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$y=a+bx+cx^2$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Cúbico&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$y=a+bx+cx^2+dx^3$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Potencial&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$y=a\cdot x^b$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Exponencial&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$y=e^{a+bx}$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Logarítmico&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$y=a+b\log x$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Inverso&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$y=a+\frac{b}{x}$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Sigmoidal&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$y=e^{a+\frac{b}{x}}$&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;La elección de un tipo u otro depende de la forma que tenga la nube de puntos del diagrama de dispersión.&lt;/p&gt;
&lt;h3 id=&#34;residuos-o-errores-predictivos&#34;&gt;Residuos o errores predictivos&lt;/h3&gt;
&lt;p&gt;Una vez elegida la familia de curvas que mejor se adapta a la nube de
puntos, se determina, dentro de dicha familia, la curva que mejor se
ajusta a la distribución, es decir, la función que mejor predice la variable dependiente.&lt;/p&gt;
&lt;p&gt;El objetivo es encontrar la función de regresión que haga mínimas las
distancias entre los valores de la variable dependiente observados en la
muestra, y los predichos por la función de regresión. Estas distancias
se conocen como &lt;em&gt;residuos&lt;/em&gt; o &lt;em&gt;errores predictivos&lt;/em&gt;.&lt;/p&gt;
&lt;div class=&#34;alert alert-def&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;&lt;strong&gt;Definición - Residuos o errores predictivos&lt;/strong&gt;. Dado el modelo de regresión $y=f(x)$ para una variable bidimensional $(X,Y)$, el &lt;em&gt;residuo&lt;/em&gt; o &lt;em&gt;error predictivo&lt;/em&gt; de un valor $(x_i,y_j)$ observado en la muestra, es la diferencia entre el valor observado de la variable dependiente $y_j$ y el predicho por la función de regresión para $x_i$,&lt;/p&gt;
&lt;p&gt;$$e_{ij} = y_j-f(x_i).$$&lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;img src=&#34;../img/regresion/residuos_y.svg&#34; alt=&#34;Residuos de un modelo de regresión&#34; width=&#34;600&#34;&gt;
&lt;h3 id=&#34;ajuste-de-mínimos-cuadrados&#34;&gt;Ajuste de mínimos cuadrados&lt;/h3&gt;
&lt;p&gt;Una forma posible de obtener la función de regresión es mediante el método de &lt;em&gt;mínimos cuadrados&lt;/em&gt; que consiste en calcular la función que haga mínima la suma de los cuadrados de los residuos&lt;/p&gt;
&lt;p&gt;$$\sum e_{ij}^2.$$&lt;/p&gt;
&lt;p&gt;En el caso de un modelo de regresión lineal $f(x) = a + bx$, como la recta depende de dos parámetros (el término independiente $a$ y la pendiente $b$), la suma también dependerá de estos parámetros&lt;/p&gt;
&lt;p&gt;$$\theta(a,b) = \sum e_{ij}^2 =\sum (y_j - f(x_i))^2 =\sum (y_j-a-bx_i)^2.$$&lt;/p&gt;
&lt;p&gt;Así pues, todo se reduce a buscar los valores $a$ y $b$ que hacen mínima esta suma.&lt;/p&gt;
&lt;p&gt;Considerando la suma de los cuadrados de los residuos como una función de dos variables $\theta(a,b)$, se pueden calcular los valores de los parámetros del modelo que hacen mínima esta suma derivando e igualando a 0 las derivadas con respecto a $a$ y $b$.&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
\frac{\partial \theta(a,b)}{\partial a} &amp;amp;=  \frac{\partial \sum (y_j-a-bx_i)^2 }{\partial a} =0\newline
\frac{\partial \theta(a,b)}{\partial b} &amp;amp;=  \frac{\partial \sum (y_j-a-bx_i)^2 }{\partial b} =0
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;Tras resolver el sistema se obtienen los valores&lt;/p&gt;
&lt;p&gt;$$a= \bar y - \frac{s_{xy}}{s_x^2}\bar x \qquad b=\frac{s_{xy}}{s_x^2}$$&lt;/p&gt;
&lt;p&gt;Estos valores hacen mínimos los residuos en $Y$ y por tanto dan la recta
de regresión óptima.&lt;/p&gt;
&lt;h2 id=&#34;recta-de-regresión&#34;&gt;Recta de regresión&lt;/h2&gt;
&lt;div class=&#34;alert alert-def&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;&lt;strong&gt;Definición - Recta de regresión&lt;/strong&gt;. Dada una variable bidimensional $(X,Y)$, la &lt;em&gt;recta de regresión&lt;/em&gt; de $Y$ sobre $X$ es&lt;/p&gt;
&lt;p&gt;$$y = \bar y +\frac{s_{xy}}{s_x^2}(x-\bar x).$$&lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;alert alert-int&#34;&gt;
  &lt;div&gt;
    La recta de regresión de $Y$ sobre $X$ es la recta que hace mínimos los errores predictivos en $Y$, y por tanto es la recta que hará mejores predicciones de $Y$ para cualquier valor de $X$.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Ejemplo&lt;/strong&gt;. Utilizando la muestra anterior de estaturas ($X$) y pesos ($Y$) con los siguientes estadísticos&lt;/p&gt;
&lt;p&gt;$$
\begin{array}{lllll}
\bar x = 174.67 \mbox{ cm} &amp;amp; \quad &amp;amp; s^2_x = 102.06 \mbox{ cm}^2 &amp;amp; \quad &amp;amp; s_x = 10.1 \mbox{ cm}\newline
\bar y = 69.67 \mbox{ Kg} &amp;amp; &amp;amp; s^2_y = 164.42 \mbox{ Kg}^2 &amp;amp; &amp;amp; s_y = 12.82 \mbox{ Kg}\newline
&amp;amp; &amp;amp; s_{xy} = 104.07 \mbox{ cm$\cdot$ Kg} &amp;amp; &amp;amp;
\end{array}
$$&lt;/p&gt;
&lt;p&gt;la recta de regresión del peso sobre la estatura es&lt;/p&gt;
&lt;p&gt;$$y = \bar y +\frac{s_{xy}}{s_x^2}(x-\bar x) = 69.67+\frac{104.07}{102.06}(x-174.67) =  -108.49 + 1.02 x.$$&lt;/p&gt;
&lt;p&gt;De igual modo, si tomamos la estatura como variable dependiente, la
recta de regresión de la estatura sobre el peso es&lt;/p&gt;
&lt;p&gt;$$x = \bar x +\frac{s_{xy}}{s_y^2}(y-\bar y) = 174.67+\frac{104.07}{164.42}(y-69.67) = +130.78 + 0.63 y.$$&lt;/p&gt;
&lt;div class=&#34;alert alert-warning&#34;&gt;
  &lt;div&gt;
    ¡Obsérvese que ambas rectas de regresión son diferentes!
  &lt;/div&gt;
&lt;/div&gt;
&lt;img src=&#34;../img/regresion/rectas_regresion.svg&#34; alt=&#34;Rectas de regresión de estaturas y pesos&#34; width=&#34;600&#34;&gt;
&lt;h3 id=&#34;posición-relativa-de-las-rectas-de-regresión&#34;&gt;Posición relativa de las rectas de regresión&lt;/h3&gt;
&lt;p&gt;Habitualmente, las rectas de regresión $Y$ sobre $X$ y de $X$ sobre $Y$ no coinciden, pero siempre se cortan en el punto de medias $(\bar x,\bar y)$.&lt;/p&gt;
&lt;p&gt;Si entre las variables la relación lineal es perfecta, entonces ambas rectas coinciden ya que esa recta hace tanto los residuos en $X$ como los residuos en $Y$ nulos.&lt;/p&gt;
&lt;img src=&#34;../img/regresion/regresion_lineal_perfecta.svg&#34; alt=&#34;Recta de regresión de una relación lineal perfecta&#34; width=&#34;500&#34;&gt;
&lt;p&gt;Si no hay relación lineal, entonces las ecuaciones de las rectas son constantes e iguales a las respectivas medias,&lt;/p&gt;
&lt;p&gt;$$y = \bar y,\quad x = \bar x,$$&lt;/p&gt;
&lt;p&gt;y se cortan perpendicularmente.&lt;/p&gt;
&lt;img src=&#34;../img/regresion/rectas_independencia_lineal.svg&#34; alt=&#34;Rectas de regresión de dos variables linealmente independientes&#34; width=&#34;500&#34;&gt;
&lt;h3 id=&#34;coeficiente-de-regresión&#34;&gt;Coeficiente de regresión&lt;/h3&gt;
&lt;p&gt;El parámetro más importante de una recta de regresión es su pendiente.&lt;/p&gt;
&lt;div class=&#34;alert alert-def&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;&lt;strong&gt;Definición - Coeficiente de regresión&lt;/strong&gt; $b_{yx}$. Dada una variable bidimensional $(X,Y)$, el &lt;em&gt;coeficiente de regresión&lt;/em&gt; de la recta de regresión de $Y$ sobre $X$ es su pendiente,&lt;/p&gt;
&lt;p&gt;$$b_{yx} = \frac{s_{xy}}{s_x^2}$$&lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;alert alert-warning&#34;&gt;
  &lt;div&gt;
    El coeficiente de regresión siempre tiene el mismo signo que la covarianza.
  &lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;alert alert-int&#34;&gt;
  &lt;div&gt;
    Refleja el crecimiento de la variable dependiente en relación a la independiente según la recta de regresión. En concreto da el número de unidades que aumenta o disminuye la variable dependiente por cada unidad que aumenta la variable independiente.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Ejemplo&lt;/strong&gt;. En el ejemplo de las estaturas y los pesos, la recta de regresión del
peso sobre la estatura era&lt;/p&gt;
&lt;p&gt;$$y=-108.49 +1.02 x,$$&lt;/p&gt;
&lt;p&gt;de manera que el coeficiente de regresión del peso sobre la estatura es&lt;/p&gt;
&lt;p&gt;$$b_{yx}= 1.02 \mbox{Kg/cm.}$$&lt;/p&gt;
&lt;p&gt;Esto significa que, según la recta de regresión del peso sobre la estatura, por cada cm más de estatura, la persona pesará $1.02$ Kg más.&lt;/p&gt;
&lt;h3 id=&#34;predicciones-con-las-rectas-de-regresión&#34;&gt;Predicciones con las rectas de regresión&lt;/h3&gt;
&lt;p&gt;Las rectas de regresión, y en general cualquier modelo de regresión, suele utilizarse con fines predictivos.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Ejemplo&lt;/strong&gt;. En la muestra de las estaturas y los pesos, si se quiere predecir
el peso de una persona que mide 180 cm, se debe utilizar la recta de regresión del peso sobre la estatura,&lt;/p&gt;
&lt;p&gt;$$y = 1.02 \cdot 180 -108.49 = 75.11 \mbox{ Kg}.$$&lt;/p&gt;
&lt;p&gt;Y si se quiere predecir la estatura de una persona que pesa 79 Kg, se debe utilizar la recta de regresión de la estatura sobre el peso,&lt;/p&gt;
&lt;p&gt;$$x = 0.63\cdot 79+ 130.78 = 180.55 \mbox{ cm}.$$&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Ahora bien, ¿qué fiabilidad tienen estas predicciones?&lt;/em&gt;&lt;/p&gt;
&lt;h2 id=&#34;correlación&#34;&gt;Correlación&lt;/h2&gt;
&lt;p&gt;Una vez construido un modelo de regresión, para saber si se trata de un buen modelo predictivo, se tiene que analizar el grado de dependencia entre las variables según el tipo de dependencia planteada en el modelo.
De ello se encarga la parte de la estadística conocida como &lt;strong&gt;correlación&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;La correlación se basa en el estudio de los residuos: cuanto menores sean éstos, más se ajustará la curva de regresión a los puntos, y más intensa será la correlación.&lt;/p&gt;
&lt;h3 id=&#34;varianza-residual-muestral&#34;&gt;Varianza residual muestral&lt;/h3&gt;
&lt;p&gt;Una medida de la bondad del ajuste del modelo de regresión es la
&lt;em&gt;varianza residual&lt;/em&gt;.&lt;/p&gt;
&lt;div class=&#34;alert alert-def&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;&lt;strong&gt;Definición - Varianza residual muestral&lt;/strong&gt; $s_{ry}^2$. Dado un modelo de regresión simple $y=f(x)$ de una variable bidimensional $(X,Y)$, su &lt;em&gt;varianza residual muestral&lt;/em&gt; es el promedio de los cuadrados de los residuos para los valores de la muestra,&lt;/p&gt;
&lt;p&gt;$$s_{ry}^2 = \frac{\sum e_{ij}^2n_{ij}}{n} = \frac{\sum (y_j - f(x_i))^2n_{ij}}{n}.$$&lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;alert alert-int&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;Cuanto más alejados estén los puntos de la curva de regresión, mayor será la varianza residual y menor la dependencia.&lt;/p&gt;
&lt;p&gt;Cuando la relación lineal es perfecta los residuos se anulan y la varianza residual vale cero. Por contra, cuando no existe relación, los residuos coinciden con las desviaciones de la media, y la varianza residual es igual a la varianza de la variable dependiente.&lt;/p&gt;
&lt;p&gt;$$0\leq s_{ry}^2\leq s_y^2$$&lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;h3 id=&#34;descomposición-de-la-variabilidad-total-variabilidad-explicada-y-no-explicada&#34;&gt;Descomposición de la variabilidad total: Variabilidad explicada y no explicada&lt;/h3&gt;
&lt;img src=&#34;../img/regresion/variation_decomposition.gif&#34; alt=&#34;Descomposición de la variabilidad de un modelo de regresión&#34; width=&#34;600&#34;&gt;
&lt;h3 id=&#34;coeficiente-de-determinación&#34;&gt;Coeficiente de determinación&lt;/h3&gt;
&lt;p&gt;A partir de la varianza residual se puede definir otro estadístico más sencillo de interpretar.&lt;/p&gt;
&lt;div class=&#34;alert alert-def&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;&lt;strong&gt;Definición - Coeficiente de determinación muestral $r^2$&lt;/strong&gt;. Dado un modelo de regresión simple $y=f(x)$ de una variable bidimensional $(X,Y)$, su &lt;em&gt;coeficiente de determinación muestral&lt;/em&gt; es&lt;/p&gt;
&lt;p&gt;$$r^2 = 1- \frac{s_{ry}^2}{s_y^2}$$&lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;alert alert-warning&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;Como la varianza residual puede tomar valores entre 0 y $s_y^2$, se tiene que&lt;/p&gt;
&lt;p&gt;$$0\leq r^2\leq 1$$&lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;alert alert-int&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;Cuanto mayor sea $r^2$, mejor explicará el modelo de regresión la relación entre las variables, en particular:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Si $r^2 =0$ entonces no existe relación del tipo planteado por el modelo.&lt;/li&gt;
&lt;li&gt;Si $r^2=1$ entonces la relación que plantea el modelo es perfecta.&lt;/li&gt;
&lt;/ul&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;alert alert-warning&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;En el caso de las rectas de regresión, el coeficiente de determinación puede calcularse con esta fórmula&lt;/p&gt;
&lt;p&gt;$$ r^2 =  \frac{s_{xy}^2}{s_x^2s_y^2}.$$&lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;spoiler &#34; &gt;
  &lt;p&gt;
    &lt;a class=&#34;btn btn-primary&#34; data-toggle=&#34;collapse&#34; href=&#34;#spoiler-18&#34; role=&#34;button&#34; aria-expanded=&#34;false&#34; aria-controls=&#34;spoiler-18&#34;&gt;
      Demostración
    &lt;/a&gt;
  &lt;/p&gt;
  &lt;div class=&#34;collapse card &#34; id=&#34;spoiler-18&#34;&gt;
    &lt;div class=&#34;card-body&#34;&gt;
      &lt;p&gt;Cuando el modelo ajustado es la recta de regresión la varianza residual vale&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
s_{ry}^2 &amp;amp; = \sum e_{ij}^2f_{ij} = \sum (y_j - f(x_i))^2f_{ij} = \sum \left(y_j - \bar y -\frac{s_{xy}}{s_x^2}(x_i-\bar x) \right)^2f_{ij}=\newline
&amp;amp; = \sum \left((y_j - \bar y)^2 +\frac{s_{xy}^2}{s_x^4}(x_i-\bar x)^2 - 2\frac{s_{xy}}{s_x^2}(x_i-\bar x)(y_j -\bar y)\right)f_{ij} =\newline
&amp;amp; = \sum (y_j - \bar y)^2f_{ij} +\frac{s_{xy}^2}{s_x^4}\sum (x_i-\bar x)^2f_{ij}- 2\frac{s_{xy}}{s_x^2}\sum (x_i-\bar x)(y_j -\bar y)f_{ij}=\newline
&amp;amp; = s_y^2 + \frac{s_{xy}^2}{s_x^4}s_x^2 - 2 \frac{s_{xy}}{s_x^2}s_{xy} = s_y^2 - \frac{s_{xy}^2}{s_x^2}.
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;y, por tanto, el coeficiente de determinación lineal vale&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
r^2 &amp;amp;= 1- \frac{s_{ry}^2}{s_y^2} = 1- \frac{s_y^2 - \frac{s_{xy}^2}{s_x^2}}{s_y^2} = 1 - 1 + \frac{s_{xy}^2}{s_x^2s_y^2} = \frac{s_{xy}^2}{s_x^2s_y^2}.
\end{aligned}
$$&lt;/p&gt;

    &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Ejemplo&lt;/strong&gt;. En el ejemplo de las estaturas y pesos se tenía&lt;/p&gt;
&lt;p&gt;$$
\begin{array}{lll}
\bar x = 174.67 \mbox{ cm} &amp;amp; \quad &amp;amp; s^2_x = 102.06 \mbox{ cm}^2\newline
\bar y = 69.67 \mbox{ Kg} &amp;amp; &amp;amp; s^2_y = 164.42 \mbox{ Kg}^2\newline
s_{xy} = 104.07 \mbox{ cm$\cdot$ Kg}
\end{array}
$$&lt;/p&gt;
&lt;p&gt;De modo que el coeficiente de determinación lineal vale&lt;/p&gt;
&lt;p&gt;$$r^2 = \frac{s_{xy}^2}{s_x^2s_y^2} = \frac{(104.07 \mbox{ cm\cdot Kg})^2}{102.06 \mbox{ cm}^2 \cdot 164.42 \mbox{ Kg}^2} = 0.65.$$&lt;/p&gt;
&lt;p&gt;Esto indica que la recta de regresión del peso sobre la estatura explica el 65% de la variabilidad del peso, y de igual modo, la recta de regresión de la estatura sobre el peso explica el 65% de la variabilidad de la estatura.&lt;/p&gt;
&lt;h3 id=&#34;coeficiente-de-correlación-lineal&#34;&gt;Coeficiente de correlación lineal&lt;/h3&gt;
&lt;div class=&#34;alert alert-def&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;&lt;strong&gt;Definición - Coeficiente de correlación lineal muestral&lt;/strong&gt;. Dada una variable bidimensional $(X,Y)$, el &lt;em&gt;coeficiente de correlación lineal muestral&lt;/em&gt; es la raíz cuadrada de su coeficiente de determinación lineal, con signo el de la covarianza&lt;/p&gt;
&lt;p&gt;$$r = \sqrt{r^2} = \dfrac{s_{xy}}{s_xs_y}.$$&lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;alert alert-warning&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;Como $r^2$ toma valores entre 0 y 1, $r$ tomará valores entre -1 y 1,&lt;/p&gt;
&lt;p&gt;$$-1\leq r\leq 1$$&lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;alert alert-int&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;El coeficiente de correlación lineal no sólo mide mide el grado de dependencia
lineal sino también su dirección (creciente o decreciente):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Si $r =0$ entonces no existe relación lineal.&lt;/li&gt;
&lt;li&gt;Si $r=1$ entonces existe una relación lineal creciente perfecta.&lt;/li&gt;
&lt;li&gt;Si $r=-1$ entonces existe una relación lineal decreciente perfecta.&lt;/li&gt;
&lt;/ul&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Ejemplo&lt;/strong&gt;. En el ejemplo de las estaturas y los pesos se tenía&lt;/p&gt;
&lt;p&gt;$$
\begin{array}{lll}
\bar x = 174.67 \mbox{ cm} &amp;amp; \quad &amp;amp; s^2_x = 102.06 \mbox{ cm}^2\newline
\bar y = 69.67 \mbox{ Kg} &amp;amp; &amp;amp; s^2_y = 164.42 \mbox{ Kg}^2\newline
s_{xy} = 104.07 \mbox{ cm$\cdot$ Kg}
\end{array}
$$&lt;/p&gt;
&lt;p&gt;De manera que el coeficiente de correlación lineal es&lt;/p&gt;
&lt;p&gt;$$r = \frac{s_{xy}}{s_xs_y} = \frac{104.07 \mbox{ cm\cdot Kg}}{10.1 \mbox{ cm} \cdot 12.82 \mbox{ Kg}} = +0.8.$$&lt;/p&gt;
&lt;p&gt;Esto indica que la relación lineal entre el peso y la estatura es fuerte, y además creciente.&lt;/p&gt;
&lt;h3 id=&#34;distintos-grados-de-correlación&#34;&gt;Distintos grados de correlación&lt;/h3&gt;
&lt;p&gt;Los siguientes diagramas de dispersión muestran modelos de regresión lineales con diferentes grados de correlación.&lt;/p&gt;
&lt;img src=&#34;../img/regresion/grados_correlacion.svg&#34; alt=&#34;Modelos de regresión lineales con diferentes grados de correlación&#34; width=&#34;700&#34;&gt;
&lt;h3 id=&#34;fiabilidad-de-las-predicciones-de-un-modelo-de-regresión&#34;&gt;Fiabilidad de las predicciones de un modelo de regresión&lt;/h3&gt;
&lt;p&gt;Aunque el coeficiente de determinación o el de correlación determinan la bondad de ajuste de un modelo de regresión, existen otros factores que influyen en la fiabilidad de las predicciones de un modelo de regresión:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;El coeficiente de determinación: Cuanto mayor sea, menores serán los errores predictivos y mayor la fiabilidad de las predicciones.&lt;/li&gt;
&lt;li&gt;La variabilidad de la población: Cuanto más variable es una población, más difícil es predecir y por tanto menos fiables serán las predicciones.&lt;/li&gt;
&lt;li&gt;El tamaño muestral: Cuanto mayor sea, más información tendremos y, en consecuencia, más fiables serán las predicciones.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;alert alert-warning&#34;&gt;
  &lt;div&gt;
    Además, hay que tener en cuenta que un modelo de regresión es válido únicamente para el rango de valores observados en la muestra. Fuera de ese rango no hay información del tipo de relación entre las variables, por lo que no deben hacerse predicciones para valores lejos de los observados en la muestra.
  &lt;/div&gt;
&lt;/div&gt;
&lt;h2 id=&#34;regresión-no-lineal&#34;&gt;Regresión no lineal&lt;/h2&gt;
&lt;p&gt;El ajuste de un modelo de regresión no lineal es similar al del modelo lineal y también puede realizarse mediante la técnica de mínimos cuadrados.&lt;/p&gt;
&lt;p&gt;No obstante, en determinados casos un ajuste no lineal puede convertirse en un ajuste lineal mediante una sencilla transformación de alguna de las variables del modelo.&lt;/p&gt;
&lt;h3 id=&#34;transformación-de-modelos-de-regresión-no-lineales&#34;&gt;Transformación de modelos de regresión no lineales&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Logarítmico&lt;/strong&gt;: Un modelo logarítmico $y = a+b \log x$ se convierte en un modelo lineal haciendo el cambio $t=\log x$:&lt;/p&gt;
&lt;p&gt;$$y=a+b\log x = a+bt.$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Exponencial&lt;/strong&gt;: Un modelo exponencial $y = ae^{bx}$ se convierte en un modelo
lineal haciendo el cambio $z = \log y$:&lt;/p&gt;
&lt;p&gt;$$z = \log y = \log(ae^{bx}) =  \log a + \log e^{bx} = a^\prime +bx.$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Potencial&lt;/strong&gt;: Un modelo potencial $y = ax^b$ se convierte en un modelo lineal
haciendo los cambios $t=\log x$ y $z=\log y$:&lt;/p&gt;
&lt;p&gt;$$z = \log y = \log(ax^b) = \log a + b \log x = a^\prime+bt.$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Inverso&lt;/strong&gt;: Un modelo inverso $y = a+b/x$ se convierte en un modelo lineal
haciendo el cambio $t=1/x$:&lt;/p&gt;
&lt;p&gt;$$y = a + b(1/x) = a+bt.$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Sigmoidal&lt;/strong&gt;: Un modelo curva S $y = e^{a+b/x}$ se convierte en un modelo lineal haciendo los cambios $t=1/x$ y $z=\log y$:&lt;/p&gt;
&lt;p&gt;$$z = \log y = \log (e^{a+b/x}) = a+b(1/x) = a+bt.$$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;relación-exponencial&#34;&gt;Relación exponencial&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Ejemplo&lt;/strong&gt; El número de bacterias de un cultivo evoluciona con el tiempo según la
siguiente tabla:&lt;/p&gt;
&lt;p&gt;$$\begin{array}{c|c}
\mbox{Horas} &amp;amp; \mbox{Bacterias}\newline
\hline
0 &amp;amp;  25 \newline
1 &amp;amp; 28 \newline
2 &amp;amp;  47\newline
3 &amp;amp; 65 \newline
4 &amp;amp; 86\newline
5 &amp;amp; 121\newline
6 &amp;amp; 190\newline
7 &amp;amp; 290\newline
8 &amp;amp; 362
\end{array}
$$&lt;/p&gt;
&lt;p&gt;El diagrama de dispersión asociado es&lt;/p&gt;
&lt;img src=&#34;../img/regresion/evolucion_bacterias.svg&#34; alt=&#34;Diagrama de dispersión de la evolución de bacterias&#34; width=&#34;500&#34;&gt;
&lt;p&gt;Si realizamos un ajuste lineal, obtenemos la siguiente recta de regresión&lt;/p&gt;
&lt;p&gt;$$\mbox{Bacterias} = -30.18+41,27,\mbox{Horas, with } r^2=0.85.$$&lt;/p&gt;
&lt;img src=&#34;../img/regresion/regresion_lineal_bacterias.svg&#34; alt=&#34;Regresión lineal de la evolución de un cultivo de bacterias&#34; width=&#34;500&#34;&gt;
&lt;p&gt;&lt;em&gt;¿Es un buen modelo?&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Aunque el modelo lineal no es malo, de acuerdo al diagrama de dispersión es más lógico construir un modelo exponencial o cuadrático.&lt;/p&gt;
&lt;p&gt;Para construir el modelo exponencial $y = ae^{bx}$ hay que realizar la
transformación $z=\log y$, es decir, aplicar el logaritmo a la variable dependiente.&lt;/p&gt;
&lt;p&gt;$$\begin{array}{c|c|c}
\mbox{Horas} &amp;amp; \mbox{Bacterias} &amp;amp; \mbox{$\log$(Bacterias)}\newline
\hline
0 &amp;amp;  25 &amp;amp; 3.22\newline
1 &amp;amp; 28 &amp;amp; 3.33\newline
2 &amp;amp;  47 &amp;amp; 3.85\newline
3 &amp;amp; 65  &amp;amp; 4.17\newline
4 &amp;amp; 86 &amp;amp; 4.45\newline
5 &amp;amp; 121 &amp;amp; 4.80\newline
6 &amp;amp; 190 &amp;amp; 5.25\newline
7 &amp;amp; 290 &amp;amp; 5.67\newline
8 &amp;amp; 362 &amp;amp; 5.89
\end{array}
$$&lt;/p&gt;
&lt;img src=&#34;../img/regresion/evolucion_log_bacterias.svg&#34; alt=&#34;Diagrama de dispersión de la evolución del logarítmo de las bacterias de un cultivo&#34; width=&#34;500&#34;&gt;
&lt;p&gt;Ahora sólo queda calcular la recta de regresión del logaritmo de Bacterias sobre Horas&lt;/p&gt;
&lt;p&gt;$$\mbox{Log Bacterias} = 3.107 + 0.352, \mbox{Horas}.$$&lt;/p&gt;
&lt;p&gt;Y, deshaciendo el cambio de variable, se obtiene el modelo exponencial&lt;/p&gt;
&lt;p&gt;$$\mbox{Bacterias} = e^{3.107+0.352,\textrm{Horas}}, \mbox{ con } r^2=0.99.$$&lt;/p&gt;
&lt;img src=&#34;../img/regresion/regresion_exponencial_bacterias.svg&#34; alt=&#34;Regresión exponencial de la evolución de las bacterias de un cultivo&#34; width=&#34;500&#34;&gt;
&lt;p&gt;Como se puede apreciar, el modelo exponencial se ajusta mucho mejor que el modelo lineal.&lt;/p&gt;
&lt;h2 id=&#34;riesgos-de-la-regresión&#34;&gt;Riesgos de la regresión&lt;/h2&gt;
&lt;h3 id=&#34;la-falta-de-ajuste-no-significa-independencia&#34;&gt;La falta de ajuste no significa independencia&lt;/h3&gt;
&lt;p&gt;Es importante señalar que cada modelo de regresión tiene su propio coeficiente de determinación.&lt;/p&gt;
&lt;div class=&#34;alert alert-warning&#34;&gt;
  &lt;div&gt;
    Así, un coeficiente de determinación cercano a cero significa que no existe relación entre las variables del tipo planteado por el modelo, pero &lt;em&gt;eso no quiere decir que las variables sean independientes&lt;/em&gt;, ya que puede existir relación de otro tipo.
  &lt;/div&gt;
&lt;/div&gt;
&lt;img src=&#34;../img/regresion/regresion_lineal_relacion_cuadratica.svg&#34; alt=&#34;Modelo de regresión lineal en una relación cuadrática&#34; width=&#34;500&#34;&gt;
&lt;img src=&#34;../img/regresion/regresion_cuadratica.svg&#34; alt=&#34;Modelo de regresión cuadrático en una relación cuadrática&#34; width=&#34;500&#34;&gt;
&lt;h3 id=&#34;datos-atípicos-en-regresión&#34;&gt;Datos atípicos en regresión&lt;/h3&gt;
&lt;p&gt;Los &lt;em&gt;datos atípicos&lt;/em&gt; en un estudio de regresión son los puntos que claramente no siguen la tendencia del resto de los puntos en el diagrama de dispersión, incluso si los valores del par no se pueden considerar atípicos para cada variable por separado.&lt;/p&gt;
&lt;img src=&#34;../img/regresion/diagrama_dispersion_con_datos_atipicos.svg&#34; alt=&#34;Diagrama de dispersión con un dato atípico&#34; width=&#34;500&#34;&gt;
&lt;div class=&#34;alert alert-warning&#34;&gt;
  &lt;div&gt;
    Los datos atípicos en regresión suelen provocar cambios drásticos en el ajuste de los modelos de regresión, y por tanto, habrá que tener mucho cuidado con ellos.
  &lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;center&#34;&gt;
&lt;img src=&#34;../img/regresion/regresion_lineal_con_datos_atipicos.svg&#34; alt=&#34;Modelo de regresión lineal con datos atípicos&#34; width=&#34;500&#34;&gt; &lt;img src=&#34;../img/regresion/regresion_lineal_sin_datos_atipicos.svg&#34; alt=&#34;Modelo de regresión lineal sin datos atípicos&#34; width=&#34;500&#34;&gt;
&lt;/div&gt;
&lt;h3 id=&#34;la-paradoja-de-simpson&#34;&gt;La paradoja de Simpson&lt;/h3&gt;
&lt;p&gt;A veces, una tendencia desaparece o incluso se revierte cuando se divide la muestra en grupos de acuerdo a una variable cualitativa que está relacionada con la variable dependiente.
Esto se conoce como la &lt;em&gt;paradoja de Simpson&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Ejemplo&lt;/strong&gt;. El siguiente diagrama de dispersión muestra una relación inversa entre entre las horas de estudio preparando un examen y la nota del examen.&lt;/p&gt;
&lt;div class=&#34;center&#34;&gt;
&lt;img src=&#34;../img/regresion/paradoja_simpson_1.svg&#34; alt=&#34;Paradoja de Simpson. Relación inversa entre las horas de estudio para un examen y la nota obtenida.&#34; width=&#34;500&#34;&gt;
&lt;/div&gt;
&lt;p&gt;Pero si se divide la muestra en dos grupos (buenos y malos estudiantes) se obtienen diferentes tendencias y ahora la relación es directa, lo que tiene más lógica.&lt;/p&gt;
&lt;div class=&#34;center&#34;&gt;
&lt;img src=&#34;../img/regresion/paradoja_simpson_2.svg&#34; alt=&#34;Paradoja de Simpson. Relación directa entre las horas de estudio para un examen y la nota obtenida.&#34; width=&#34;500&#34;&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Relaciones entre variables cualitativas</title>
      <link>/docencia/estadistica/manual/relaciones-cualitativas/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/docencia/estadistica/manual/relaciones-cualitativas/</guid>
      <description>&lt;p&gt;Los modelos de regresión vistos sólo pueden aplicarse cuando las variables estudiadas son cuantitativas.&lt;/p&gt;
&lt;p&gt;Cuando se desea estudiar la relación entre atributos, tanto ordinales como nominales, es necesario recurrir a otro tipo de medidas de relación o de asociación. En este capítulo veremos tres de ellas:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Coeficiente de correlación de Spearman.&lt;/li&gt;
&lt;li&gt;Coeficiente chi-cuadrado.&lt;/li&gt;
&lt;li&gt;Coeficiente de contingencia.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;relación-entre-atributos-ordinales&#34;&gt;Relación entre atributos ordinales&lt;/h2&gt;
&lt;h3 id=&#34;coeficiente-de-correlación-de-spearman&#34;&gt;Coeficiente de correlación de Spearman&lt;/h3&gt;
&lt;p&gt;Cuando se tengan atributos ordinales es posible ordenar sus categorías y asignarles valores ordinales, de manera que se puede calcular el coeficiente de correlación lineal entre estos valores ordinales.&lt;/p&gt;
&lt;p&gt;Esta medida de relación entre el orden que ocupan las categorías de dos atributos ordinales se conoce como &lt;em&gt;coeficiente de correlación de Spearman&lt;/em&gt;.&lt;/p&gt;
&lt;div class=&#34;alert alert-def&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;&lt;strong&gt;Definición - Coeficiente de correlación de Spearman&lt;/strong&gt;. Dada una muestra de $n$ individuos en los que se han medido dos atributos ordinales $X$ e $Y$, el coeficiente de correlación de Spearman se define como&lt;/p&gt;
&lt;p&gt;$$r_s = 1-\frac{6\sum d_i^2}{n(n^2-1)}$$&lt;/p&gt;
&lt;p&gt;donde $d_i$ es la diferencia entre el valor ordinal de $X$ y el valor ordinal de $Y$ del individuo $i$.&lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;alert alert-warning&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;Como el coeficiente de correlación de Spearman es en el fondo el coeficiente de correlación lineal aplicado a los órdenes, se tiene que&lt;/p&gt;
&lt;p&gt;$$-1\leq r_s\leq 1,$$&lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;alert alert-int&#34;&gt;
  &lt;div&gt;
    &lt;ul&gt;
&lt;li&gt;Si $r_s=0$ entonces no existe relación entre los atributos ordinales.&lt;/li&gt;
&lt;li&gt;Si $r_s=1$ entonces los órdenes de los atributos coinciden y existe una relación directa perfecta.&lt;/li&gt;
&lt;li&gt;Si $r_s=-1$ entonces los órdenes de los atributos están invertidos y existe una relación inversa perfecta.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;En general, cuanto más cerca de $1$ o $-1$ esté $r_s$, mayor será la relación entre los atributos, y cuanto más cerca de $0$, menor será la relación.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Ejemplo&lt;/strong&gt;. Una muestra de 5 alumnos realizaron dos tareas diferentes $X$ e $Y$, y se ordenaron de acuerdo a la destreza que manifestaron en cada tarea:&lt;/p&gt;
&lt;p&gt;$$
\begin{array}{lrrrr}
\hline
\text{Alumnos} &amp;amp; X &amp;amp; Y &amp;amp; d_i &amp;amp; d_i^2\newline
\hline
\text{Alumno 1} &amp;amp; 2 &amp;amp; 3 &amp;amp; -1 &amp;amp; 1\newline
\text{Alumno 2} &amp;amp; 5 &amp;amp; 4 &amp;amp; 1 &amp;amp; 1 \newline
\text{Alumno 3} &amp;amp; 1 &amp;amp; 2 &amp;amp; -1 &amp;amp; 1\newline
\text{Alumno 4} &amp;amp; 3 &amp;amp; 1 &amp;amp; 2 &amp;amp; 4\newline
\text{Alumno 5} &amp;amp; 4 &amp;amp; 5 &amp;amp; -1 &amp;amp; 1\newline
\hline
\sum &amp;amp;  &amp;amp;  &amp;amp; 0 &amp;amp; 8 \newline
\hline
\end{array}
$$&lt;/p&gt;
&lt;p&gt;El coeficiente de correlación de Spearman para esta muestra es&lt;/p&gt;
&lt;p&gt;$$r_s = 1-\frac{6\sum d_i^2}{n(n^2-1)} = 1- \frac{6\cdot 8}{5(5^2-1)} = 0.6.$$&lt;/p&gt;
&lt;p&gt;Esto indica que existe bastante relación directa entre las destrezas manifestadas en ambas tareas.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Ejemplo con empates&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Cuando hay empates en el orden de las categorías se atribuye a cada valor empatado la media aritmética de los valores ordinales que hubieran ocupado esos individuos en caso de no haber estado empatados.&lt;/p&gt;
&lt;p&gt;Si en el ejemplo anterior los alumnos 4 y 5 se hubiesen comportado igual en la primera tarea y los alumnos 3 y 4 se hubiesen comportado igual en la segunda tarea, entonces se tendría&lt;/p&gt;
&lt;p&gt;$$
\begin{array}{lrrrr}
\hline
\text{Alumnos} &amp;amp; X &amp;amp; Y &amp;amp; d_i &amp;amp; d_i^2\newline
\hline
\text{Alumno 1} &amp;amp; 2 &amp;amp; 3 &amp;amp; -1 &amp;amp; 1\newline
\text{Alumno 2} &amp;amp; 5 &amp;amp; 4 &amp;amp; 1 &amp;amp; 1 \newline
\text{Alumno 3} &amp;amp; 1 &amp;amp; 1.5 &amp;amp; -0.5 &amp;amp; 0.25\newline
\text{Alumno 4} &amp;amp; 3.5 &amp;amp; 1.5 &amp;amp; 2 &amp;amp; 4\newline
\text{Alumno 5} &amp;amp; 3.5 &amp;amp; 5 &amp;amp; -1.5 &amp;amp; 2.25\newline
\hline
\sum &amp;amp;  &amp;amp;  &amp;amp; 0 &amp;amp; 8.5 \newline
\hline
\end{array}
$$&lt;/p&gt;
&lt;p&gt;El coeficiente de correlación de Spearman para esta muestra es&lt;/p&gt;
&lt;p&gt;$$r_s = 1-\frac{6\sum d_i^2}{n(n^2-1)} = 1- \frac{6\cdot 8.5}{5(5^2-1)} = 0.58.$$&lt;/p&gt;
&lt;h2 id=&#34;relación-entre-atributos-nominales&#34;&gt;Relación entre atributos nominales&lt;/h2&gt;
&lt;p&gt;Cuando se quiere estudiar la relación entre atributos nominales no tiene sentido calcular el coeficiente de correlación de Spearman ya que las categorías no pueden ordenarse.&lt;/p&gt;
&lt;p&gt;Para estudiar la relación entre atributos nominales se utilizan medidas basadas en las frecuencias de la tabla de frecuencias bidimensional, que para atributos se suele llamar &lt;em&gt;tabla de contingencia&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Ejemplo&lt;/strong&gt;. En un estudio para ver si existe relación entre el sexo y el hábito de fumar se ha tomado una muestra de 100 personas. La tabla de contingencia resultante es&lt;/p&gt;
&lt;p&gt;$$
\begin{array}{|l|rr|r|}
\hline
\text{Sexo}\backslash\text{Fuma} &amp;amp; \text{Si} &amp;amp; \text{No} &amp;amp; n_i\newline
\hline
\text{Mujer} &amp;amp; 12 &amp;amp; 28 &amp;amp; 40 \newline
\text{Hombre} &amp;amp; 26 &amp;amp; 34 &amp;amp; 60 \newline
\hline
n_j &amp;amp; 38 &amp;amp; 62 &amp;amp; 100\newline
\hline
\end{array}
$$&lt;/p&gt;
&lt;p&gt;Si el hábito de fumar fuese independiente del sexo, la proporción de fumadores en mujeres y hombres sería la misma.&lt;/p&gt;
&lt;h3 id=&#34;frecuencias-teóricas-o-esperadas&#34;&gt;Frecuencias teóricas o esperadas&lt;/h3&gt;
&lt;p&gt;En general, dada una tabla de contingencia para dos atributos $X$ e $Y$,&lt;/p&gt;
&lt;p&gt;$$
\begin{array}{|c|ccccc|c|}
\hline
X\backslash Y &amp;amp; y_1 &amp;amp; \cdots &amp;amp; y_j &amp;amp; \cdots &amp;amp; y_q &amp;amp; n_x\newline
\hline
x_1 &amp;amp; n_{11} &amp;amp; \cdots &amp;amp; n_{1j} &amp;amp; \cdots &amp;amp; n_{1q} &amp;amp; n_{x_1}\newline
\vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots &amp;amp; \vdots \newline
x_i &amp;amp; n_{i1} &amp;amp; \cdots &amp;amp; n_{ij} &amp;amp; \cdots &amp;amp; n_{iq} &amp;amp; n_{x_i}\newline
\vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots &amp;amp; \vdots\newline
x_p &amp;amp; n_{p1} &amp;amp; \cdots &amp;amp; n_{pj} &amp;amp; \cdots &amp;amp; n_{pq} &amp;amp; n_{x_p} \newline
\hline
n_y &amp;amp; n_{y_1} &amp;amp; \cdots &amp;amp; n_{y_j} &amp;amp; \cdots &amp;amp; n_{y_q} &amp;amp; n\newline
\hline
\end{array}
$$&lt;/p&gt;
&lt;p&gt;si $X$ e $Y$ fuesen independientes, para cualquier valor $y_j$ se tendría&lt;/p&gt;
&lt;p&gt;$$\frac{n_{1j}}{n_{x_1}} = \frac{n_{2j}}{n_{x_2}} = \cdots = \frac{n_{pj}}{n_{x_p}} = \frac{n_{1j}+\cdots
+n_{pj}}{n_{x_1}+\cdots+n_{x_p}} = \frac{n_{y_j}}{n},$$ de donde se deduce que $$n_{ij} = \frac{n_{x_i}n_{y_j}}{n}.$$&lt;/p&gt;
&lt;p&gt;A esta última expresión se le llama &lt;em&gt;frecuencia teórica&lt;/em&gt; o &lt;em&gt;frecuencia esperada&lt;/em&gt; del par $(x_i,y_j)$.&lt;/p&gt;
&lt;h3 id=&#34;coeficiente-chi-cuadrado-chi2&#34;&gt;Coeficiente chi-cuadrado $\chi^2$&lt;/h3&gt;
&lt;p&gt;Es posible estudiar la relación entre dos atributos $X$ e $Y$ comparando las frecuencias reales con las esperadas:&lt;/p&gt;
&lt;div class=&#34;alert alert-def&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;&lt;strong&gt;Definición - Coeficiente Chi-cuadrado $\chi^2$&lt;/strong&gt;. Dada una muestra de tamaño $n$ en la que se han medido dos atributos $X$ e $Y$, se define el coeficiente $\chi^2$ como&lt;/p&gt;
&lt;p&gt;$$\chi^2 = \sum_{i=1}^p\sum_{j=1}^q \frac{\left(n_{ij}-\frac{n_{x_i}n_{y_j}}{n}\right)^2}{\frac{n_{x_i}n_{y_j}}{n}},$$&lt;/p&gt;
&lt;p&gt;donde $p$ es el número de categorías de $X$ y $q$ el número de categorías de $Y$.&lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;alert alert-warning&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;Por ser suma de cuadrados, se cumple que&lt;/p&gt;
&lt;p&gt;$$\chi^2 \geq 0.$$&lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;alert alert-int&#34;&gt;
  &lt;div&gt;
    $\chi^2=0$ cuando los atributos son independientes, y crece a medida que aumenta la dependencia entre las variables.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Ejemplo&lt;/strong&gt;. Siguiendo con el ejemplo anterior, a partir de la tabla de contingencia&lt;/p&gt;
&lt;p&gt;$$
\begin{array}{|l|rr|r|}
\hline
\text{Sexo}\backslash\text{Fuma} &amp;amp; \text{Si} &amp;amp; \text{No} &amp;amp; n_i\newline
\hline
\text{Mujer} &amp;amp; 12 &amp;amp; 28 &amp;amp; 40 \newline
\text{Hombre} &amp;amp; 26 &amp;amp; 34 &amp;amp; 60 \newline
\hline
n_j &amp;amp; 38 &amp;amp; 62 &amp;amp; 100\newline
\hline
\end{array}
$$&lt;/p&gt;
&lt;p&gt;se obtienen las siguientes frecuencias esperadas&lt;/p&gt;
&lt;p&gt;$$
\renewcommand{\arraystretch}{1.5}
\begin{array}{|l|rr|r|}
\hline
\mbox{Sexo\backslash Fuma} &amp;amp; \mbox{Si} &amp;amp; \mbox{No} &amp;amp; n_i\newline
\hline
\text{Mujer} &amp;amp; \frac{40\cdot 38}{100}=15.2 &amp;amp; \frac{40\cdot 62}{100}=24.8 &amp;amp; 40 \newline
\text{Hombre} &amp;amp; \frac{60\cdot 38}{100}=22.8 &amp;amp; \frac{60\cdot 62}{100}=37.2 &amp;amp; 60 \newline
\hline
n_j &amp;amp; 38 &amp;amp; 62 &amp;amp; 100\newline
\hline
\end{array}
$$&lt;/p&gt;
&lt;p&gt;y el coeficiente $\chi^2$ vale&lt;/p&gt;
&lt;p&gt;$$\chi^2 = \frac{(12-15.2)^2}{15.2}+\frac{(28-24.8)^2}{24.8}+\frac{(26-22.8)^2}{22.8}+\frac{(34-37.2)^2}{37.2} = 1.81.$$&lt;/p&gt;
&lt;p&gt;Esto indica que no existe gran relación entre el sexo y el hábito de fumar.&lt;/p&gt;
&lt;h3 id=&#34;coeficiente-de-contingencia&#34;&gt;Coeficiente de contingencia&lt;/h3&gt;
&lt;p&gt;El coeficiente $\chi^2$ depende del tamaño muestral, ya que al multiplicar por una constante las frecuencias de todas las casillas, su valor queda multiplicado por dicha constante, lo que podría llevarnos al equívoco de pensar que ha aumentado la relación, incluso cuando las proporciones se mantienen. En consecuencia el valor de $\chi^2$ no está acotado superiormente y resulta difícil de interpretar.&lt;/p&gt;
&lt;p&gt;Para evitar estos problemas se suele utilizar el siguiente estadístico.&lt;/p&gt;
&lt;div class=&#34;alert alert-def&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;&lt;strong&gt;Definición - Coeficiente de contingencia&lt;/strong&gt;. Dada una muestra de tamaño $n$ en la que se han medido dos atributos $X$ e $Y$, se define el &lt;em&gt;coeficiente de contingencia&lt;/em&gt; como&lt;/p&gt;
&lt;p&gt;$$C = \sqrt{\frac{\chi^2}{\chi^2+n}}$$&lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;alert alert-warning&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;De la definición anterior se deduce que&lt;/p&gt;
&lt;p&gt;$$0\leq C\leq 1,$$&lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;alert alert-int&#34;&gt;
  &lt;div&gt;
    $C=0$ cuando las variables son independientes, y crece a medida que aumenta la relación.
  &lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;alert alert-warning&#34;&gt;
  &lt;div&gt;
    Aunque $C$ nunca puede llegar a valer 1, se puede demostrar que para tablas de contingencia con $k$ filas y $k$ columnas, el valor máximo que puede alcanzar $C$ es $\sqrt{(k-1)/k}$.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Ejemplo&lt;/strong&gt;. En el ejemplo anterior el coeficiente de contingencia vale&lt;/p&gt;
&lt;p&gt;$$C = \sqrt{\frac{1.81}{1.81+100}} = 0.13.$$&lt;/p&gt;
&lt;p&gt;Como se trata de una tabla de contingencia de $2\times 2$, el valor máximo que podría tomar el coeficiente de contingencia es $\sqrt{(2-1)/2}=\sqrt{1/2}=0.707$, y como $0.13$ está bastante lejos de este valor, se puede concluir que no existe demasiada relación entre el hábito de fumar y el sexo.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Probabilidad</title>
      <link>/docencia/estadistica/manual/probabilidad/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/docencia/estadistica/manual/probabilidad/</guid>
      <description>&lt;p&gt;La estadística descriptiva permite describir el comportamiento y las relaciones entre las variables en la muestra, pero no permite sacar conclusiones sobre el resto de la población.&lt;/p&gt;
&lt;p&gt;Ha llegado el momento de dar el salto de la muestra a la población y pasar de la estadística descriptiva a la inferencia estadística, y el puente que lo permite es la &lt;strong&gt;Teoría de la Probabilidad&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Hay que tener en cuenta que el conocimiento que se puede obtener de la población a partir de la muestra es limitado, y que para obtener conclusiones válidas para la población la muestra debe ser
representativa de esta. Por esta razón, para garantizar la representatividad de la muestra, esta debe extraerse &lt;em&gt;aleatoriamente&lt;/em&gt;, es decir, al &lt;em&gt;azar&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;La teoría de la probabilidad precisamente se encarga de controlar ese azar para saber hasta qué punto son fiables las conclusiones obtenidas a partir de una muestra.&lt;/p&gt;
&lt;h2 id=&#34;experimentos-y-sucesos-aleatorios&#34;&gt;Experimentos y sucesos aleatorios&lt;/h2&gt;
&lt;h3 id=&#34;experimentos-aleatorios&#34;&gt;Experimentos aleatorios&lt;/h3&gt;
&lt;p&gt;El estudio de una característica en una población se realiza a través de experimentos aleatorios.&lt;/p&gt;
&lt;div class=&#34;alert alert-def&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;&lt;strong&gt;Definición - Experimento aleatorio&lt;/strong&gt;. Un &lt;em&gt;experimento aleatorio&lt;/em&gt; es un experimento que cumple dos condiciones:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;El conjunto de posibles resultados es conocido.&lt;/li&gt;
&lt;li&gt;No se puede predecir con absoluta certeza el resultado del experimento.&lt;/li&gt;
&lt;/ol&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Ejemplo&lt;/strong&gt;. Un ejemplo típico de experimentos aleatorios son los juegos
de azar. El lanzamiento de un dado, por ejemplo, es un experimento
aleatorio ya que:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Se conoce el conjunto posibles de resultados $\{1,2,3,4,5,6\}$.&lt;/li&gt;
&lt;li&gt;Antes de lanzar el dado, es imposible predecir con absoluta certeza el valor que saldrá.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Otro ejemplo de experimento aleatorio sería la selección de un individuo de una población al azar y la determinación de su grupo sanguíneo.&lt;/p&gt;
&lt;p&gt;En general, la obtención de cualquier muestra mediante procedimientos aleatorios será un experimento
aleatorio.&lt;/p&gt;
&lt;h3 id=&#34;espacio-muestral&#34;&gt;Espacio muestral&lt;/h3&gt;
&lt;div class=&#34;alert alert-def&#34;&gt;
  &lt;div&gt;
    &lt;strong&gt;Definición - Espacio muestral&lt;/strong&gt;. Al conjunto $\Omega$ de todos los posibles resultados de un
experimento aleatorio se le llama &lt;em&gt;espacio muestral&lt;/em&gt;.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Algunos ejemplos de espacios muestrales son:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Lanzamiento de una moneda: $\Omega=\{c,x\}$.&lt;/li&gt;
&lt;li&gt;Lanzamiento de un dado: $\Omega=\{1,2,3,4,5,6\}$.&lt;/li&gt;
&lt;li&gt;Grupo sanguíneo de un individuo seleccionado al azar:
$\Omega=\{\mbox{A},\mbox{B},\mbox{AB},\mbox{0}\}$.&lt;/li&gt;
&lt;li&gt;Estatura de un individuo seleccionado al azar:
$\Omega=\mathbb{R}^+$.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;diagrama-de-árbol&#34;&gt;Diagrama de árbol&lt;/h3&gt;
&lt;p&gt;En experimentos donde se mide más de una variable, la determinación del espacio muestral puede resultar compleja. En tales casos es recomendable utilizar un para construir el espacio muestral.&lt;/p&gt;
&lt;p&gt;En un diagrama de árbol cada variable se representa en un nivel del árbol y cada posible valor de la variable como una rama.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Ejemplo&lt;/strong&gt; El siguiente diagrama de árbol representa el espacio muestral de un experimento aleatorio en el que se mide el sexo y el grupo sanguineo de un individuo al azar.&lt;/p&gt;
&lt;img src=&#34;../img/probabilidad/espacio_muestral.svg&#34; alt=&#34;Diagrama de árbol del espacio muestral del sexo y el grupo sanguineo&#34; width=&#34;500&#34;&gt;
&lt;h3 id=&#34;sucesos-aleatorios&#34;&gt;Sucesos aleatorios&lt;/h3&gt;
&lt;div class=&#34;alert alert-def&#34;&gt;
  &lt;div&gt;
    &lt;strong&gt;Definición - Suceso aleatorio&lt;/strong&gt;. Un &lt;em&gt;suceso aleatorio&lt;/em&gt; es cualquier subconjunto del espacio muestral $\Omega$ de un experimento aleatorio.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Existen distintos tipos de sucesos:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Suceso imposible&lt;/strong&gt;: Es el suceso vacío $\emptyset$. Este suceso nunca ocurre.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Sucesos elementales&lt;/strong&gt;: Son los sucesos formados por un solo elemento.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Sucesos compuestos&lt;/strong&gt;: Son los sucesos formados por dos o más elementos.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Suceso seguro&lt;/strong&gt;: Es el suceso que contiene el propio espacio muestral $\Omega$. Este suceso siempre ocurre.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;teoría-de-conjuntos&#34;&gt;Teoría de conjuntos&lt;/h2&gt;
&lt;h3 id=&#34;espacio-de-sucesos&#34;&gt;Espacio de sucesos&lt;/h3&gt;
&lt;div class=&#34;alert alert-def&#34;&gt;
  &lt;div&gt;
    &lt;strong&gt;Definición - Espacio de sucesos&lt;/strong&gt;. Dado un espacio muestral $\Omega$ de un experimento aleatorio, el conjunto formado por todos los posibles sucesos de $\Omega$ se llama &lt;em&gt;espacio de sucesos de $\Omega$&lt;/em&gt; y se denota $\mathcal{P}(\Omega)$.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Ejemplo&lt;/strong&gt;. Dado el espacio muestral $\Omega=\{a,b,c\}$, su espacio de sucesos es&lt;/p&gt;
&lt;p&gt;$$\mathcal{P}(\Omega)=\left\{\emptyset, \{a\},\{b\},\{c\},\{a,b\},\{a,c\},\{b,c\},\{a,b,c\}\right\}$$&lt;/p&gt;
&lt;h3 id=&#34;operaciones-entre-sucesos&#34;&gt;Operaciones entre sucesos&lt;/h3&gt;
&lt;p&gt;Puesto que los sucesos son conjuntos, por medio de la teoría de
conjuntos se pueden definir las siguientes operaciones entre sucesos:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Unión.&lt;/li&gt;
&lt;li&gt;Intersección.&lt;/li&gt;
&lt;li&gt;Complementario.&lt;/li&gt;
&lt;li&gt;Diferencia.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;unión-de-sucesos&#34;&gt;Unión de sucesos&lt;/h3&gt;
&lt;div class=&#34;alert alert-def&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;&lt;strong&gt;Definición - Suceso unión&lt;/strong&gt;. Dados dos sucesos $A,B\subseteq \Omega$, se llama &lt;em&gt;suceso unión&lt;/em&gt; de $A$ y $B$, y se denota $A\cup B$, al suceso formado por los elementos de $A$ junto a los elementos de $B$, es decir,&lt;/p&gt;
&lt;p&gt;$$A\cup B = \{x\,|\, x\in A\textrm{ o }x\in B\}.$$&lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;img src=&#34;../img/probabilidad/union.svg&#34; alt=&#34;Union de dos sucesos&#34; width=&#34;300&#34;&gt;
&lt;p&gt;El suceso unión $A\cup B$ ocurre siempre que ocurre $A$ &lt;span style=&#34;color:red;&#34;&gt;o&lt;/span&gt; $B$.&lt;/p&gt;
&lt;h3 id=&#34;intersección-de-sucesos&#34;&gt;Intersección de sucesos&lt;/h3&gt;
&lt;div class=&#34;alert alert-def&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;&lt;strong&gt;Definición - Suceso intersección&lt;/strong&gt;. Dados dos sucesos $A,B\subseteq \Omega$, se llama &lt;em&gt;suceso intersección&lt;/em&gt; de $A$ y $B$, y se denota $A\cap B$, al suceso formado por los elementos comunes de $A$ y $B$, es decir,&lt;/p&gt;
&lt;p&gt;$$A\cap B = \{x\,|\, x\in A\textrm{ y }x\in B\}.$$&lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;img src=&#34;../img/probabilidad/interseccion.svg&#34; alt=&#34;Intersección de dos sucesos&#34; width=&#34;300&#34;&gt;
&lt;p&gt;El suceso intersección $A\cap B$ ocurre siempre que ocurren $A$ &lt;span style=&#34;color:red;&#34;&gt;y&lt;/span&gt; $B$.&lt;/p&gt;
&lt;p&gt;Diremos que dos sucesos son &lt;strong&gt;incompatibles&lt;/strong&gt; si su intersección es vacía.&lt;/p&gt;
&lt;h3 id=&#34;contrario-de-un-suceso&#34;&gt;Contrario de un suceso&lt;/h3&gt;
&lt;div class=&#34;alert alert-def&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;&lt;strong&gt;Definición - Suceso contrario&lt;/strong&gt;. Dado suceso $A\subseteq \Omega$, se llama &lt;em&gt;suceso contrario o complementario&lt;/em&gt; de $A$, y se denota $\overline A$, al suceso formado por los elementos de $\Omega$ que no pertenecen a $A$, es decir,&lt;/p&gt;
&lt;p&gt;$$\overline A = \{x\,|\, x\not\in A\}.$$&lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;img src=&#34;../img/probabilidad/contrario.svg&#34; alt=&#34;Contrario de un suceso&#34; width=&#34;300&#34;&gt;
&lt;p&gt;El suceso contrario $\overline A$ ocurre siempre que &lt;span style=&#34;color:red;&#34;&gt;no&lt;/span&gt; ocurre $A$.&lt;/p&gt;
&lt;h3 id=&#34;diferencia-de-sucesos&#34;&gt;Diferencia de sucesos&lt;/h3&gt;
&lt;div class=&#34;alert alert-def&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;&lt;strong&gt;Definición - Suceso diferencia&lt;/strong&gt;. Dados dos sucesos $A,B\subseteq \Omega$, se llama &lt;em&gt;suceso diferencia&lt;/em&gt; de $A$ y $B$, y se denota $A-B$, al suceso formado por los elementos de $A$ que no pertenecen a $B$, es decir,&lt;/p&gt;
&lt;p&gt;$$A-B = \{x\,|\, x\in A\mbox{ y }x\not\in B\} = A \cap \overline B.$$&lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;img src=&#34;../img/probabilidad/diferencia.svg&#34; alt=&#34;Diferencia de sucesos&#34; width=&#34;300&#34;&gt;
&lt;p&gt;El suceso diferencia $A-B$ ocurre siempre que ocurre $A$ pero no ocurre $B$, y también puede expresarse como $A\cap \bar B$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Ejemplo&lt;/strong&gt;. Dado el espacio muestral correspondiente al lanzamiento de un dado
$\Omega=\{1,2,3,4,5,6\}$ y los sucesos $A=\{2,4,6\}$ y $B=\{1,2,3,4\}$,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;La unión de $A$ y $B$ es $A\cup B=\{1,2,3,4,6\}$.&lt;/li&gt;
&lt;li&gt;La intersección de $A$ y $B$ es $A\cap B=\{2,4\}$.&lt;/li&gt;
&lt;li&gt;El contrario de $A$ es $\overline A=\{1,3,5\}$.&lt;/li&gt;
&lt;li&gt;Los eventos $A$ y $\overline A$ son incompatibles.&lt;/li&gt;
&lt;li&gt;La diferencia de $A$ y $B$ es $A-B=\{6\}$, y la diferencia de $B$ y $A$ es $B-A=\{1,3\}$.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;álgebra-de-sucesos&#34;&gt;Álgebra de sucesos&lt;/h3&gt;
&lt;p&gt;Dados los sucesos $A,B,C\in  \mathcal{P}(\Omega)$, se cumplen las
siguientes propiedades:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$A\cup A=A$, $A\cap A=A$ (idempotencia).&lt;/li&gt;
&lt;li&gt;$A\cup B=B\cup A$, $A\cap B = B\cap A$ (conmutativa).&lt;/li&gt;
&lt;li&gt;$(A\cup B)\cup C = A\cup (B\cup C)$, $(A\cap B)\cap C = A\cap (B\cap C)$ (asociativa).&lt;/li&gt;
&lt;li&gt;$(A\cup B)\cap C = (A\cap C)\cup (B\cap C)$, $(A\cap B)\cup C = (A\cup C)\cap (B\cup C)$ (distributiva).&lt;/li&gt;
&lt;li&gt;$A\cup \emptyset=A$, $A\cap E=A$ (elemento neutro).&lt;/li&gt;
&lt;li&gt;$A\cup E=E$, $A\cap \emptyset=\emptyset$ (elemento absorbente).&lt;/li&gt;
&lt;li&gt;$A\cup \overline A = E$, $A\cap \overline A= \emptyset$ (elemento simétrico complementario).&lt;/li&gt;
&lt;li&gt;$\overline{\overline A} = A$ (doble contrario).&lt;/li&gt;
&lt;li&gt;$\overline{A\cup B} = \overline A\cap \overline B$, $\overline{A\cap B} = \overline A\cup \overline B$ (leyes de Morgan).&lt;/li&gt;
&lt;li&gt;$A\cap B\subseteq A\cup B$.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;definición-de-probabilidad&#34;&gt;Definición de probabilidad&lt;/h2&gt;
&lt;h3 id=&#34;definición-clásica-de-probabilidad&#34;&gt;Definición clásica de probabilidad&lt;/h3&gt;
&lt;div class=&#34;alert alert-def&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;Dado un espacio muestral $\Omega$ de un experimento aleatorio donde todos los elementos de $\Omega$ son equiprobables, la &lt;em&gt;probabilidad&lt;/em&gt; de un suceso $A\subseteq \Omega$ es el cociente entre el número de elementos de $A$ y el número de elementos de $\Omega$&lt;/p&gt;
&lt;p&gt;$$P(A) = \frac{|A|}{|\Omega|} = \frac{\mbox{nº casos favorables a A}}{\mbox{nº casos posibles}}$$&lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Esta definición es ampliamente utilizada, aunque tiene importantes
restricciones:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Es necesario que todos los elementos del espacio muestral tengan la
misma probabilidad de ocurrir (&lt;em&gt;equiprobabilidad&lt;/em&gt;).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;No puede utilizarse con espacios muestrales infinitos, o de los que
no se conoce el número de casos posibles.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&#34;alert&#34;&gt;&lt;em&gt;¡Ojo! Esto no se cumple en muchos experimentos
aleatorios reales.&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Ejemplo&lt;/strong&gt;. Dado el espacio muestral correspondiente al lanzamiento de un dado $\Omega=\{1,2,3,4,5,6\}$ y el suceso $A=\{2,4,6\}$, la probabilidad de $A$ es&lt;/p&gt;
&lt;p&gt;$$P(A) = \frac{|A|}{|\Omega|} = \frac{3}{6} = 0.5.$$&lt;/p&gt;
&lt;p&gt;Sin embargo, si se considera el espacio muestral correspondiente a observar el grupo sanguíneo de un individuo al azar, $\Omega=\{O,A,B,AB\}$, no se puede usar la definición clásica de probabilidad para calcular la probabilidad de que tenga grupo sanguíneo $A$,&lt;/p&gt;
&lt;p&gt;$$P(A) \neq \frac{|A|}{|\Omega|} = \frac{1}{4} = 0.25,$$&lt;/p&gt;
&lt;p&gt;ya que los grupos sanguíneos no son igualmente probables en las poblaciones humanas.&lt;/p&gt;
&lt;h3 id=&#34;definición-frecuentista-de-probabilidad&#34;&gt;Definición frecuentista de probabilidad&lt;/h3&gt;
&lt;div class=&#34;alert alert-theo&#34;&gt;
  &lt;div&gt;
    &lt;strong&gt;Teorema - Ley de los grandes números&lt;/strong&gt;.Cuando un experimento aleatorio se repite un gran número de veces, las frecuencias relativas de los sucesos del experimento tienden a estabilizarse en torno a cierto número, que es precisamente su probabilidad.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;De acuerdo al teorema anterior, podemos dar la siguiente definición&lt;/p&gt;
&lt;div class=&#34;alert alert-def&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;&lt;strong&gt;Definición - Probabilidad frecuentista&lt;/strong&gt;. Dado un espacio muestral $\Omega$ de un experimento aleatorio
reproducible, la &lt;em&gt;probabilidad&lt;/em&gt; de un suceso $A\subseteq \Omega$ es la frecuencia relativa del suceso $A$ en infinitas repeticiones del experimento&lt;/p&gt;
&lt;p&gt;$$P(A) = lim_{n\rightarrow \infty}\frac{n_{A}}{n}$$&lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Aunque esta definición es muy útil en experimentos científicos reproducibles, también tiene serios inconvenientes, ya que&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Sólo se calcula una aproximación de la probabilidad real.&lt;/li&gt;
&lt;li&gt;La repetición del experimento debe ser en las mismas condiciones.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Ejemplo&lt;/strong&gt;. Dado el espacio muestral correspondiente al lanzamiento de una moneda $\Omega=\{C,X\}$, si después de lanzar la moneda 100 veces obtenemos 54 caras, entonces la probabilidad de $C$ es aproximadamente&lt;/p&gt;
&lt;p&gt;$$P(C) = \frac{n_C}{n} = \frac{54}{100} = 0.54.$$&lt;/p&gt;
&lt;p&gt;Si se considera el espacio muestral correspondiente a observar el grupo sanguíneo de un individuo al azar, $\Omega=\{O,A,B,AB\}$, si se toma una muestra aleatoria de 1000 personas y se observa que 412 tienen grupo sanguíneo $A$, entonces la probabilidad del grupo sanguíneo $A$ es aproximadamente&lt;/p&gt;
&lt;p&gt;$$P(A) = \frac{n_A}{n} = \frac{412}{1000} = 0.412.$$&lt;/p&gt;
&lt;h3 id=&#34;definición-axiomática-de-probabilidad&#34;&gt;Definición axiomática de probabilidad&lt;/h3&gt;
&lt;div class=&#34;alert alert-def&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;&lt;strong&gt;Definición - Probabilidad (Kolmogórov)&lt;/strong&gt;.Dado un espacio muestral $\Omega$ de un experimento aleatorio, una función de &lt;em&gt;probabilidad&lt;/em&gt; es una aplicación que asocia a cada suceso $A\subseteq \Omega$ un número real $P(A)$, conocido como probabilidad de $A$, que cumple los siguientes axiomas:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;La probabilidad de un suceso cualquiera es positiva o nula,
$$P(A)\geq 0.$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;La probabilidad del suceso seguro es igual a la unidad,
$$P(\Omega)=1.$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;La probabilidad de la unión de dos sucesos incompatibles
($A\cap B=\emptyset$) es igual a la suma de las probabilidades de
cada uno de ellos,
$$P(A\cup B) = P(A)+P(B).$$&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;h3 id=&#34;consecuencias-de-los-axiomas-de-probabilidad&#34;&gt;Consecuencias de los axiomas de probabilidad&lt;/h3&gt;
&lt;p&gt;A partir de los axiomas de la definición de probabilidad se pueden
deducir los siguientes resultados:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;$P(\overline A) = 1-P(A)$.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$P(\emptyset)= 0$.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Si $A\subseteq B$ entonces $P(A)\leq P(B)$.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$P(A) \leq 1$.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Si $A$ y $B$ son sucesos compatibles, es decir, su intersección no es vacía, entonces&lt;/p&gt;
&lt;p&gt;$$P(A\cup B)= P(A) + P(B) - P(A\cap B).$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Si el suceso $A$ está compuesto por los sucesos elementales
$e_1,e_2,&amp;hellip;,e_n$, entonces&lt;/p&gt;
&lt;p&gt;$$P(A)=\sum_{i=1}^n P(e_i).$$&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&#34;spoiler &#34; &gt;
  &lt;p&gt;
    &lt;a class=&#34;btn btn-primary&#34; data-toggle=&#34;collapse&#34; href=&#34;#spoiler-12&#34; role=&#34;button&#34; aria-expanded=&#34;false&#34; aria-controls=&#34;spoiler-12&#34;&gt;
      Demostración
    &lt;/a&gt;
  &lt;/p&gt;
  &lt;div class=&#34;collapse card &#34; id=&#34;spoiler-12&#34;&gt;
    &lt;div class=&#34;card-body&#34;&gt;
      &lt;ol&gt;
&lt;li&gt;
&lt;p&gt;$\overline A = \Omega \Rightarrow P(A\cup \overline A) = P(\Omega) \Rightarrow P(A)+P(\overline A) = 1 \Rightarrow P(\overline A)=1-P(A)$.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$\emptyset = \overline \Omega \Rightarrow P(\emptyset) = P(\overline \Omega) = 1-P(\Omega) = 1-1 = 0.$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$B = A\cup (B-A)$. Como $A$ y $B-A$ son incompatibles, $P(B) = P(A\cup (B-A)) = P(A)+P(B-A) \geq P(A).$&lt;/p&gt;
&lt;p&gt;Si pensamos en probabilidades como áreas, es fácil de ver gráficamente,
&lt;img src=&#34;../img/probabilidad/probabilidad_inclusion.svg&#34; alt=&#34;Probabilidad de un suceso incluido en otro&#34; width=&#34;300&#34;&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$A\subseteq \Omega \Rightarrow P(A)\leq P(\Omega)=1.$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$A=(A-B)\cup (A\cap B)$. Como $A-B$ y $A\cap B$ son incompatibles, $P(A)=P(A-B)+P(A\cap B) \Rightarrow P(A-B)=P(A)-P(A\cap B)$.&lt;/p&gt;
&lt;p&gt;Si pensamos en probabilidades como áreas, es fácil de ver gráficamente,&lt;/p&gt;
 &lt;img src=&#34;../img/probabilidad/probabilidad_diferencia.svg&#34; alt=&#34;Probabilidad de la diferencia de dos sucesos&#34; width=&#34;300&#34;&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$A\cup B= (A-B) \cup (B-A) \cup (A\cap B)$. Como $A-B$, $B-A$ y $A\cap B$ son incompatibles, $P(A\cup
B)=P(A-B)+P(B-A)+P(A\cap B) = P(A)-P(A\cap B)+P(B)-P(A\cap B)+P(A\cap B)= P(A)+P(B)-P(A\cup B)$.&lt;/p&gt;
&lt;p&gt;Si pensamos en probabilidades como áreas, es fácil de ver gráficamente,&lt;/p&gt;
 &lt;img src=&#34;../img/probabilidad/probabilidad_union.svg&#34; alt=&#34;Probabilidad de la unión de dos sucesos&#34; width=&#34;300&#34;&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$A=\{e_1,\cdots,e_n\} = \{e_1\}\cup \cdots \cup \{e_n\} \Rightarrow$ $P(A)=P(\{e_1\}\cup \cdots \cup \{e_n\}) = P(\{e_1\})+ \cdots P(\{e_n\}).$&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;

    &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;h3 id=&#34;interpretación-de-la-probabilidad&#34;&gt;Interpretación de la probabilidad&lt;/h3&gt;
&lt;p&gt;Como ha quedado claro en los axiomas anteriores, la probabilidad de un evento $A$ es un número real $P(A)$ que está siempre entre 0 y 1.&lt;/p&gt;
&lt;p&gt;En cierto modo, este número expresa la verosimilitud del evento, es decir, la confianza que hay en que ocurra $A$ en el experimento. Por tanto, también nos da una medida de la incertidumbre sobre el suceso.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;La mayor incertidumbre corresponde a $P(A)=0.5$ (Es tan probable que ocurra $A$ como que no ocurra).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;La menor incertidumbre corresponde a $P(A)=1$ ($A$ sucederá con absoluta certeza) y $P(A)=0$ ($A$ no sucederá con absoluta certeza).&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Cuando $P(A)$ está más próximo a 0 que a 1, la confianza en que no ocurra $A$ es mayor que la de que ocurra $A$. Por el contrario, cuando $P(A)$ está más próximo a 1 que a 0, la confianza en que ocurra
$A$ es mayor que la de que no ocurra $A$.&lt;/p&gt;
&lt;h2 id=&#34;probabilidad-condicionada&#34;&gt;Probabilidad condicionada&lt;/h2&gt;
&lt;h3 id=&#34;experimentos-condicionados&#34;&gt;Experimentos condicionados&lt;/h3&gt;
&lt;p&gt;En algunas ocasiones, es posible que tengamos alguna información sobre el experimento antes de su realización. Habitualmente esa información se da en forma de un suceso $B$ del mismo espacio muestral que sabemos que es cierto antes de realizar el experimento.&lt;/p&gt;
&lt;p&gt;En tal caso se dice que el suceso $B$ es un suceso &lt;em&gt;condicionante&lt;/em&gt;, y la probabilidad de otro suceso $A$ se conoce como y se expresa $$P(A|B).$$&lt;/p&gt;
&lt;p&gt;Esto debe leerse como &lt;em&gt;probabilidad de $A$ dado $B$&lt;/em&gt; o &lt;em&gt;probabilidad de $A$ bajo la condición de $B$&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Los condicionantes suelen cambiar el espacio muestral del experimento y por tanto las probabilidades de sus sucesos.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Ejemplo&lt;/strong&gt;. Supongamos que tenemos una muestra de 100 hombres y 100 mujeres con las siguientes frecuencias&lt;/p&gt;
&lt;p&gt;$$
\begin{array}{|c|c|c|}
\hline
&amp;amp; \mbox{No fumadores} &amp;amp; \mbox{Fumadores} \newline
\hline
\mbox{Mujeres} &amp;amp; 80 &amp;amp; 20 \newline
\hline
\mbox{Hombres} &amp;amp; 60 &amp;amp; 40 \newline
\hline
\end{array}
$$&lt;/p&gt;
&lt;p&gt;Entonces, usando la definición frecuentista de probabilidad, la probabilidad de que una persona elegida al azar sea fumadora es&lt;/p&gt;
&lt;p&gt;$$P(\mbox{Fumadora})= \frac{60}{200}=0.3.$$&lt;/p&gt;
&lt;p&gt;Sin embargo, si se sabe que la persona elegida es mujer, entonces la muestra se reduce a la primera fila, y la probabilidad de ser fumadora es&lt;/p&gt;
&lt;p&gt;$$P(\mbox{Fumadora}|\mbox{Mujer})=\frac{20}{100}=0.2.$$&lt;/p&gt;
&lt;h3 id=&#34;probabilidad-condicionada-1&#34;&gt;Probabilidad condicionada&lt;/h3&gt;
&lt;div class=&#34;alert alert-def&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;&lt;strong&gt;Definición - Probabilidad condicionada&lt;/strong&gt;. Dado un espacio muestral $\Omega$ de un experimento aleatorio, y dos dos sucesos $A,B\subseteq \Omega$, la probabilidad de $A$ &lt;em&gt;condicionada&lt;/em&gt; por $B$ es&lt;/p&gt;
&lt;p&gt;$$P(A|B) = \frac{P(A\cap B)}{P(B)},$$&lt;/p&gt;
&lt;p&gt;siempre y cuando, $P(B)\neq 0$.&lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Esta definición permite calcular probabilidades sin tener que alterar el espacio muestral original del experimento.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Ejemplo&lt;/strong&gt;. En el ejemplo anterior&lt;/p&gt;
&lt;p&gt;$$P(\mbox{Fumadora}|\mbox{Mujer})= \frac{P(\mbox{Fumadora}\cap \mbox{Mujer})}{P(\mbox{Mujer})} =  \frac{20/200}{100/200}=\frac{20}{100}=0.2.$$&lt;/p&gt;
&lt;h3 id=&#34;probabilidad-del-suceso-intersección&#34;&gt;Probabilidad del suceso intersección&lt;/h3&gt;
&lt;p&gt;A partir de la definición de probabilidad condicionada es posible obtener la fórmula para calcular la probabilidad de la intersección de dos sucesos.&lt;/p&gt;
&lt;p&gt;$$P(A\cap B) = P(A)P(B|A) = P(B)P(A|B).$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Ejemplo&lt;/strong&gt;. En una población hay un 30% de fumadores y se sabe que el 40% de los fumadores tiene cáncer de pulmón. La probabilidad de que una persona elegida al azar sea fumadora y tenga cáncer de pulmón es&lt;/p&gt;
&lt;p&gt;$$P(\mbox{Fumadora}\cap \mbox{Cáncer})= P(\mbox{Fumadora})P(\mbox{Cáncer}|\mbox{Fumadora}) = 0.3\times 0.4 = 0.12.$$&lt;/p&gt;
&lt;h3 id=&#34;independencia-de-sucesos&#34;&gt;Independencia de sucesos&lt;/h3&gt;
&lt;p&gt;En ocasiones, la ocurrencia del suceso condicionante no cambia la
probabilidad original del suceso principal.&lt;/p&gt;
&lt;div class=&#34;alert alert-def&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;&lt;strong&gt;Definición - Sucesos independientes&lt;/strong&gt;. Dado un espacio muestral $\Omega$ de un experimento aleatorio, dos
sucesos $A,B\subseteq \Omega$ son &lt;em&gt;independientes&lt;/em&gt; si la probabilidad de $A$ no se ve alterada al condicionar por $B$, y viceversa, es decir,&lt;/p&gt;
&lt;p&gt;$$P(A|B) = P(A) \quad \mbox{and} \quad P(B|A)=P(B),$$&lt;/p&gt;
&lt;p&gt;si $P(A)\neq 0$ y $P(B)\neq 0$.&lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Esto significa que la ocurrencia de uno evento no aporta información relevante para cambiar la incertidumbre sobre el otro.&lt;/p&gt;
&lt;p&gt;Cuando dos eventos son independientes, la probabilidad de su intersección es igual al producto de sus probabilidades,&lt;/p&gt;
&lt;p&gt;$$P(A\cap B) = P(A)P(B).$$&lt;/p&gt;
&lt;h2 id=&#34;espacio-probabilístico&#34;&gt;Espacio probabilístico&lt;/h2&gt;
&lt;div class=&#34;alert alert-def&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;&lt;strong&gt;Definición - Espacio probabilístico&lt;/strong&gt;. Un &lt;em&gt;espacio probabilístico&lt;/em&gt; de un experimento aleatorio es una terna $(\Omega,\mathcal{F},P)$ donde&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\Omega$ es el espacio muestral del experimento.&lt;/li&gt;
&lt;li&gt;$\mathcal{F}$ es un un conjunto de sucesos del experimento.&lt;/li&gt;
&lt;li&gt;$P$ es una función de probabilidad.&lt;/li&gt;
&lt;/ul&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Si conocemos la probabilidad de todos los elementos de $\Omega$, entonces podemos calcular la  probabilidad de cualquier suceso en $\mathcal{F}$ y se puede construir fácilmente el espacio probabilístico.&lt;/p&gt;
&lt;h3 id=&#34;construcción-del-espacio-probabilístico&#34;&gt;Construcción del espacio probabilístico&lt;/h3&gt;
&lt;p&gt;Para determinar la probabilidad de cada suceso elemental se puede utilizar un diagrama de árbol, mediante las siguientes reglas:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Para cada nodo del árbol, etiquetar la rama que conduce hasta él con la probabilidad de que la variable en ese nivel tome el valor del nodo, condicionada por los sucesos correspondientes a sus nodos
antecesores en el árbol.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;La probabilidad de cada suceso elemental en las hojas del árbol es el producto de las probabilidades de las ramas que van desde la raíz a la hoja del árbol.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;img src=&#34;../img/probabilidad/espacio_probabilistico.svg&#34; alt=&#34;Diagrama de árbol de un espacio probabilístico&#34; width=&#34;600&#34;&gt;
&lt;h3 id=&#34;árboles-de-probabilidad-con-variables-dependientes&#34;&gt;Árboles de probabilidad con variables dependientes&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Ejemplo&lt;/strong&gt;. Sea una población en la que el 30% de las personas fuman, y que la incidencia del cáncer de pulmón en fumadores es del 40% mientras que en los no fumadores es del 10%.&lt;/p&gt;
&lt;p&gt;El espacio probabilístico del experimento aleatorio que consiste en elegir una persona al azar y medir las variables Fumar y Cáncer de pulmón se muestra a continuación.&lt;/p&gt;
&lt;img src=&#34;../img/probabilidad/espacio_probabilistico_fumar_cancer.svg&#34; alt=&#34;Diagrama de árbol del espacio probabilístico de fumar y tener cáncer de pulmón&#34; width=&#34;550&#34;&gt;
&lt;h3 id=&#34;árboles-de-probabilidad-con-variables-independientes&#34;&gt;Árboles de probabilidad con variables independientes&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Ejemplo&lt;/strong&gt; El árbol de probabilidad asociado al experimento aleatorio que consiste en el lanzamiento de dos monedas se muestra a continuación.&lt;/p&gt;
&lt;img src=&#34;../img/probabilidad/espacio_probabilistico_monedas.svg&#34; alt=&#34;Diágrama de árbol del espacio probabilístico del lanzamiento de dos monedas&#34; width=&#34;550&#34;&gt;
&lt;h3 id=&#34;árboles-de-probabilidad-con-variables-independientes-1&#34;&gt;Árboles de probabilidad con variables independientes&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Ejemplo&lt;/strong&gt;. Dada una población en la que hay un 40% de hombres y un 60% de mujeres, el experimento aleatorio que consiste en tomar una muestra aleatoria de tres personas tiene el árbol de probabilidad que se muestra a continuación.&lt;/p&gt;
&lt;img src=&#34;../img/probabilidad/espacio_probabilistico_muestra.svg&#34; alt=&#34;Diagrama de árbol del espacio probabilístico del sexo de tres individuos elegidos al azar&#34; width=&#34;600&#34;&gt;
&lt;h2 id=&#34;teorema-de-la-probabilidad-total&#34;&gt;Teorema de la probabilidad total&lt;/h2&gt;
&lt;h3 id=&#34;sistema-completo-de-sucesos&#34;&gt;Sistema completo de sucesos&lt;/h3&gt;
&lt;p&gt;Una colección de sucesos $A_1,A_2,\ldots,A_n$ de un mismo espacio muestral $\Omega$ es un &lt;em&gt;sistema completo&lt;/em&gt; si cumple las siguientes condiciones:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;La unión de todos es el espacio muestral:
$A_1\cup \cdots\cup A_n =\Omega$.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Son incompatibles dos a dos: $A_i\cap A_j = \emptyset$
$\forall i\neq j$.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;img src=&#34;../img/probabilidad/particion_espacio_muestral.svg&#34; alt=&#34;Partición del espacio muestral en un sistema completo de sucesos&#34; width=&#34;300&#34;&gt;
&lt;p&gt;En realidad un sistema completo de sucesos es una partición del espacio muestral de acuerdo a algún atributo, como por ejemplo el sexo o el grupo sanguíneo.&lt;/p&gt;
&lt;h3 id=&#34;teorema-de-la-probabilidad-total-1&#34;&gt;Teorema de la probabilidad total&lt;/h3&gt;
&lt;p&gt;Conocer las probabilidades de un determinado suceso en cada una de las partes de un sistema completo puede ser útil para calcular su probabilidad.&lt;/p&gt;
&lt;div class=&#34;alert alert-def&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;&lt;strong&gt;Definición - Teorema de la probabilidad total&lt;/strong&gt;. Dado un sistema completo de sucesos $A_1,\ldots,A_n$ y un suceso $B$ de un espacio muestral $\Omega$, la probabilidad de cualquier suceso $B$ del espacio muestral se puede calcular mediante la fórmula&lt;/p&gt;
&lt;p&gt;$$P(B) = \sum_{i=1}^n P(A_i\cap B) = \sum_{i=1}^n P(A_i)P(B|A_i).$$&lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;spoiler &#34; &gt;
  &lt;p&gt;
    &lt;a class=&#34;btn btn-primary&#34; data-toggle=&#34;collapse&#34; href=&#34;#spoiler-17&#34; role=&#34;button&#34; aria-expanded=&#34;false&#34; aria-controls=&#34;spoiler-17&#34;&gt;
      Demostración
    &lt;/a&gt;
  &lt;/p&gt;
  &lt;div class=&#34;collapse card &#34; id=&#34;spoiler-17&#34;&gt;
    &lt;div class=&#34;card-body&#34;&gt;
      &lt;p&gt;La demostración del teorema es sencilla, ya que al ser $A_1,\ldots,A_n$ un sistema completo tenemos&lt;/p&gt;
&lt;p&gt;$$B = B\cap E = B\cap (A_1\cup \cdots \cup A_n) = (B\cap A_1)\cup \cdots \cup (B\cap A_n)$$&lt;/p&gt;
&lt;p&gt;y como estos sucesos son incompatibles entre sí, se tiene&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
P(B) &amp;amp;= P((B\cap A_1)\cup \cdots \cup (B\cap A_n)) = P(B\cap A_1)+\cdots + P(B\cap A_n) =\newline
&amp;amp;= P(A_1)P(B/A_1)+\cdots + P(A_n)P(B/A_n) = \sum_{i=1}^n P(A_i)P(B/A_i).
\end{aligned}
$$&lt;/p&gt;
&lt;img src=&#34;../img/probabilidad/probabilidad_total.svg&#34; alt=&#34;Teorema de la probabilidad total&#34; width=&#34;400&#34;&gt;

    &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Ejemplo&lt;/strong&gt;. Un determinado síntoma $S$ puede ser originado por una enfermedad $E$ pero también lo pueden presentar las personas sin la enfermedad.
Sabemos que la prevalencia de la enfermedad $E$ es $0.2$. Además, se sabe que el $90%$ de las personas con la enfermedad presentan el síntoma, mientras que sólo el $40%$ de las personas sin la enfermedad lo presentan. Si se toma una persona al azar de la población, &lt;em&gt;¿qué probabilidad hay de que tenga el síntoma?&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Para responder a la pregunta se puede aplicar el teorema de la probabilidad total usando el sistema completo $\{E,\overline{E}\}$:&lt;/p&gt;
&lt;p&gt;$$P(S) = P(E)P(S|E)+P(\overline E)P(S|\overline E) = 0.2\cdot 0.9 + 0.8\cdot 0.4 = 0.5.$$&lt;/p&gt;
&lt;p&gt;Es decir, la mitad de la población tendrá el síntoma.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;¡En el fondo se trata de una media ponderada de probabilidades!&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;La respuesta a la pregunta anterior es evidente a la luz del árbol de probabilidad del espacio probabilístico del experimento.&lt;/p&gt;
&lt;img src=&#34;../img/probabilidad/espacio_probabilistico_total.svg&#34; alt=&#34;Aplicación del teorema de la probabilidad total en un espacio probabilístico&#34; width=&#34;600&#34;&gt;
&lt;p&gt;$$
\begin{aligned}
P(S) &amp;amp;= P(E,S) + P(\overline E,S) = P(E)P(S|E)+P(\overline E)P(S|\overline E)\newline
&amp;amp; = 0.2\cdot 0.9+ 0.8\cdot 0.4 = 0.18 + 0.32 = 0.5.
\end{aligned}
$$&lt;/p&gt;
&lt;h2 id=&#34;teorema-de-bayes&#34;&gt;Teorema de Bayes&lt;/h2&gt;
&lt;p&gt;Los sucesos de un sistema completo de sucesos $A_1,\cdots,A_n$ también pueden verse como las distintas hipótesis ante un determinado hecho $B$.&lt;/p&gt;
&lt;p&gt;En estas condiciones resulta útil poder calcular las probabilidades a posteriori $P(A_i|B)$ de cada una de las hipótesis.&lt;/p&gt;
&lt;div class=&#34;alert alert-def&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;&lt;strong&gt;Definición - Teorema de Bayes&lt;/strong&gt;. Dado un sistema completo de sucesos $A_1,\ldots,A_n$ y un suceso $B$
de un espacio muestral $\Omega$ y otro suceso $B$ del mismo espacio muestral, la probabilidad de cada suceso $A_i$ $i=1,\ldots,n$ condicionada por $B$ puede calcularse con la siguiente fórmula&lt;/p&gt;
&lt;p&gt;$$P(A_i|B) = \frac{P(A_i\cap B)}{P(B)} = \frac{P(A_i)P(B|A_i)}{\sum_{i=1}^n P(A_i)P(B|A_i)}.$$&lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Ejemplo&lt;/strong&gt;. En el ejemplo anterior, una pregunta más interesante es qué diagnosticar a una persona que presenta el síntoma.&lt;/p&gt;
&lt;p&gt;En este caso se puede interpretar $E$ y $\overline{E}$ como las dos posibles hipótesis para el síntoma $S$. Las probabilidades a priori para ellas son $P(E)=0.2$ y $P(\overline E)=0.8$. Esto quiere decir que si no se dispone de información sobre el síntoma, el diagnóstico será que la persona no tiene la enfermedad.&lt;/p&gt;
&lt;p&gt;Sin embargo, si al reconocer a la persona se observa que presenta el síntoma, dicha información condiciona a las hipótesis, y para decidir entre ellas es necesario calcular sus probabilidades a posteriori, es
decir, $P(E|S)$ y P(\overline{E}|S)$.&lt;/p&gt;
&lt;p&gt;Para calcular las probabilidades a posteriori se puede utilizar el teorema de Bayes:&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
P(E|S) &amp;amp;= \frac{P(E)P(S|E)}{P(E)P(S|E)+P(\overline{E})P(S|\overline{E})} = \frac{0.2\cdot 0.9}{0.2\cdot 0.9 + 0.8\cdot 0.4} = \frac{0.18}{0.5}=0.36,\newline
P(\overline{E}|S) &amp;amp;= \frac{P(\overline{E})P(S|\overline{E})}{P(E)P(S|E)+P(\overline{E})P(S|\overline{E})} = \frac{0.8\cdot 0.4}{0.2\cdot 0.9 + 0.8\cdot 0.4} = \frac{0.32}{0.5}=0.64.
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;Como se puede ver la probabilidad de tener la enfermedad ha aumentado.
No obstante, la probabilidad de no tener la enfermedad sigue siendo mayor que la de tenerla, y por esta razón el diagnóstico seguirá siendo que no tiene la enfermedad.&lt;/p&gt;
&lt;p&gt;En este caso se dice que el síntoma $S$ &lt;em&gt;no es determinante&lt;/em&gt; a la hora de diagnosticar la enfermedad.&lt;/p&gt;
&lt;h2 id=&#34;epidemiología&#34;&gt;Epidemiología&lt;/h2&gt;
&lt;p&gt;Una de las ramas de la Medicina que hace un mayor uso de la probabilidad es la , que estudia la distribución y las causas de las enfermedades en las poblaciones, identificando factores de riesgos para las enfermedades de cara a la atención médica preventiva.&lt;/p&gt;
&lt;p&gt;En Epidemiología interesa la frecuencia de un &lt;em&gt;suceso médico&lt;/em&gt; $E$ (típicamente una enfermedad como la gripe, un factor de riesgo como fumar o un factor de protección como vacunarse) que se mide mediante una
variable nominal con dos categorías (ocurrencia o no del suceso).&lt;/p&gt;
&lt;p&gt;Hay diferentes medidas relativas a la frecuencia de un suceso médico. Las más importantes son:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Prevalencia&lt;/li&gt;
&lt;li&gt;Incidencia&lt;/li&gt;
&lt;li&gt;Riesgo relativo&lt;/li&gt;
&lt;li&gt;Odds ratio&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;prevalencia&#34;&gt;Prevalencia&lt;/h3&gt;
&lt;div class=&#34;alert alert-def&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;&lt;strong&gt;Definición - Prevalencia&lt;/strong&gt;. La &lt;em&gt;prevalencia&lt;/em&gt; de un suceso médico $E$ es la proporción de una población que está afectada por el suceso.&lt;/p&gt;
&lt;p&gt;$$\mbox{Prevalencia}(E) = \frac{\mbox{Nº individuos afectados por $E$}}{\mbox{Tamaño poblacional}}$$&lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;A menudo, la prevalencia se estima mediante una muestra como la frecuencia relativa de los individuos afectados por el suceso en la muestra. Es también común expresarla esta frecuencia como un porcentaje.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Ejemplo&lt;/strong&gt;. Para estimar la prevalencia de la gripe se estudió una muestra de 1000 personas de las que 150 presentaron gripe. Así, la prevalencia de la gripe es aproximadamente 150/1000=0.15, es decir, un
15%.&lt;/p&gt;
&lt;h3 id=&#34;incidencia&#34;&gt;Incidencia&lt;/h3&gt;
&lt;p&gt;La mide la probabilidad de ocurrencia de un suceso médico en una población durante un periodo de tiempo específico. La incidencia puede medirse como una proporción acumulada o como una tasa.&lt;/p&gt;
&lt;div class=&#34;alert alert-def&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;&lt;strong&gt;Definición - Incidencia acumulada&lt;/strong&gt;. La &lt;em&gt;incidencia acumulada&lt;/em&gt; de un suceso médico $E$ es la proporción de individuos que experimentaron el evento en un periodo de tiempo, es decir, el número de nuevos casos afectados por el evento en el periodo de tiempo, divido por el tamaño de la población inicialmente en riesgo de verse afectada.&lt;/p&gt;
&lt;p&gt;$$R(E)=\frac{\mbox{Nº de nuevos casos con $E$}}{\mbox{Tamaño de la población en riesgo}}$$&lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Ejemplo&lt;/strong&gt;. Una población contenía inicialmente $1000$ personas sin gripe y después de dos años se observó que 160 de ellas sufrieron gripe. La incidencia acumulada de la gripe es 160 casos pro 1000 personas por dos años, es decir, 16% en dos años.&lt;/p&gt;
&lt;h3 id=&#34;tasa-de-incidencia-o-riesgo-absoluto&#34;&gt;Tasa de incidencia o Riesgo absoluto&lt;/h3&gt;
&lt;div class=&#34;alert alert-def&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;&lt;strong&gt;Definición - Riesgo absoluto&lt;/strong&gt;.La &lt;em&gt;tasa de incidencia&lt;/em&gt; o &lt;em&gt;riesgo absoluto&lt;/em&gt; de un suceso médico $E$ es el número de nuevos casos afectados por el evento divido por la población en riesgo y por el número de unidades temporales del periodo considerado.&lt;/p&gt;
&lt;p&gt;$$R(E)=\frac{\mbox{Nº nuevos casos con $E$}}{\mbox{Tamaño población en riesgo}\times \mbox{Nº unidades de tiempo}}$$&lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Ejemplo&lt;/strong&gt;. Una población contenía inicialmente $1000$ personas sin gripe y después de dos años se observó que 160 de ellas sufrieron gripe. Si se considera el año como intervalo de tiempo, la tasa de incidencia de la gripe es 160 casos dividida por 1000 personas y por dos años, es decir, 80 casos por 1000 personas-año o 8% de personas al año.&lt;/p&gt;
&lt;h3 id=&#34;prevalencia-vs-incidencia&#34;&gt;Prevalencia vs Incidencia&lt;/h3&gt;
&lt;p&gt;La prevalencia no debe confundirse con la incidencia. La prevalencia indica cómo de extendido está el suceso médico en una población, sin preocuparse por cuándo los sujetos se han expuesto al riesgo o durante
cuánto tiempo, mientras que la incidencia se fija en el riesgo de verse afectado por el suceso en un periodo concreto de tiempo.&lt;/p&gt;
&lt;p&gt;Así, la prevalencia se calcula en estudios transversales en un momento temporal puntual, mientras que para medir la incidencia se necesita un estudio longitudinal que permita observar a los individuos durante un
periodo de tiempo.&lt;/p&gt;
&lt;p&gt;La incidencia es más útil cuando se pretende entender la causalidad del suceso: por ejemplo, si la incidencia de una enfermedad en una población aumenta, seguramente hay un factor de riesgo que lo está promoviendo.&lt;/p&gt;
&lt;p&gt;Cuando la tasa de incidencia es aproximadamente constante en la duración del suceso, la prevalencia es aproximadamente el producto de la incidencia por la duración media del suceso, es decir,&lt;/p&gt;
&lt;p&gt;$$ \mbox{Prevalencia} = \mbox{Incidencia} \times \mbox{duración}$$&lt;/p&gt;
&lt;h3 id=&#34;comparación-de-riesgos&#34;&gt;Comparación de riesgos&lt;/h3&gt;
&lt;p&gt;Para determinar si un factor o característica está asociada con el suceso médico es necesario comparar el riesgo del suceso en dos poblaciones, una expuesta al factor y la otra no. El grupo expuesto al factor se conoce como &lt;em&gt;grupo tratamiento&lt;/em&gt; o &lt;em&gt;grupo experimental&lt;/em&gt; $T$ y el grupo no expuesto como &lt;em&gt;grupo control&lt;/em&gt; $C$.&lt;/p&gt;
&lt;p&gt;Habitualmente los casos observados para cada grupo se representan en una tabla de 2$\times$2 como la siguiente:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th style=&#34;text-align: left;&#34;&gt;&lt;/th&gt;
&lt;th style=&#34;text-align: center;&#34;&gt;Suceso $E$&lt;/th&gt;
&lt;th style=&#34;text-align: center;&#34;&gt;No suceso $\overline E$&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;Grupo tratamiento $T$&lt;/td&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;$a$&lt;/td&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;$b$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;Grupo control $C$&lt;/td&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;$c$&lt;/td&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;$d$&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;riesgo-atribuible-o-diferencia-de-riesgos-ra&#34;&gt;Riesgo atribuible o diferencia de riesgos $RA$&lt;/h3&gt;
&lt;div class=&#34;alert alert-def&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;&lt;strong&gt;Definición - Riesgo atribuible&lt;/strong&gt;. El &lt;em&gt;riesgo atribuible&lt;/em&gt; o &lt;em&gt;diferencia de riesgo&lt;/em&gt; de un suceso médico
$E$ para los individuos expuestos a un factor es la diferencia entre los riesgos absolutos de los grupos tratamiento y control.&lt;/p&gt;
&lt;p&gt;$$RA(E)=R_T(E)-R_C(E)=\frac{a}{a+b}-\frac{c}{c+d}.$$&lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;El riesgo atribuible es el riesgo de un suceso que es debido específicamente al factor de interés.&lt;/p&gt;
&lt;p&gt;Obsérvese que el riesgo atribuible puede ser positivo, cuando el riesgo del grupo tratamiento es mayor que el del grupo control, o negativo, de lo contrario.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Ejemplo&lt;/strong&gt;. Para determinar la efectividad de una vacuna contra la gripe, una muestra de 1000 personas sin gripe fueron seleccionadas al comienzo del año. La mitad de ellas fueron vacunadas (grupo tratamiento) y la otra mitad recibieron un placebo (grupo control). La tabla siguiente resume los resultados al final del año.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th style=&#34;text-align: left;&#34;&gt;&lt;/th&gt;
&lt;th style=&#34;text-align: center;&#34;&gt;Gripe $E$&lt;/th&gt;
&lt;th style=&#34;text-align: center;&#34;&gt;No gripe $\overline E$&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;Grupo tratamiento (vacunados)&lt;/td&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;20&lt;/td&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;480&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;Grupo control (No vacunados)&lt;/td&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;80&lt;/td&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;420&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;El riesgo atribuible de contraer la gripe cuando se es vacunado es&lt;/p&gt;
&lt;p&gt;$$AR(D) = \frac{20}{20+480}-\frac{80}{80+420} = -0.12.$$&lt;/p&gt;
&lt;p&gt;Esto quiere decir que el riesgo de contraer la gripe es un 12% menor en vacunados
que en no vacunados.&lt;/p&gt;
&lt;h3 id=&#34;riesgo-relativo-rr&#34;&gt;Riesgo relativo $RR$&lt;/h3&gt;
&lt;div class=&#34;alert alert-def&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;&lt;strong&gt;Definición - Teorema de Bayes&lt;/strong&gt;. El &lt;em&gt;riesgo relativo&lt;/em&gt; de un suceso médico $E$ para los individuos expuestos a un factor es el cociente entre las proporciones de individuos afectados por el suceso en un periodo de tiempo de los grupos tratamiento y control. Es decir, el cociente entre las incidencias de
grupo tratamiento y el grupo control.&lt;/p&gt;
&lt;p&gt;$$RR(D)=\frac{\mbox{Riesgo grupo tratamiento}}{\mbox{Riesgo grupo control}}=\frac{R_T(E)}{R_C(E)}=\frac{a/(a+b)}{c/(c+d)}$$&lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;El riesgo relativo compara el riesgo de desarrollar un suceso médico entre el grupo tratamiento y el grupo control.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$RR=1$ $\Rightarrow$ No hay asociación entre el suceso y la exposición al factor.&lt;/li&gt;
&lt;li&gt;$RR&amp;lt;1$ $\Rightarrow$ La exposición al factor disminuye el riesgo del suceso.&lt;/li&gt;
&lt;li&gt;$RR&amp;gt;1$ $\Rightarrow$ La exposición al factor aumenta el riesgo del suceso.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Cuanto más lejos de 1, más fuerte es la asociación.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Ejemplo&lt;/strong&gt;. Para determinar la efectividad de una vacuna contra la gripe, una muestra de 1000 personas sin gripe fueron seleccionadas al comienzo del año. La mitad de ellas fueron vacunadas (grupo tratamiento) y la otra mitad recibieron un placebo (grupo control). La tabla siguiente resume los resultados al final del año.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th style=&#34;text-align: left;&#34;&gt;&lt;/th&gt;
&lt;th style=&#34;text-align: center;&#34;&gt;Gripe $E$&lt;/th&gt;
&lt;th style=&#34;text-align: center;&#34;&gt;No gripe $\overline E$&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;Grupo tratamiento (vacunados)&lt;/td&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;20&lt;/td&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;480&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;Grupo control (No vacunados)&lt;/td&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;80&lt;/td&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;420&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;El riesgo relativo de contraer la gripe cuando se es vacunado es&lt;/p&gt;
&lt;p&gt;$$RR(D) = \frac{20/(20+480)}{80/(80+420)} = 0.25.$$&lt;/p&gt;
&lt;p&gt;Así, la probabilidad de contraer la gripe en los individuos vacunados fue la cuarta parte de
la de contraerla en el caso de no haberse vacunado, es decir, la vacuna reduce el riesgo de gripe un 75%.&lt;/p&gt;
&lt;h3 id=&#34;odds&#34;&gt;Odds&lt;/h3&gt;
&lt;p&gt;Una forma alternativa de medir el riesgo de un suceso médico es el &lt;em&gt;odds&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;El &lt;em&gt;odds&lt;/em&gt; de un suceso médico $E$ en una población es el cociente entre el número de individuos que adquirieron el suceso y los que no en un periodo de tiempo.&lt;/p&gt;
&lt;p&gt;$$ODDS(E)=\frac{\mbox{Nº nuevos casos con $E$}}{\mbox{Nº casos sin $E$}}=\frac{P(E)}{P(\overline E)}$$&lt;/p&gt;
&lt;p&gt;A diferencia de la incidencia, que es una proporción menor o igual que 1, el odds puede ser mayor que 1. No obstante es posible convertir el odds en una probabilidad con al fórmula&lt;/p&gt;
&lt;p&gt;$$P(E) = \frac{ODDS(E)}{ODDS(E)+1}$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Ejemplo&lt;/strong&gt; Una población contenía inicialmente $1000$ personas sin gripe. Después de un año 160 de ellas tuvieron gripe. Entonces el odds de la gripe es 160/840.&lt;/p&gt;
&lt;p&gt;Obsérvese que la incidencia es 160/1000.&lt;/p&gt;
&lt;h3 id=&#34;odds-ratio-or&#34;&gt;Odds ratio $OR$&lt;/h3&gt;
&lt;div class=&#34;alert alert-def&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;&lt;strong&gt;Definición - Odds ratio&lt;/strong&gt;. El &lt;em&gt;odds ratio&lt;/em&gt; o la &lt;em&gt;oportunidad relativa&lt;/em&gt; de un suceso médico $E$
para los individuos expuestos a un factor es el cociente entre los odds del sucesos de los grupos tratamiento y control.&lt;/p&gt;
&lt;p&gt;$$OR(E)=\frac{\mbox{Odds en grupo tratamiento}}{\mbox{Odds en grupo control}}=\frac{a/b}{c/d}=\frac{ad}{bc}$$&lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;El odds ratio compara los odds de un suceso médico entre el grupo tratamiento y control. La interpretación es similar a la del riesgo relativo:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$OR=1$ $\Rightarrow$ No existe asociación entre el suceso y la exposición al factor.&lt;/li&gt;
&lt;li&gt;$OR&amp;lt;1$ $\Rightarrow$ La exposición al factor disminuye el riesgo del suceso.&lt;/li&gt;
&lt;li&gt;$OR&amp;gt;1$ $\Rightarrow$ La exposición al factor aumenta el riesgo del suceso.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Cuanto más lejos de 1, más fuerte es la asociación.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Ejemplo&lt;/strong&gt;. Para determinar la efectividad de una vacuna contra la gripe, una muestra de 1000 personas sin gripe fueron seleccionadas al comienzo del año. La mitad de ellas fueron vacunadas (grupo tratamiento) y la otra mitad recibieron un placebo (grupo control). La tabla siguiente resume los resultados al final del año.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th style=&#34;text-align: left;&#34;&gt;&lt;/th&gt;
&lt;th style=&#34;text-align: center;&#34;&gt;Gripe $E$&lt;/th&gt;
&lt;th style=&#34;text-align: center;&#34;&gt;No gripe $\overline E$&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;Grupo tratamiento (vacunados)&lt;/td&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;20&lt;/td&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;480&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;Grupo control (No vacunados)&lt;/td&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;80&lt;/td&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;420&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;El odds ratio de sufrir la gripe para los individuos vacunados es&lt;/p&gt;
&lt;p&gt;$$OR(D) = \frac{20/480}{80/420} = 0.21875.$$&lt;/p&gt;
&lt;p&gt;Esto quiere decir que el odds de sufrir la gripe frente a no sufrirla en los vacunados es casi un quinto del de los no vacunados, es decir, que aproximadamente por cada 22 personas vacunadas con gripe habrá 100 personas no vacunadas con gripe.&lt;/p&gt;
&lt;h3 id=&#34;riesgo-relativo-vs-odds-ratio&#34;&gt;Riesgo relativo vs Odds ratio&lt;/h3&gt;
&lt;p&gt;El riesgo relativo y el odds ratio son dos medidas de asociación pero su interpretación es ligeramente diferente. Mientras que el riesgo relativo expresa una comparación de riesgos entre los grupos tratamiento y control, el odds ratio expresa una comparación de odds, que no es lo mismo que el riesgo. Así, un odds ratio de 2 &lt;em&gt;no&lt;/em&gt; significa que el grupo tratamiento tiene el doble de riesgo de adquirir el suceso.&lt;/p&gt;
&lt;p&gt;La interpretación del odds ratio es un poco más enrevesada porque es contrafactual, y nos da cuántas veces es más frecuente el suceso en el grupo tratamiento en comparación con el control, asumiendo que en el
grupo control es tan frecuente que ocurra el suceso como que no.&lt;/p&gt;
&lt;p&gt;La ventaja del odds ratio es que no depende de la prevalencia o la incidencia del suceso, y debe usarse siempre que el número de individuos que presenta el suceso se selecciona arbitrariamente en ambos grupos,
como ocurre en los estudios casos-control.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Ejemplo&lt;/strong&gt;. Para determinar la asociación entre el cáncer de pulmón y fumar se tomaron dos muestras (la segunda con el doble de individuos sin cáncer) obteniendo los siguientes resultados:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Sample 1&lt;/strong&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th style=&#34;text-align: left;&#34;&gt;&lt;/th&gt;
&lt;th style=&#34;text-align: center;&#34;&gt;Cáncer&lt;/th&gt;
&lt;th style=&#34;text-align: center;&#34;&gt;No cáncer&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;Fumadores&lt;/td&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;60&lt;/td&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;80&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;No fumadores&lt;/td&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;40&lt;/td&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;320&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;$$
\begin{aligned}
RR(D) &amp;amp;= \frac{60/(60+80)}{40/(40+320)} = 3.86.\newline
OR(D) &amp;amp;= \frac{60/80}{40/320} = 6.
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Sample 2&lt;/strong&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th style=&#34;text-align: left;&#34;&gt;&lt;/th&gt;
&lt;th style=&#34;text-align: center;&#34;&gt;Cáncer&lt;/th&gt;
&lt;th style=&#34;text-align: center;&#34;&gt;No cáncer&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;Fumadores&lt;/td&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;60&lt;/td&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;160&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;No fumadores&lt;/td&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;40&lt;/td&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;640&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;$$
\begin{aligned}
RR(D) &amp;amp;= \frac{60/(60+160)}{40/(40+640)} = 4.64.\newline
OR(D) &amp;amp;= \frac{60/160}{40/640} = 6.
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;Así, cuando cambia la incidencia o prevalencia de un suceso (cáncer de pulmón) el riesgo relativo cambia, mientras que el odds ratio no.&lt;/p&gt;
&lt;h3 id=&#34;riesgo-relativo-vs-odds-ratio-1&#34;&gt;Riesgo relativo vs Odds ratio&lt;/h3&gt;
&lt;p&gt;La relación entre el riesgo relativo y el odds ratio viene dada por la siguiente fórmula&lt;/p&gt;
&lt;p&gt;$$RR = \frac{OR}{1-R_0+R_0\cdot OR} = OR \frac{1-R_1}{1-R_0},$$&lt;/p&gt;
&lt;p&gt;donde $R_C$ and $R_T$ son la prevalencia o la incidencia en los grupos control y tratamiento respectivamente.&lt;/p&gt;
&lt;p&gt;El odds ratio siempre sobrestima el riesgo relativo cuando este es mayor que 1 y lo subestima cuando es menor que 1. No obstante, con sucesos médicos raros (con una prevalencia o incidencia baja) el riesgo
relativo y el odds ratio son casi iguales.&lt;/p&gt;
&lt;img src=&#34;../img/probabilidad/odds_ratio_vs_riesgo_relativo.svg&#34; alt=&#34;Odss ratio versus riesgo relativo&#34; width=&#34;600&#34;&gt;
&lt;h2 id=&#34;tests-diagnósticos&#34;&gt;Tests diagnósticos&lt;/h2&gt;
&lt;p&gt;En Epidemiología es común el uso de test para diagnosticar enfermedades.&lt;/p&gt;
&lt;p&gt;Generalmente estos test no son totalmente fiables, sino que hay cierta probabilidad de acierto o fallo en el diagnóstico, que suele representarse en la siguiente tabla:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th style=&#34;text-align: left;&#34;&gt;&lt;/th&gt;
&lt;th style=&#34;text-align: center;&#34;&gt;Presencia enfermedad $E$&lt;/th&gt;
&lt;th style=&#34;text-align: center;&#34;&gt;Ausencia enfermedad $\overline E$&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;Test positivo $+$&lt;/td&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;&lt;span style=&#34;color: green&#34;&gt;Verdadero positivo &lt;/span&gt;$VP$&lt;/td&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;&lt;span style=&#34;color: red&#34;&gt;Falso positivo &lt;/span&gt;$FP$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;Test negativo $−$&lt;/td&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;&lt;span style=&#34;color: red&#34;&gt;Falso negativo &lt;/span&gt;$FN$&lt;/td&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;&lt;span style=&#34;color: green&#34;&gt;Verdadero Negativo &lt;/span&gt;$VN$&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;sensibilidad-y-especificidad-de-un-test-diagnóstico&#34;&gt;Sensibilidad y especificidad de un test diagnóstico&lt;/h3&gt;
&lt;p&gt;La fiabilidad de un test diagnóstico depende de las siguientes probabilidades.&lt;/p&gt;
&lt;div class=&#34;alert alert-def&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;&lt;strong&gt;Definición - Sensibilidad&lt;/strong&gt;. La &lt;em&gt;sensibilidad&lt;/em&gt; de un test diagnóstico es la proporción de resultados positivos del test en personas con la enfermedad,&lt;/p&gt;
&lt;p&gt;$$P(+|E)=\frac{VP}{VP+FN}$$&lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;alert alert-def&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;&lt;strong&gt;Definición - Especificidad&lt;/strong&gt;. La &lt;em&gt;especificidad&lt;/em&gt; de un test diagnóstico es la proporción de resultados
negativos del test en personas sin la enfermedad,&lt;/p&gt;
&lt;p&gt;$$P(-|\overline{E})=\frac{VN}{VN+FP}$$&lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Normalmente existe un balance entre la sensibilidad y la especificidad.&lt;/p&gt;
&lt;p&gt;Un test con una alta sensibilidad detectará la enfermedad en la mayoría de las personas enfermas, pero también dará más falsos positivos que un test menos sensible. De este modo, un resultado positivo en un test con una gran sensibilidad no es muy útil para confirmar la enfermedad, pero un resultado negativo es útil para descartar la enfermedad, ya que raramente da resultados negativos en personas con la enfermedad.&lt;/p&gt;
&lt;p&gt;Por otro lado, un test con una alta especificidad descartará la enfermedad en la mayoría de las personas sin la enfermedad, pero también producirá más falsos negativos que un test menos específico. Así, un
resultado negativo en un test con una gran especificidad no es útil para descartar la enfermedad, pero un resultado positivo es muy útil para confirmar la enfermedad, ya que raramente da resultados positivos en
personas sin la enfermedad.&lt;/p&gt;
&lt;p&gt;Decidir entre un test con una gran sensibilidad o un test con una gran especificidad depende del tipo de enfermedad y el objetivo del test. En general, utilizaremos un test sensible cuando:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;La enfermedad es grave y es importante detectarla.&lt;/li&gt;
&lt;li&gt;La enfermedad es curable.&lt;/li&gt;
&lt;li&gt;Los falsos positivos no provocan traumas serios.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Y utilizaremos un test específico cuando:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;La enfermedad es importante pero difícil o imposible de curar.&lt;/li&gt;
&lt;li&gt;Los falsos positivos pueden provocar traumas serios.&lt;/li&gt;
&lt;li&gt;El tratamiento de los falsos positivos puede tener graves consecuencias.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;valores-predictivos-de-un-test-diagnóstico&#34;&gt;Valores predictivos de un test diagnóstico&lt;/h3&gt;
&lt;p&gt;Pero el aspecto más importante de un test diagnóstico es su poder predictivo, que se mide con las siguientes probabilidades a posteriori.&lt;/p&gt;
&lt;div class=&#34;alert alert-def&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;&lt;strong&gt;Definición - Valor predictivo positivo&lt;/strong&gt;. El &lt;em&gt;valor predictivo positivo&lt;/em&gt; de un test diagnóstico es la proporción de personas con la enfermedad entre las personas con resultado positivo
en el test,&lt;/p&gt;
&lt;p&gt;$$P(E|+) = \frac{VP}{VP+FP}$$&lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;alert alert-def&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;&lt;strong&gt;Definición - Valor predictivo negativo&lt;/strong&gt;. El &lt;em&gt;valor predictivo negativo&lt;/em&gt; de un test diagnóstico es la proporción de personas sin la enfermedad entre las personas con resultado negativo en el test,&lt;/p&gt;
&lt;p&gt;$$P(\overline{E}|-) = \frac{VN}{VN+FN}$$&lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Los valores predictivos positivo y negativo permiten confirmar o descartar la enfermedad, respectivamente, si alcanzan al menos el umbral de $0.5$.&lt;/p&gt;
&lt;p&gt;$$
\begin{array}{rcl}
VPP&amp;gt;0.5 &amp;amp; \Rightarrow &amp;amp; \mbox{Diagnosticar la enfermedad}\newline
VPN&amp;gt;0.5 &amp;amp; \Rightarrow &amp;amp; \mbox{Diagnosticar la no enfermedad}
\end{array}
$$&lt;/p&gt;
&lt;p&gt;No obstante, estas probabilidades dependen de la proporción de personas con la enfermedad en la población $P(E)$ que se conoce como de la enfermedad. Pueden calcularse a partir de la sensibilidad y la especificidad del test diagnóstico usando el teorema de Bayes.&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
VPP=P(E|+) &amp;amp;= \frac{P(E)P(+|E)}{P(E)P(+|E)+P(\overline{E})P(+|\overline{E})}\newline
VPN=P(\overline{E}|-) &amp;amp;= \frac{P(\overline{E})P(-|\overline{E})}{P(E)P(-|E)+P(\overline{E})P(-|\overline{E})}
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;Así, con enfermedades frecuentes, el valor predictivo positivo aumenta, y con enfermedades raras, el valor predictivo negativo aumenta.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Ejemplo&lt;/strong&gt; Un test diagnóstico para la gripe se ha aplicado a una muestra aleatoria de 1000 personas. Los resultados aparecen resumidos en la siguiente
tabla.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th style=&#34;text-align: left;&#34;&gt;&lt;/th&gt;
&lt;th style=&#34;text-align: center;&#34;&gt;Presencia de gripe $E$&lt;/th&gt;
&lt;th style=&#34;text-align: center;&#34;&gt;Ausencia de gripe $\overline E$&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;Test $+$&lt;/td&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;95&lt;/td&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;90&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;Test $−$&lt;/td&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;5&lt;/td&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;810&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Según esta muestra, la prevalencia de la gripe puede estimarse como&lt;/p&gt;
&lt;p&gt;$$P(E) = \frac{95+5}{1000} = 0.1.$$&lt;/p&gt;
&lt;p&gt;La sensibilidad del test diagnóstico es&lt;/p&gt;
&lt;p&gt;$$P(+|E) = \frac{95}{95+5}= 0.95.$$&lt;/p&gt;
&lt;p&gt;Y la especificidad es&lt;/p&gt;
&lt;p&gt;$$P(-|\overline{E}) = \frac{810}{90+810}=0.9.$$&lt;/p&gt;
&lt;p&gt;El valor predictivo positivo del test es&lt;/p&gt;
&lt;p&gt;$$VPP = P(E|+) = \frac{95}{95+90} = 0.5135.$$&lt;/p&gt;
&lt;p&gt;Como este valor es mayor que $0.5$, eso significa que se diagnosticará la gripe si el resultado del test es positivo. No obstante, la confianza en el diagnóstico será baja, ya que el valor es poco mayor que $0.5$.&lt;/p&gt;
&lt;p&gt;Por otro lado, el valor predictivo negativo es&lt;/p&gt;
&lt;p&gt;$$VPN = P(\overline{E}|-) = \frac{810}{5+810} = 0.9939.$$&lt;/p&gt;
&lt;p&gt;Como este valor es casi 1, eso significa que es casi seguro que no se tiene la gripe cuando el resultado del test es negativo.&lt;/p&gt;
&lt;p&gt;Así, se puede concluir que este test es muy potente para descartar la gripe, pero no lo est tanto para confirmarla.&lt;/p&gt;
&lt;h3 id=&#34;razón-de-verosimilitud-de-un-test-diagnóstico&#34;&gt;Razón de verosimilitud de un test diagnóstico&lt;/h3&gt;
&lt;p&gt;La siguientes medidas también se derivan de la sensibilidad y la especificidad de un test diagnóstico.&lt;/p&gt;
&lt;div class=&#34;alert alert-def&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;&lt;strong&gt;Definición - Razón de verosimilitud positiva&lt;/strong&gt;. La &lt;em&gt;razón de verosimilitud positiva&lt;/em&gt; de un test diagnóstico es el cociente entre la probabilidad de un resultado positivo en personas con
la enfermedad y personas sin la enfermedad, respectivamente.&lt;/p&gt;
&lt;p&gt;$$RV+=\frac{P(+|E)}{P(+|\overline{E})} = \frac{\mbox{Sensibilidad}}{1-\mbox{Especificidad}}$$&lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;alert alert-def&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;&lt;strong&gt;Definición - Razón de verosimilitud negativa&lt;/strong&gt;. La &lt;em&gt;razón de verosimilitud negativa&lt;/em&gt; de un test diagnóstico es el cociente entre la probabilidad de un resultado negativo en personas con la enfermedad y personas sin la enfermedad, respectivamente.&lt;/p&gt;
&lt;p&gt;$$RV-=\frac{P(-|E)}{P(-|\overline{E})} = \frac{1-\mbox{Sensibilidad}}{\mbox{Especificidad}}$$&lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;La razón de verosimilitud positiva puede interpretarse como el número de veces que un resultado positivo es más probable en personas con la enfermedad que en personas sin la enfermedad.&lt;/p&gt;
&lt;p&gt;Por otro lado, la razón de verosimilitud negativa puede interpretarse como el número de veces que un resultado negativo es más probable en personas con la enfermedad que en personas sin la enfermedad.&lt;/p&gt;
&lt;p&gt;Las probabilidades a posteriori pueden calculares a partir de las probabilidades a priori usando las razones de verosimilitud&lt;/p&gt;
&lt;p&gt;$$P(E|+) = \frac{P(E)P(+|E)}{P(E)P(+|E)+P(\overline{E})P(+|\overline{E})} = \frac{P(E)RV+}{1-P(E)+P(E)RV+}$$&lt;/p&gt;
&lt;p&gt;Así,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Una razón de verosimilitud positiva mayor que 1 aumenta la probabilidad de la enfermedad.&lt;/li&gt;
&lt;li&gt;Una razón de verosimilitud positiva menor que 1 disminuye la probabilidad de la enfermedad.&lt;/li&gt;
&lt;li&gt;Una razón de verosimilitud 1 no cambia la probabilidad a priori de la de tener la enfermedad.&lt;/li&gt;
&lt;/ul&gt;
&lt;img src=&#34;../img/probabilidad/razon_verosimilitud.svg&#34; alt=&#34;Razón de verosimilitud&#34; width=&#34;600&#34;&gt;
</description>
    </item>
    
    <item>
      <title>Contrastes de Hipótesis</title>
      <link>/docencia/estadistica/manual/contrastes/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/docencia/estadistica/manual/contrastes/</guid>
      <description>&lt;h2 id=&#34;hipótesis-estadísticas-y-tipos-de-contrastes-de-hipótesis&#34;&gt;Hipótesis estadísticas y tipos de contrastes de hipótesis&lt;/h2&gt;
&lt;h3 id=&#34;hipótesis-estadística&#34;&gt;Hipótesis estadística&lt;/h3&gt;
&lt;p&gt;En muchos estudios estadísticos, el objetivo, más que estimar el valor de un parámetro desconocido en la población, es comprobar la veracidad de una hipótesis formulada sobre la población objeto de estudio.&lt;/p&gt;
&lt;p&gt;El investigador, de acuerdo a su experiencia o a estudios previos, suele tener conjeturas sobre la población estudiada que expresa en forma de hipótesis.&lt;/p&gt;
&lt;div class=&#34;alert alert-def&#34;&gt;
  &lt;div&gt;
    &lt;strong&gt;Definición - Hipótesis estadística&lt;/strong&gt;. Una &lt;em&gt;hipótesis estadística&lt;/em&gt; es cualquier afirmación o conjetura que determina, total o parcialmente, la distribución de una o varias variables de la población.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Ejemplo&lt;/strong&gt;. Para contrastar el rendimiento académico de un grupo de alumnos en una determinada asignatura, podríamos platear la hipótesis de si el porcentaje de aprobados es mayor del 50%.&lt;/p&gt;
&lt;h3 id=&#34;contraste-de-hipótesis&#34;&gt;Contraste de hipótesis&lt;/h3&gt;
&lt;p&gt;En general nunca se sabrá con absoluta certeza si una hipótesis estadística es cierta o falsa, ya que para ello habría que estudiar a todos los individuos de la población.&lt;/p&gt;
&lt;p&gt;Para comprobar la veracidad o falsedad de estas hipótesis hay que contrastarlas con los resultados empíricos obtenidos de las muestras. Si los resultados observados en las muestras coinciden, dentro del margen de error admisible debido al azar, con lo que cabría esperar en caso de que la hipótesis fuese cierta, la hipótesis se aceptará como verdadera, mientras que en caso contrario se rechazará como falsa y se buscarán nuevas hipótesis capaces de explicar los datos observados.&lt;/p&gt;
&lt;p&gt;Como las muestras se obtienen aleatoriamente, &lt;em&gt;la decisión de aceptar o rechazar una hipótesis estadística se tomará sobre una base de probabilidad&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;La metodología que se encarga de contrastar la veracidad de las hipótesis estadísticas se conoce como &lt;em&gt;contraste de hipótesis&lt;/em&gt;.&lt;/p&gt;
&lt;h3 id=&#34;tipos-de-contrastes-de-hipótesis&#34;&gt;Tipos de contrastes de hipótesis&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Contrastes de bondad de ajuste&lt;/strong&gt;: El objetivo es comprobar una hipótesis sobre la forma de la distribución de la población.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Ejemplo&lt;/strong&gt;. Contrastar si las notas de un grupo de alumnos siguen una distribución normal.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Contrastes de conformidad&lt;/strong&gt;: El objetivo es comprobar una hipótesis sobre alguno de los parámetros de la población.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Ejemplo&lt;/strong&gt;. Contrastar si las nota media en un grupo de alumnos es igual a 5.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Contrastes de homogeneidad&lt;/strong&gt; : El objetivo es comparar dos poblaciones con respecto a alguno de sus parámetros.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Ejemplo&lt;/strong&gt;. Contrastar si el rendimiento de dos grupos de alumnos es el mismo comparando sus notas medias.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Contrastes de independencia&lt;/strong&gt;: El objetivo es comprobar si existe relación entre dos variables de la población.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Ejemplo&lt;/strong&gt;. Contrastar si existe relación entre la notas de dos asignaturas diferentes.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Cuando las hipótesis se plantean sobre parámetros de la población, también se habla de &lt;strong&gt;contrastes paramétricos&lt;/strong&gt;.&lt;/p&gt;
&lt;h2 id=&#34;planteamiento-de-un-contraste-de-hipótesis&#34;&gt;Planteamiento de un contraste de hipótesis&lt;/h2&gt;
&lt;h3 id=&#34;hipótesis-nula-e-hipótesis-alternativa&#34;&gt;Hipótesis nula e hipótesis alternativa&lt;/h3&gt;
&lt;p&gt;En la mayoría de los casos un contraste supone tomar una decisión entre dos hipótesis antagonistas:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Hipótesis nula&lt;/strong&gt;: Es la hipótesis conservadora, ya que se mantendrá mientras que los datos de las muestras no reflejen claramente su falsedad. Se representa como $H_0$.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Hipótesis alternativa&lt;/strong&gt;: Es la negación de la hipótesis nula y generalmente representa la afirmación que se pretende probar. Se representa como $H_1$.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Ambas hipótesis se eligen de acuerdo con el principio de simplicidad científica:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;“Solamente se debe abandonar un modelo simple por otro más complejo cuando la evidencia a favor del último sea fuerte.”&lt;/em&gt; (Navaja de Occam)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;elección-de-las-hipótesis-nula-y-alternativa&#34;&gt;Elección de las hipótesis nula y alternativa&lt;/h3&gt;
&lt;h4 id=&#34;analogía-con-un-juicio&#34;&gt;Analogía con un juicio&lt;/h4&gt;
&lt;p&gt;En el caso de un juicio, en el que el juez debe decidir si el acusado es culpable o inocente, la elección de hipótesis debería ser&lt;/p&gt;
&lt;p&gt;$$
\begin{align*}
H_0: &amp;amp; \mbox{ Inocente}\newline
H_1: &amp;amp; \mbox{ Culpable}
\end{align*}
$$&lt;/p&gt;
&lt;p&gt;ya que la inocencia se asume, mientras que la culpabilidad hay que demostrarla.&lt;/p&gt;
&lt;p&gt;Según esto, el juez sólo aceptaría la hipótesis alternativa cuando hubiese pruebas significativas de la culpabilidad del acusado.&lt;/p&gt;
&lt;p&gt;El investigador jugaría el papel del fiscal, ya que su objetivo consistiría en intentar rechazar la hipótesis nula, es decir, demostrar culpabilidad del acusado.&lt;/p&gt;
&lt;div class=&#34;alert alert-warning&#34;&gt;
  &lt;div&gt;
    ¡Esta metodología siempre favorece a la hipótesis nula!
  &lt;/div&gt;
&lt;/div&gt;
&lt;h3 id=&#34;contrastes-de-hipótesis-paramétricos&#34;&gt;Contrastes de hipótesis paramétricos&lt;/h3&gt;
&lt;p&gt;En muchos contrastes, sobre todo en las pruebas de conformidad y de homogeneidad, las hipótesis se formulan sobre parámetros desconocidos de la población como puede ser una media, una varianza o una proporción.&lt;/p&gt;
&lt;p&gt;En tal caso, la hipótesis nula siempre asigna al parámetro un valor concreto, mientras que la alternativa suele ser una hipótesis abierta que, aunque opuesta a la hipótesis nula, no fija el valor del parámetro.&lt;/p&gt;
&lt;p&gt;Esto da lugar a tres tipos de contrastes:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Bilateral&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Unilateral menor&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Unilateral mayor&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$H_0$: $\theta = \theta_0$&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$H_0$: $\theta = \theta_0$&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;$H_0$: $\theta = \theta_0$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$H_1$: $\theta \neq \theta_0$&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$H_1$: $\theta &amp;lt; \theta_0$&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;$H_1$: $\theta &amp;gt; \theta_0$&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;elección-del-tipo-de-contraste&#34;&gt;Elección del tipo de contraste&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Ejemplo&lt;/strong&gt;. Supóngase que existen sospechas de que en una población hay menos hombres que mujeres.&lt;/p&gt;
&lt;p&gt;¿Qué tipo de contraste debería plantearse para validar o refutar esta sospecha?&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Las sospechas se refieren al porcentaje o la proporción $p$ de hombres en la población, por lo que se trata de un &lt;em&gt;contraste paramétrico&lt;/em&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;El objetivo es averiguar el valor de $p$, por lo que se trata de una &lt;em&gt;prueba de conformidad&lt;/em&gt;. En la hipótesis nula el valor de $p$ se fijará a $0.5$ ya que, de acuerdo a las leyes de la genética, en la población debería haber la misma proporción de hombres que de mujeres.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Finalmente, existen sospechas de que el porcentaje de hombres es menor que el de mujeres, por lo que la hipótesis alternativa será de menor $p&amp;lt;0.5$.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Así pues, el contraste que debería plantearse es el siguiente:&lt;/p&gt;
&lt;p&gt;$$
\begin{align*}
H_0: &amp;amp; p=0.5\newline
H_1: &amp;amp; p&amp;lt;0.5
\end{align*}
$$&lt;/p&gt;
&lt;h3 id=&#34;estadístico-del-contraste&#34;&gt;Estadístico del contraste&lt;/h3&gt;
&lt;p&gt;La aceptación o rechazo de la hipótesis nula depende, en última instancia, de lo que se observe en la muestra.&lt;/p&gt;
&lt;p&gt;La decisión se tomará según el valor que presente algún estadístico de la muestra relacionado con el parámetro o característica que se esté contrastando, y cuya distribución de probabilidad debe ser conocida suponiendo cierta la hipótesis nula y una vez fijado el tamaño de la muestra. Este estadístico recibe el nombre de &lt;strong&gt;estadístico del contraste&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Para cada muestra, el estadístico dará una estimación a partir de la cual se tomará la decisión: &lt;em&gt;si la estimación difiere demasiado del valor esperado bajo la hipótesis $H_0$, entonces se rechazará, y en caso contrario se aceptará.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;La lógica que guía la decisión es la de mantener la hipótesis nula a no ser que en la muestra haya pruebas contundentes de su falsedad. Siguiendo con el símil del juicio, se trataría de mantener la inocencia mientras no haya pruebas claras de culpabilidad.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Ejemplo&lt;/strong&gt;. Volviendo al ejemplo del contraste sobre la proporción de hombres de una población&lt;/p&gt;
&lt;p&gt;$$
\begin{align*}
H_0: &amp;amp; p=0.5\newline
H_1: &amp;amp; p&amp;lt;0.5
\end{align*}
$$&lt;/p&gt;
&lt;p&gt;Si para resolver el contraste se toma una muestra aleatoria de 10 personas, podría tomarse como estadístico del contraste $X$ el número de hombres en la muestra.&lt;/p&gt;
&lt;p&gt;Suponiendo cierta la hipótesis nula, el estadístico del contraste seguiría una distribución binomial $X\sim B(10,,0.5)$, de manera que el número esperado de hombres en la muestra sería 5.&lt;/p&gt;
&lt;p&gt;Así pues, es lógico aceptar la hipótesis nula si en la muestra se obtiene un número de hombres próximo a 5 y rechazarla cuando el número de hombres sea muy inferior a 5. Pero, &lt;em&gt;¿dónde poner el límite entre los valores $X$ que lleven a la aceptación y los que lleven al rechazo?&lt;/em&gt;&lt;/p&gt;
&lt;h3 id=&#34;regiones-de-aceptación-y-de-rechazo&#34;&gt;Regiones de aceptación y de rechazo&lt;/h3&gt;
&lt;p&gt;Una vez elegido el estadístico del contraste, lo siguiente es decidir para qué valores de este estadístico se decidirá aceptar la hipótesis nula y para que valores se rechazará. Esto divide del conjunto de valores posibles del estadístico en dos regiones:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Región de aceptación&lt;/strong&gt;: Es el conjunto de valores del estadístico del contraste a partir de los cuales se decidirá aceptar la hipótesis nula.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Región de rechazo&lt;/strong&gt;: Es el conjunto de valores del estadístico del contraste a partir de los cuales se decidirá rechazar la hipótesis nula, y por tanto, aceptar la hipótesis alternativa.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Dependiendo de la dirección del contraste, la región de rechazo quedará a un lado u otro del valor esperado del estadístico del contraste según la hipótesis nula:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Contraste bilateral $H_0:\ \theta=\theta_0$ $H_1:\ \theta\neq\theta_0$.
&lt;img src=&#34;../img/contrastes/regiones_bilateral.svg&#34; alt=&#34;Regiones de un contraste bilateral&#34; width=&#34;700&#34;&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Contraste unilateral de menor $H_0:\ \theta=\theta_0$ &amp;amp;H_1:\ \theta&amp;lt;\theta_0$.
&lt;img src=&#34;../img/contrastes/regiones_unilateral_menor.svg&#34; alt=&#34;Regiones de un contraste unilateral de menor&#34; width=&#34;700&#34;&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Contraste unilateral de mayor $H_0:\ \theta=\theta_0$ $H_1:\ \theta&amp;gt;\theta_0$.
&lt;img src=&#34;../img/contrastes/regiones_unilateral_mayor.svg&#34; alt=&#34;Regiones de un contraste unilateral de mayor&#34; width=&#34;700&#34;&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Ejemplo&lt;/strong&gt;. Siguiendo con el ejemplo del contraste sobre la proporción de hombres de una población&lt;/p&gt;
&lt;p&gt;$$
\begin{align*}
H_0: &amp;amp; p=0.5\newline
H_1: &amp;amp; p&amp;lt;0.5
\end{align*}
$$&lt;/p&gt;
&lt;p&gt;Como el estadístico del contraste tenía una distribución binomial $X\sim B(10,,0.5)$ suponiendo cierta la hipótesis nula, su recorrido será de 0 a 10 y su valor esperado 5, por lo que, al tratarse de un contraste unilateral de menor, la región de rechazo quedará por debajo del 5. Pero, &lt;em&gt;¿dónde poner el límite entre las regiones de aceptación y de rechazo?&lt;/em&gt;&lt;/p&gt;
&lt;img src=&#34;../img/contrastes/regiones_contraste_proporcion_hombres.svg&#34; alt=&#34;Regiones de un contraste sobre la proporción de hombres en una muestra de tamño 10.&#34; width=&#34;700&#34;&gt;
&lt;h3 id=&#34;errores-en-un-contraste-de-hipótesis&#34;&gt;Errores en un contraste de hipótesis&lt;/h3&gt;
&lt;p&gt;Hemos visto que un contraste de hipótesis se realiza mediante una regla de decisión que permite aceptar o rechazar la hipótesis nula dependiendo del valor que tome el estadístico del contraste.&lt;/p&gt;
&lt;p&gt;Al final el contraste se resuelve tomando una decisión de acuerdo a esta regla. El problema es que nunca se conocerá con absoluta certeza la veracidad o falsedad de una hipótesis, de modo que al aceptarla o rechazarla es posible que se esté tomando una decisión equivocada.&lt;/p&gt;
&lt;p&gt;Los errores que se pueden cometer en un contraste de hipótesis son de dos tipos:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Error de tipo I&lt;/strong&gt;: Se comete cuando se rechaza la hipótesis nula siendo esta verdadera.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Error de tipo II&lt;/strong&gt;: Se comete cuando se acepta la hipótesis nula siendo esta falsa.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
\begin{array}{|c|c|c|}
\hline
\mbox{Decisión} &amp;amp; H_0 \mbox{ cierta} &amp;amp; H_1 \mbox{ cierta}\newline
\hline
\mbox{Aceptar } H_0 &amp;amp; \color{green}{\mbox{Decisión correcta}} &amp;amp; \color{red}{\mbox{Error de tipo II}}\newline
\hline
\mbox{Rechazar }H_0 &amp;amp; \color{red}{\mbox{Error de tipo I}} &amp;amp; \color{green}{\mbox{Decisión correcta}}\newline
\hline
\end{array}
$$&lt;/p&gt;
&lt;h3 id=&#34;riesgos-de-los-errores-de-un-contraste-de-hipótesis&#34;&gt;Riesgos de los errores de un contraste de hipótesis&lt;/h3&gt;
&lt;p&gt;Los riesgos de cometer cada tipo de error se cuantifican mediante probabilidades:&lt;/p&gt;
&lt;div class=&#34;alert alert-def&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;&lt;strong&gt;Definición - Riesgos $\alpha$ y $\beta$&lt;/strong&gt;. En un contraste de hipótesis, se define el &lt;em&gt;riesgo&lt;/em&gt; $\alpha$ como la máxima probabilidad de cometer un error de tipo I, es decir,&lt;/p&gt;
&lt;p&gt;$$P(\mbox{Rechazar }H_0|H_0) \leq \alpha,$$&lt;/p&gt;
&lt;p&gt;y se define el &lt;em&gt;riesgo&lt;/em&gt; $\beta$ como la máxima probabilidad de cometer un error de tipo II, es decir,&lt;/p&gt;
&lt;p&gt;$P(\mbox{Aceptar }H_0|H_1) \leq \beta.$$&lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;h3 id=&#34;interpretación-del-riesgo-alpha&#34;&gt;Interpretación del riesgo $\alpha$&lt;/h3&gt;
&lt;div class=&#34;alert alert-warning&#34;&gt;
  &lt;div&gt;
    En principio, puesto que esta metodología favorece a la hipótesis nula, el error del tipo I suele ser más grave que el error del tipo II, y por tanto, el riesgo $\alpha$ suele fijarse a niveles bajos de $0.1$, $0.05$ o $0.01$, siendo $0.05$ lo más habitual.
  &lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;alert alert-int&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;Debe tenerse cuidado al interpretar el riesgo $\alpha$ ya que se trata de una probabilidad condicionada a que la hipótesis nula sea cierta. Por tanto, cuando se rechace la hipótesis nula con un riesgo $\alpha=0.05$, es erróneo decir 5 de cada 100 veces nos equivocaremos, ya que esto sería cierto sólo si la hipótesis nula fuese siempre verdadera.&lt;/p&gt;
&lt;p&gt;Tampoco tiene sentido hablar de la probabilidad de haberse equivocado una vez tomada una decisión a partir de una muestra concreta, pues en tal caso, si se ha tomado la decisión acertada, la probabilidad de error es 0 y si se ha tomado la decisión equivocada, la probabilidad de error es 1.&lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;h3 id=&#34;determinación-de-las-regiones-de-aceptación-y-de-rechazo-en-función-del-riesgo-alpha&#34;&gt;Determinación de las regiones de aceptación y de rechazo en función del riesgo $\alpha$&lt;/h3&gt;
&lt;p&gt;Una vez fijado el riesgo $\alpha$ que se está dispuesto a tolerar, es posible delimitar las regiones de aceptación y de rechazo para el estadístico del contraste de manera que la probabilidad acumulada en la región de rechazo sea $\alpha$, suponiendo cierta la hipótesis nula.&lt;/p&gt;
&lt;img src=&#34;../img/contrastes/regiones_bilateral_normal.svg&#34; alt=&#34;Regiones de un contraste bilateral en una distribución normal&#34; width=&#34;600&#34;&gt;
&lt;img src=&#34;../img/contrastes/regiones_unilateral_mayor_normal.svg&#34; alt=&#34;Regiones de un contraste unilateral de mayor en una distribución normal&#34; width=&#34;600&#34;&gt;
&lt;p&gt;&lt;strong&gt;Ejemplo&lt;/strong&gt;. Siguiendo con el contraste sobre la proporción de hombres de una población, como el estadístico del contraste sigue una distribución binomial $X\sim B(10,0.5)$, si se decide rechazar la hipótesis nula
cuando en la muestra haya 2 o menos hombres, la probabilidad de cometer un error de tipo I será&lt;/p&gt;
&lt;p&gt;$$P(X\leq 2)= f(0)+f(1)+f(2)= 0.0010 + 0.0098 + 0.0439 = 0.0547.$$&lt;/p&gt;
&lt;p&gt;Si riesgo máximo de error de tipo I que se está dispuesto a tolerar es $\alpha=0.05$, ¿qué valores del estadístico permitirán rechazar la hipótesis nula? $$P(X\leq 1)= f(0)+f(1) = 0.0010 + 0.0098 = 0.0107.$$ Es
decir, sólo se podría rechazar la hipótesis nula con 0 o 1 hombres en la muestra.&lt;/p&gt;
&lt;img src=&#34;../img/contrastes/regiones_contraste_proporcion_hombres_2.svg&#34; alt=&#34;Regiones de un contraste sobre la proporción de hombres en una muestra de tamño 10.&#34; width=&#34;700&#34;&gt;
&lt;h3 id=&#34;riesgo-beta-y-tamaño-del-efecto&#34;&gt;Riesgo $\beta$ y tamaño del efecto&lt;/h3&gt;
&lt;p&gt;Aunque el error de tipo II pueda parecer menos grave, también interesa que el riesgo $\beta$ sea bajo, ya que de lo contrario será difícil rechazar la hipótesis nula (que es lo que se persigue la mayoría de las veces), aunque haya pruebas muy claras de su falsedad.&lt;/p&gt;
&lt;p&gt;El problema, en el caso de contrastes paramétricos, es que la hipótesis alternativa es una hipótesis abierta en la que no se fija el valor del parámetro a contrastar, de modo que, para poder calcular el riesgo $\beta$ es necesario fijar dicho valor.&lt;/p&gt;
&lt;p&gt;Lo normal es fijar el valor del parámetro del contraste a la mínima cantidad para admitir diferencias significativas desde un punto de vista práctico o clínico. Esa mínima diferencia que se considera clínicamente significativa se conoce como &lt;strong&gt;tamaño del efecto&lt;/strong&gt; y se representa por $\delta$.&lt;/p&gt;
&lt;h3 id=&#34;potencia-de-un-contraste&#34;&gt;Potencia de un contraste&lt;/h3&gt;
&lt;p&gt;Puesto que el objetivo del investigador suele ser rechazar la hipótesis nula, a menudo, lo más interesante de un contraste es su capacidad para detectar la falsedad de la hipótesis nula cuando realmente hay diferencias mayores que $\delta$ entre el verdadero valor del parámetro y el que establece la hipótesis nula.&lt;/p&gt;
&lt;div class=&#34;alert alert-def&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;&lt;strong&gt;Definición - Potencia de un contraste&lt;/strong&gt;. La &lt;em&gt;potencia&lt;/em&gt; de un contraste de hipótesis se define como&lt;/p&gt;
&lt;p&gt;$$\mbox{Potencia} = P(\mbox{Rechazar }H_0|H_1) = 1 - P(\mbox{Aceptar }H_0|H_1) = 1-\beta.$$&lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Así pues, al reducir el riesgo $\beta$ se aumentará la potencia del contraste.&lt;/p&gt;
&lt;p&gt;Un contraste poco potente no suele ser interesante ya que no permitirá rechazar la hipótesis nula aunque haya evidencias en su contra.&lt;/p&gt;
&lt;h3 id=&#34;cálculo-del-riesgo-beta-y-de-la-potencia-1-beta&#34;&gt;Cálculo del riesgo $\beta$ y de la potencia $1-\beta$&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Ejemplo&lt;/strong&gt; Supóngase que en el contraste sobre la proporción de hombres no se considera importante una diferencia de menos de un 10% con respecto al valor que establece la hipótesis nula, es decir, $\delta=0.1$.&lt;/p&gt;
&lt;p&gt;Esto permite fijar la hipótesis alternativa&lt;/p&gt;
&lt;p&gt;$$H_1:\ p=0.5-0.1=0.4.$$&lt;/p&gt;
&lt;p&gt;Suponiendo cierta esta hipótesis el estadístico del contraste seguiría una distribución binomial $X\sim B(10,,0.4)$.&lt;/p&gt;
&lt;p&gt;En tal caso, el riesgo $\beta$ para las regiones de aceptación y rechazo fijadas antes será&lt;/p&gt;
&lt;p&gt;$$\beta = P(\mbox{Aceptar }H_0|H_1) = P(X\geq 2) = 1 - P(X&amp;lt;2) = 1-0.0464 = 0.9536.$$&lt;/p&gt;
&lt;p&gt;Como puede apreciarse, se trata de un riesgo $\beta$ muy alto, por lo que la potencia del contraste sería sólo de&lt;/p&gt;
&lt;p&gt;$$1-\beta = 1-0.9536 = 0.0464,$$&lt;/p&gt;
&lt;p&gt;lo que indica que no se trataría de un buen contraste para detectar diferencias de un 10% en el valor del parámetro.&lt;/p&gt;
&lt;h3 id=&#34;relación-del-riesgo-beta-y-el-tamaño-del-efecto-delta&#34;&gt;Relación del riesgo $\beta$ y el tamaño del efecto $\delta$&lt;/h3&gt;
&lt;p&gt;El riesgo $\beta$ depende directamente de la mínima diferencia $\delta$ que se desea detectar con respecto al valor del parámetro que establece la hipótesis nula.&lt;/p&gt;
&lt;img src=&#34;../img/contrastes/riesgo_beta_tamaño_efecto_pequeño.svg&#34; alt=&#34;Riesgo beta para un tamaño del efecto pequeño.&#34; width=&#34;600&#34;&gt;
&lt;img src=&#34;../img/contrastes/riesgo_beta_tamaño_efecto_grande.svg&#34; alt=&#34;Riesgo beta para un tamaño del efecto grande.&#34; width=&#34;600&#34;&gt;
&lt;p&gt;&lt;strong&gt;Ejemplo&lt;/strong&gt;. Si en el contraste sobre la proporción de hombres se desease detectar una diferencia de al menos un 20% con respecto al valor que establece la hipótesis nula, es decir, $\delta=0.2$, entonces la hipótesis alternativa se fijaría a&lt;/p&gt;
&lt;p&gt;$$H_1:\ p=0.5-0.2=0.3,$$&lt;/p&gt;
&lt;p&gt;y bajo esta hipótesis el estadístico del contraste seguiría una distribución binomial $X\sim B(10,,0.3)$.&lt;/p&gt;
&lt;p&gt;En tal caso, el riesgo $\beta$ para las regiones de aceptación y rechazo fijadas antes sería&lt;/p&gt;
&lt;p&gt;$$\beta = P(\mbox{Aceptar }H_0|H_1) = P(X\geq 2) = 1 - P(X&amp;lt;2) = 1-0.1493 = 0.8507,$$&lt;/p&gt;
&lt;p&gt;por lo que el riesgo riesgo $\beta$ disminuiría y la potencia del contraste aumentaría&lt;/p&gt;
&lt;p&gt;$$1-\beta = 1-0.8507 = 0.1493,$$&lt;/p&gt;
&lt;p&gt;aunque seguiría siendo un contraste poco potente.&lt;/p&gt;
&lt;h3 id=&#34;relación-entre-los-riesgos-alpha-y-beta&#34;&gt;Relación entre los riesgos $\alpha$ y $\beta$&lt;/h3&gt;
&lt;p&gt;Los riesgos $\alpha$ y $\beta$ están enfrentados, es decir, cuando uno aumenta el otro disminuye y viceversa.&lt;/p&gt;
&lt;img src=&#34;../img/contrastes/relacion_riesgos_alpha_pequeño.svg&#34; alt=&#34;Riesgo beta para un tamaño del efecto pequeño.&#34; width=&#34;600&#34;&gt;
&lt;img src=&#34;../img/contrastes/relacion_riesgos_alpha_grande.svg&#34; alt=&#34;Riesgo beta para un tamaño del efecto grande.&#34; width=&#34;600&#34;&gt;
&lt;p&gt;&lt;strong&gt;Ejemplo&lt;/strong&gt;. Si en el contraste sobre la proporción de hombres toma como riesgo $\alpha=0.1,$ entonces la región de rechazo sería $X\leq 2$ ya que, suponiendo cierta la hipótesis nula, $X\sim B(10,, 0.5)$, y&lt;/p&gt;
&lt;p&gt;$$P(X\leq 2) = 0.0547 \leq 0.1=\alpha.$$&lt;/p&gt;
&lt;p&gt;Entonces, para una diferencia mínima $\delta=0.1$ y suponiendo cierta la hipótesis alternativa,
$X\sim B(10,,0.4)$, el riesgo $\beta$ será&lt;/p&gt;
&lt;p&gt;$$\beta = P(\mbox{Aceptar }H_0|H_1) = P(X\geq 3) = 1- P(X&amp;lt;3) = 1-0.1673 = 0.8327,$$&lt;/p&gt;
&lt;p&gt;y ahora la potencia ha subido hasta&lt;/p&gt;
&lt;p&gt;$$1-\beta = 1-0.8327 = 0.1673.$$&lt;/p&gt;
&lt;h3 id=&#34;relación-de-los-riesgos-de-error-y-el-tamaño-muestral&#34;&gt;Relación de los riesgos de error y el tamaño muestral&lt;/h3&gt;
&lt;p&gt;Los riesgos de error también dependen el tamaño de la muestra, ya que al aumentar el tamaño de la muestra, la dispersión del estadístico del contraste disminuye y con ello también lo hacen los riesgos de error.&lt;/p&gt;
&lt;img src=&#34;../img/contrastes/relacion_riesgos_tamaño_muestral_pequeño.svg&#34; alt=&#34;Riesgo beta para un tamaño del efecto grande.&#34; width=&#34;600&#34;&gt;
&lt;img src=&#34;../img/contrastes/relacion_riesgos_tamaño_muestral_grande.svg&#34; alt=&#34;Riesgo beta para un tamaño del efecto grande.&#34; width=&#34;600&#34;&gt;
&lt;p&gt;&lt;strong&gt;Ejemplo&lt;/strong&gt;. Si para realizar el contraste sobre la proporción de hombres se hubiese
tomado una muestra de tamaño 100, en lugar de 10, entonces, bajo la suposición de certeza de la hipótesis nula, el estadístico del contraste seguiría una distribución binomial $B(100,,0.5)$, y ahora la región de rechazo sería $X\leq 41$, ya que&lt;/p&gt;
&lt;p&gt;$$P(X\leq 41) = 0.0443 \leq 0.05 =\alpha.$$&lt;/p&gt;
&lt;p&gt;Entonces, para $\delta=0.1$ y suponiendo cierta la hipótesis alternativa, $X\sim B(100,,0.4)$, el
riesgo $\beta$ sería&lt;/p&gt;
&lt;p&gt;$$\beta = P(\mbox{Aceptar }H_0|H_1) = P(X\geq 42) = 0.3775,$$&lt;/p&gt;
&lt;p&gt;y ahora la potencia habría aumentado considerablemente&lt;/p&gt;
&lt;p&gt;$$1-\beta = 1-0.3775 = 0.6225.$$&lt;/p&gt;
&lt;p&gt;Este contraste sería mucho más útil para detectar una diferencia de al menos un 10% con respecto al valor del parámetro que establece la hipótesis nula.&lt;/p&gt;
&lt;h3 id=&#34;curva-de-potencia&#34;&gt;Curva de potencia&lt;/h3&gt;
&lt;p&gt;La potencia de un contraste depende del valor del parámetro que establezca la hipótesis alternativa y, por tanto, es una función de este&lt;/p&gt;
&lt;p&gt;$$\mbox{Potencia}(x)= P(\mbox{Rechazar }H_0|\theta=x).$$&lt;/p&gt;
&lt;p&gt;Esta función da la probabilidad de rechazar la hipótesis nula para cada valor del parámetro y se conoce como &lt;strong&gt;curva de potencia&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Cuando no se puede fijar el valor concreto del parámetro en la hipótesis alternativa, resulta útil representar esta curva para ver la bondad del contraste cuando no se rechaza la hipótesis nula. También es útil cuando sólo de dispone de un número determinado de individuos en la muestra, para ver si merece la pena hacer el estudio.&lt;/p&gt;
&lt;div class=&#34;alert alert-int&#34;&gt;
  &lt;div&gt;
    Un contraste será mejor cuanto mayor sea el área encerrada por debajo de la curva de potencia.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Ejemplo&lt;/strong&gt;. La curva de potencia correspondiente al contraste sobre la proporción de hombres en la población es la siguiente&lt;/p&gt;
&lt;img src=&#34;../img/contrastes/potencia.svg&#34; alt=&#34;Curva de potencia para muestras de distinto tamaño.&#34; width=&#34;600&#34;&gt;
&lt;h3 id=&#34;p-valor-de-un-contraste-de-hipótesis&#34;&gt;$p$-valor de un contraste de hipótesis&lt;/h3&gt;
&lt;p&gt;En general, siempre que la estimación del estadístico caiga dentro de la región de rechazo, rechazaremos la hipótesis nula, pero evidentemente, si dicha estimación se aleja bastante de la región de aceptación tendremos más confianza en el rechazo que si la estimación está cerca del límite entre las regiones de aceptación y rechazo.&lt;/p&gt;
&lt;p&gt;Por este motivo, al realizar un contraste, también se calcula la probabilidad de obtener una discrepancia mayor o igual a la observada entre la estimación del estadístico del contraste y su valor esperado según la hipótesis nula.&lt;/p&gt;
&lt;div class=&#34;alert alert-def&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;&lt;strong&gt;Definición- $p$-valor&lt;/strong&gt;. En un contraste de hipótesis, para cada estimación $x_0$ del estadístico del contraste $X$, dependiendo del tipo de contraste, se define el $p$-valor del contraste como&lt;/p&gt;
&lt;p&gt;$$
\begin{array}{lc}
\mbox{Contraste bilateral}: &amp;amp; 2P(X\geq x_0|H_0)\newline
\mbox{Contraste unilateral de menor}: &amp;amp; P(X\leq x_0|H_0)\newline
\mbox{Contraste unilateral de mayor}: &amp;amp; P(X\geq x_0|H_0)
\end{array}
$$&lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;alert alert-int&#34;&gt;
  &lt;div&gt;
    En cierto modo, el $p$-valor expresa la confianza que se tiene al tomar la decisión de rechazar la hipótesis nula. Cuanto más próximo esté el $p$-valor a 1, mayor confianza existe al aceptar la hipótesis nula, y cuanto más próximo esté a 0, mayor confianza hay al rechazarla.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Una vez fijado el riesgo $\alpha$, la regla de decisión para realizar un contraste también puede expresarse de la siguiente manera:&lt;/p&gt;
&lt;div class=&#34;alert alert-warning&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;&lt;strong&gt;Regla de decisión de un contraste&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;$$
\begin{array}{ccc}
\mbox{Si $p$-valor $\leq \alpha$} &amp;amp; \rightarrow &amp;amp; \mbox{Rechazar $H_0$}\newline
\mbox{Si $p$-valor $&amp;gt; \alpha$} &amp;amp; \rightarrow &amp;amp; \mbox{Aceptar $H_0$}.
\end{array}
$$&lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;De este modo, el $p$-valor nos da información de para qué niveles de significación puede rechazarse la hipótesis nula y para cuales no.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Ejemplo&lt;/strong&gt;. Si el contraste sobre la proporción de hombres se toma una muestra de tamaño 10 y se observa 1 hombre, entonces el $p$-valor, bajo a supuesta certeza de la hipótesis nula, $X\sim B(10,, 0.5)$, será&lt;/p&gt;
&lt;p&gt;$$p = P(X\leq 1)= 0.0107,$$&lt;/p&gt;
&lt;p&gt;mientras que si en la muestra se observan 0 hombres, entonces el $p$-valor será&lt;/p&gt;
&lt;p&gt;$$p = P(X\leq 0)= 0.001.$$&lt;/p&gt;
&lt;p&gt;En el primer caso se rechazaría la hipótesis nula para un riesgo $\alpha=0.05$, pero no podría rechazarse par un riesgo $\alpha=0.01$, mientas que en el segundo caso también se rechazaría para $\alpha=0.01$. Es evidente que en el segundo la decisión de rechazar la hipótesis nula se tomaría con mayor confianza.&lt;/p&gt;
&lt;h3 id=&#34;pasos-para-la-realización-de-un-contraste-de-hipótesis&#34;&gt;Pasos para la realización de un contraste de hipótesis&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Formular la hipótesis nula $H_0$ y la alternativa $H_1$.&lt;/li&gt;
&lt;li&gt;Fijar los riesgos $\alpha$ y $\beta$ deseados.&lt;/li&gt;
&lt;li&gt;Seleccionar el estadístico del contraste.&lt;/li&gt;
&lt;li&gt;Fijar la mínima diferencia clínicamente significativa (tamaño del efecto) $\delta$.&lt;/li&gt;
&lt;li&gt;Calcular el tamaño muestral necesario $n$.&lt;/li&gt;
&lt;li&gt;Delimitar las regiones de aceptación y rechazo.&lt;/li&gt;
&lt;li&gt;Tomar una muestra de tamaño $n$.&lt;/li&gt;
&lt;li&gt;Calcular el estadístico del contraste en la muestra.&lt;/li&gt;
&lt;li&gt;Rechazar la hipótesis nula si la estimación cae en la región de rechazo o bien si el $p$-valor es menor que el riesgo $\alpha$ y aceptarla en caso contrario.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;contrastes-paramétricos-más-importantes&#34;&gt;Contrastes paramétricos más importantes&lt;/h2&gt;
&lt;p&gt;Pruebas de conformidad:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Contraste para la media de una población normal con varianza conocida.&lt;/li&gt;
&lt;li&gt;Contraste para la media de una población normal con varianza desconocida.&lt;/li&gt;
&lt;li&gt;Contraste para la media de una población con varianza desconocida a partir de muestras grandes.&lt;/li&gt;
&lt;li&gt;Contraste para la varianza de una población normal.&lt;/li&gt;
&lt;li&gt;Contraste para un proporción de una población.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Pruebas de homogeneidad:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Contraste de comparación de medias de dos poblaciones normales con varianzas conocidas.&lt;/li&gt;
&lt;li&gt;Contraste de comparación de medias de dos poblaciones normales con varianzas desconocidas pero iguales.&lt;/li&gt;
&lt;li&gt;Contraste de comparación de medias de dos poblaciones normales con varianzas desconocidas y diferentes.&lt;/li&gt;
&lt;li&gt;Contraste de comparación de varianzas de dos poblaciones normales.&lt;/li&gt;
&lt;li&gt;Contraste de comparación de proporciones de dos poblaciones.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;pruebas-de-conformidad&#34;&gt;Pruebas de conformidad&lt;/h2&gt;
&lt;h3 id=&#34;contraste-para-la-media-de-una-población-normal-con-varianza-conocida&#34;&gt;Contraste para la media de una población normal con varianza conocida&lt;/h3&gt;
&lt;p&gt;Sea $X$ una variable aleatoria que cumple las siguientes condiciones:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Su distribución es normal $X\sim N(\mu,\sigma)$.&lt;/li&gt;
&lt;li&gt;La media $\mu$ es desconocida, pero su varianza $\sigma^2$ es conocida.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Contraste&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
H_0 &amp;amp;: \mu=\mu_0\newline
H_1 &amp;amp;: \mu\neq \mu_0\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Estadístico del contraste&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;$$
\bar x\sim N\left(\mu_0,\frac{\sigma}{\sqrt{n}}\right) \Rightarrow Z=\frac{\bar x-\mu_0}{\sigma/\sqrt{n}}\sim N(0,1).
$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Región de aceptación&lt;/strong&gt;: $z_{\alpha/2}&amp;lt; Z &amp;lt; z_{1-\alpha/2}$.&lt;br&gt;
&lt;strong&gt;Región de rechazo&lt;/strong&gt;: $Z\leq z_{\alpha/2}$ y $Z\geq z_{1-\alpha/2}$.&lt;/p&gt;
&lt;h3 id=&#34;contraste-para-la-media-de-una-población-normal-con-varianza-desconocida&#34;&gt;Contraste para la media de una población normal con varianza desconocida&lt;/h3&gt;
&lt;p&gt;Sea $X$ una variable aleatoria que cumple las siguientes condiciones:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Su distribución es normal $X\sim N(\mu,\sigma)$.&lt;/li&gt;
&lt;li&gt;Tanto su media $\mu$ como su varianza $\sigma^2$ son desconocidas.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Contraste&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
H_0 &amp;amp;: \mu=\mu_0\newline
H_1 &amp;amp;: \mu\neq \mu_0\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Estadístico del contraste&lt;/strong&gt;: Utilizando la cuasivarianza como estimador de la varianza poblacional se tiene&lt;/p&gt;
&lt;p&gt;$$
\bar x\sim N\left(\mu_0,\frac{\sigma}{\sqrt{n}}\right) \Rightarrow T=\frac{\bar x-\mu_0}{\hat s/\sqrt{n}}\sim T(n-1).
$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Región de aceptación&lt;/strong&gt;: $t^{n-1}_{\alpha/2} &amp;lt; T &amp;lt; t^{n-1}_{1-\alpha/2}$.&lt;br&gt;
&lt;strong&gt;Región de rechazo&lt;/strong&gt;: $T\leq t^{n-1}_{\alpha/2}$ y $T\geq t^{n-1}_{1-\alpha/2}$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Ejemplo&lt;/strong&gt;. En un grupo de alumnos se quiere contrastar si la nota media de estadística es mayor que 5 puntos. Para ello se toma la siguiente muestra:&lt;/p&gt;
&lt;div style=&#34;text-align:center&#34;&gt;
6.3, 5.4, 4.1, 5.0, 8.2, 7.6, 6.4, 5.6, 4.3, 5.2
&lt;/div&gt;
&lt;p&gt;El contraste que se plantea es&lt;/p&gt;
&lt;p&gt;$$H_0: \mu=5 \quad H_1: \mu&amp;gt;5$$&lt;/p&gt;
&lt;p&gt;Para realizar el contraste se tiene:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\bar x = \frac{6.3+\cdots+5.2}{10}=\frac{58.1}{10}=5.81$ puntos.&lt;/li&gt;
&lt;li&gt;$\hat s^2 = \frac{(6.3-5.56)^2+\cdots+(5.2-5.56)^2}{9} = \frac{15.949}{9}=1.7721$ puntos$^2$, y $\hat s=1.3312$ puntos.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Y el estadístico del contraste vale&lt;/p&gt;
&lt;p&gt;$$
T=\frac{\bar x-\mu_0}{\hat s/\sqrt{n}} = \frac{5.81-5}{1.3312/\sqrt{10}}= 1.9246.
$$&lt;/p&gt;
&lt;p&gt;El $p$-valor del contraste es $P(T(9)\geq 1.9246) = 0.04323$, lo que indica que se rechazaría la hipótesis nula para $\alpha=0.05$.&lt;/p&gt;
&lt;p&gt;La región de rechazo es&lt;/p&gt;
&lt;p&gt;$$
T=\frac{\bar x-5}{1.3312/\sqrt{10}} \geq t^9_{0.95} = 1.8331 \Leftrightarrow \bar x \geq 5+1.8331\frac{1.3312}{\sqrt
10} = 5.7717,
$$&lt;/p&gt;
&lt;p&gt;de modo que se rechazará la hipótesis nula siempre que la media de la muestra sea mayor que $5.7717$ y se aceptará en caso contrario.&lt;/p&gt;
&lt;p&gt;Suponiendo que en la práctica la mínima diferencia importante en la nota media fuese de un punto $\delta=1$, entonces bajo la hipótesis alternativa $H_1:\mu=6$, si se decidiese rechazar la hipótesis nula, el riesgo $\beta$ sería&lt;/p&gt;
&lt;p&gt;$$
\beta = P\left(T(9)\leq \frac{5.7717-6}{1.3312\sqrt 10}\right) = P(T(9)\leq -0.5424) = 0.3004,
$$&lt;/p&gt;
&lt;p&gt;de manera que la potencia del contraste para detectar una diferencia de $\delta=1$ punto sería $1-\beta=1-0.3004 = 0.6996$.&lt;/p&gt;
&lt;h3 id=&#34;determinación-del-tamaño-muestral-en-un-contraste-para-la-media&#34;&gt;Determinación del tamaño muestral en un contraste para la media&lt;/h3&gt;
&lt;p&gt;Se ha visto que para un riesgo $\alpha$ la región de rechazo era&lt;/p&gt;
&lt;p&gt;$$
T=\frac{\bar x-\mu_0}{\hat s/\sqrt{n}} \geq t^{n-1}&lt;em&gt;{1-\alpha} \approx z&lt;/em&gt;{1-\alpha}\quad \mbox{para } n\geq 30.
$$&lt;/p&gt;
&lt;p&gt;o lo que es equivalente&lt;/p&gt;
&lt;p&gt;$$
\bar x \geq \mu_0+z_{1-\alpha}\frac{\hat s}{\sqrt n}.
$$&lt;/p&gt;
&lt;p&gt;Si el tamaño del efecto es $\delta$, para una hipótesis  alternativa $H_1:\mu=\mu_0+\delta$, el riesgo $\beta$ es&lt;/p&gt;
&lt;p&gt;$$
\beta = P\left(Z&amp;lt; \frac{\mu_0+z_{1-\alpha}\frac{\hat s}{\sqrt n}-(\mu_0+\delta)}{\frac{\hat s}{\sqrt n}} \right) = P\left(Z&amp;lt; \frac{z_{1-\alpha}\frac{\hat s}{\sqrt n}-\delta}{\frac{\hat s}{\sqrt n}} \right).
$$&lt;/p&gt;
&lt;p&gt;de modo que&lt;/p&gt;
&lt;p&gt;$$
z_\beta = \frac{z_{1-\alpha}\frac{\hat s}{\sqrt n}-\delta}{\frac{\hat s}{\sqrt n}} \Leftrightarrow \delta = (z_{1-\alpha}-z_\beta)\frac{\hat s}{\sqrt n} \Leftrightarrow n = (z_{1-\alpha}-z_\beta)^2\frac{\hat s^2}{\delta^2} = (z_\alpha+z_\beta)^2\frac{\hat s^2}{\delta^2}.
$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Ejemplo&lt;/strong&gt;. Se ha visto en el ejemplo anterior que la potencia del contraste para detectar una diferencia en la nota media de 1 punto era del $69.96%$.
Para aumentar la potencia del test hasta un $90%$, ¿cuántos alumnos habría que tomar en la muestra?&lt;/p&gt;
&lt;p&gt;Como se desea una potencia $1-\beta=0.9$, el riesgo $\beta=0.1$ y mirando en la tabla de la normal estándar se puede comprobar que $z_\beta = z_{0.1}=1.2816$.&lt;/p&gt;
&lt;p&gt;Aplicando la fórmula anterior para determinar el tamaño muestral necesario, se tiene&lt;/p&gt;
&lt;p&gt;$$
n = (z_\alpha+z_\beta)^2\frac{\hat s^2}{\delta^2} = (1.6449+1.2816)^2\frac{1.7721}{1^2} = 15.18,
$$&lt;/p&gt;
&lt;p&gt;de manera que habría que haber tomado al menos 16 alumnos.&lt;/p&gt;
&lt;h3 id=&#34;contraste-para-la-media-de-una-población-con-varianza-desconocida-y-muestras-grandes-ngeq-30&#34;&gt;Contraste para la media de una población con varianza desconocida y muestras grandes $n\geq 30$&lt;/h3&gt;
&lt;p&gt;Sea $X$ una variable aleatoria que cumple las siguientes condiciones:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Su distribución puede ser de cualquier tipo.&lt;/li&gt;
&lt;li&gt;Tanto su media $\mu$ como su varianza $\sigma^2$ son desconocidas.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Contraste&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
H_0 &amp;amp;: \mu=\mu_0\newline
H_1 &amp;amp;: \mu\neq \mu_0
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Estadístico del contraste&lt;/strong&gt;: Utilizando la cuasivarianza como estimador de la varianza poblacional y gracias al teorema central del límite por tratarse de muestras grandes ($n\geq 30)$ se tiene&lt;/p&gt;
&lt;p&gt;$$
\bar x\sim N\left(\mu_0,\frac{\sigma}{\sqrt{n}}\right) \Rightarrow Z=\frac{\bar x-\mu_0}{\hat s/\sqrt{n}}\sim N(0,1).
$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Región de aceptación&lt;/strong&gt;: $-z_{\alpha/2}&amp;lt; Z &amp;lt; z_{\alpha/2}$.&lt;br&gt;
&lt;strong&gt;Región de rechazo&lt;/strong&gt;: $Z\leq -z_{\alpha/2}$ y $Z\geq z_{\alpha/2}$.&lt;/p&gt;
&lt;h3 id=&#34;contraste-para-la-varianza-de-una-población-normal&#34;&gt;Contraste para la varianza de una población normal&lt;/h3&gt;
&lt;p&gt;Sea $X$ una variable aleatoria que cumple las siguientes hipótesis:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Su distribución es normal $X\sim N(\mu,\sigma)$.&lt;/li&gt;
&lt;li&gt;Tanto su media $\mu$ como su varianza $\sigma^2$ son desconocidas.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Contraste&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
H_0 &amp;amp;: \sigma=\sigma_0\newline
H_1 &amp;amp;: \sigma\neq \sigma_0
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Estadístico del contraste&lt;/strong&gt;: Partiendo de la cuasivarianza muestral como estimador de la varianza poblacional, se tiene&lt;/p&gt;
&lt;p&gt;$$
J=\frac{nS^2}{\sigma_0^2} = \frac{(n-1)\hat{S}^2}{\sigma_0^2}\sim \chi^2(n-1),
$$&lt;/p&gt;
&lt;p&gt;que sigue una distribución chi-cuadrado de $n-1$ grados de libertad.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Región de aceptación&lt;/strong&gt;: $\chi_{\alpha/2}^{n-1} &amp;lt; J &amp;lt; \chi_{1-\alpha/2}^{n-1}$.&lt;br&gt;
&lt;strong&gt;Región de rechazo&lt;/strong&gt;: $J\leq \chi_{\alpha/2}^{n-1}$ y $J\geq \chi_{1-\alpha/2}^{n-1}$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Ejemplo&lt;/strong&gt;. En un grupo de alumnos se quiere contrastar si la desviación típica de la nota es mayor de 1 punto. Para ello se toma la siguiente muestra:&lt;/p&gt;
&lt;div style=&#34;text-align:center&#34;&gt;
6.3, 5.4, 4.1, 5.0, 8.2, 7.6, 6.4, 5.6, 4.3, 5.2
&lt;/div&gt;
&lt;p&gt;El contraste que se plantea es&lt;/p&gt;
&lt;p&gt;$$
H_0: \sigma=1 \quad H_1: \sigma&amp;gt;1
$$&lt;/p&gt;
&lt;p&gt;Para realizar el contraste se tiene:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\bar x = \frac{6.3+\cdots+5.2}{10}=\frac{58.1}{10}=5.81$ puntos.&lt;/li&gt;
&lt;li&gt;$\hat s^2 = \frac{(6.3-5.56)^2+\cdots+(5.2-5.56)^2}{9} = \frac{15.949}{9}=1.7721$ puntos$^2$.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;El estadístico del contraste vale&lt;/p&gt;
&lt;p&gt;$$
J= \frac{(n-1)\hat{S}^2}{\sigma_0^2} = \frac{9\cdot1.7721}{1^2} = 15.949,
$$&lt;/p&gt;
&lt;p&gt;y el $p$-valor del contraste es $P(\chi(9)\geq 15.949) = 0.068$, por lo que no se puede rechazar la hipótesis nula para $\alpha=0.05$.&lt;/p&gt;
&lt;h3 id=&#34;contraste-para-proporción-de-una-población&#34;&gt;Contraste para proporción de una población&lt;/h3&gt;
&lt;p&gt;Sea $p$ la proporción de individuos de una población que tienen una determinada característica.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Contraste&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
H_0 &amp;amp;: p=p_0\newline
H_1 &amp;amp;: p\neq p_0
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Estadístico del contraste&lt;/strong&gt;: La variable que mide el número de individuos con la característica en una muestra aleatoria de tamaño $n$ sigue una distribución binomial $X\sim B(n,p_0)$. De acuerdo al teorema central del límite, para muestras grandes ($np\geq 5$ y $n(1-p)\geq 5$), $X\sim N(np_0,\sqrt{np_0(1-p_0)})$, y se cumple&lt;/p&gt;
&lt;p&gt;$$
\hat{p}=\frac{X}{n} \sim N\left(p_0,\sqrt{\frac{p_0(1-p_0)}{n}}\right) \Rightarrow Z = \frac{\hat
p-p_0}{\sqrt{p_0(1-p_0)/n}}\sim N(0,1).
$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Región de aceptación&lt;/strong&gt;: $z_{\alpha/2}&amp;lt; Z &amp;lt; z_{1-\alpha/2}$.&lt;br&gt;
&lt;strong&gt;Región de rechazo&lt;/strong&gt;: $Z\leq z_{\alpha/2}$ y $Z\geq z_{1-\alpha/2}$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Ejemplo&lt;/strong&gt;. En un grupo de alumnos se desea estimar si el porcentaje de aprobados es mayor del $50%$. Para ello se toma una muestra de 80 alumnos entre los que hay 50 aprobados.&lt;/p&gt;
&lt;p&gt;El contraste que se plantea es&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
H_0 &amp;amp;: p=0.5\newline
H_1 &amp;amp;: p&amp;gt;0.5
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;Para realizar el contraste se tiene que $\hat p= 50/80 = 0.625$ y como se cumple $n\hat p=80\cdot 0.625 = 50\geq 5$ y $n(1-\hat p)=80(1-0.625)=30\geq 5$, el estadístico del contraste vale&lt;/p&gt;
&lt;p&gt;$$
Z = \frac{\hat p-p_0}{\sqrt{p_0(1-p_0)/n}} = \frac{0.625-0.5}{\sqrt{0.5(1-0.5)/80}} = 2.2361.
$$&lt;/p&gt;
&lt;p&gt;y el $p$-valor del contraste es $P(Z\geq 2.2361)=0.0127$, por lo que se rechaza la hipótesis nula para $\alpha=0.05$ y se concluye que el porcentaje de aprobados es mayor de la mitad.&lt;/p&gt;
&lt;h2 id=&#34;pruebas-de-homogeneidad&#34;&gt;Pruebas de homogeneidad&lt;/h2&gt;
&lt;h3 id=&#34;contraste-de-comparación-de-medias-de-dos-poblaciones-normales-con-varianzas-conocidas&#34;&gt;Contraste de comparación de medias de dos poblaciones normales con varianzas conocidas&lt;/h3&gt;
&lt;p&gt;Sean $X_1$ y $X_2$ dos variables aleatorias que cumplen las siguientes condiciones:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Su distribución es normal $X_1\sim N(\mu_1,\sigma_1)$
$X_2\sim N(\mu_2,\sigma_2)$.&lt;/li&gt;
&lt;li&gt;Sus medias $\mu_1$ y $\mu_2$ son desconocidas, pero sus varianzas $\sigma^2_1$ y $\sigma^2_2$ son conocidas.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Contraste&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
H_0 &amp;amp;: \mu_1=\mu_2\newline
H_1 &amp;amp;: \mu_1\neq \mu_2
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Estadístico del contraste&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;$$
\left.
\begin{array}{l}
\bar{X}_1\sim N\left(\mu_1,\frac{\sigma_1}{\sqrt{n_1}} \right)\newline
\bar{X}_2\sim N\left(\mu_2,\frac{\sigma_2}{\sqrt{n_2}} \right)
\end{array}
\right\}
\Rightarrow
$$
$$
\Rightarrow
\bar{X}_1-\bar{X}_2 \sim N\left(\mu_1-\mu_2,\sqrt{\frac{\sigma^2_1}{n_1}+\frac{\sigma^2_2}{n_2}}\right)
\Rightarrow Z= \frac{\bar{X}_1-\bar{X}_2}{\sqrt{\frac{\sigma^2_1}{n_1}+\frac{\sigma^2_2}{n_2}}}\sim
N(0,1).
$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Región de aceptación&lt;/strong&gt;: $-z_{\alpha/2}&amp;lt; Z &amp;lt; z_{\alpha/2}$.&lt;br&gt;
&lt;strong&gt;Región de rechazo&lt;/strong&gt;: $Z\leq -z_{\alpha/2}$ y $Z\geq z_{\alpha/2}$.&lt;/p&gt;
&lt;h3 id=&#34;contraste-de-comparación-de-medias-de-dos-poblaciones-normales-con-varianzas-desconocidas-e-iguales&#34;&gt;Contraste de comparación de medias de dos poblaciones normales con varianzas desconocidas e iguales&lt;/h3&gt;
&lt;p&gt;Sean $X_1$ y $X_2$ dos variables aleatorias que cumplen las siguientes condiciones:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Su distribución es normal $X_1\sim N(\mu_1,\sigma_1)$ y $X_2\sim N(\mu_2,\sigma_2)$.&lt;/li&gt;
&lt;li&gt;Sus medias $\mu_1$ y $\mu_2$ son desconocidas y sus varianzas también, pero son iguales $\sigma^2_1=\sigma^2_2=\sigma^2$.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Contraste&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
H_0 &amp;amp;: \mu_1=\mu_2\newline
H_1 &amp;amp;: \mu_1\neq \mu_2
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Estadístico del contraste&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;$$
\left.
\begin{array}{l}
\bar{X}_1-\bar{X}_2\sim N\left(\mu_1-\mu_2,\sigma\sqrt{\frac{n_1+n_2}{n_1n_2}} \right)\newline
\displaystyle \frac{n_1S_1^2+n_2S_2^2}{\sigma^2} \sim \chi^2(n_1+n_2-2)
\end{array}
\right\}
\Rightarrow
T=\frac{\bar{X}_1-\bar{X}_2}{\hat{S}_p\sqrt{\frac{n_1+n_2}{n_1n_2}}} \sim T(n_1+n_2-2).
$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Región de aceptación&lt;/strong&gt;: $-t_{\alpha/2}^{n_1+n_2-2} &amp;lt; T &amp;lt; t_{\alpha/2}^{n_1+n_2-2}$.&lt;br&gt;
&lt;strong&gt;Región de rechazo&lt;/strong&gt;: $T\leq -t_{\alpha/2}^{n_1+n_2-2}$ y $T\geq t_{\alpha/2}^{n_1+n_2-2}$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Ejemplo&lt;/strong&gt;. Se quiere comparar el rendimiento académico de dos grupos de alumnos, uno con 10 alumnos y otro con 12, que han seguido metodologías diferentes. Para ello se les realiza un examen y se obtienen las siguientes puntuaciones:&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
X_1 &amp;amp;: 4 - 6 - 8 - 7 - 7 - 6 - 5 - 2 - 5 - 3 \newline
X_2 &amp;amp;: 8 - 9 - 5 - 3 - 8 - 7 - 8 - 6 - 8 - 7 - 5 - 7
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;El contraste que se plantea es&lt;/p&gt;
&lt;p&gt;$$
H_0: \mu_1=\mu_2\qquad H_1: \mu_1\neq \mu_2
$$&lt;/p&gt;
&lt;p&gt;Para realizar el contraste, se tiene&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\bar{X}_1 = \frac{4+\cdots +3}{10}=5.3$ puntos y $\bar{X}_2=\frac{8+\cdots +7}{12}=6.75$ puntos.&lt;/li&gt;
&lt;li&gt;$S_1^2= \frac{(4^2+\cdots + 3^2}{10}-5.3^2=3.21$ puntos$^2$ y $S_2^2= \frac{8^2+\cdots +3^2}{12}-6.75^2=2.69$ puntos$^2$.&lt;/li&gt;
&lt;li&gt;$\hat{S}_p^2 = \frac{10\cdot 3.21+12\cdot 2.6875}{10+12-2}= 3.2175$ puntos$^2$, y $\hat S_p=1.7937$.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Si se suponen varianzas iguales, el estadístico del contraste vale&lt;/p&gt;
&lt;p&gt;$$
T=\frac{\bar{X}_1-\bar{X}_2}{\hat{S}_p\sqrt{\frac{n_1+n_2}{n_1n_2}}} = \frac{5.3-6.75}{1.7937\sqrt{\frac{10+12}{10\cdot 12}}} = -1.8879,
$$&lt;/p&gt;
&lt;p&gt;y el $p$-valor del contraste es $2P(T(20)\leq -1.8879) = 0.0736$, de modo que no se puede rechazar la hipótesis nula y se concluye que no hay diferencias significativas entre las notas medias de los grupos.&lt;/p&gt;
&lt;h3 id=&#34;contraste-de-comparación-de-medias-de-dos-poblaciones-normales-con-varianzas-desconocidas&#34;&gt;Contraste de comparación de medias de dos poblaciones normales con varianzas desconocidas&lt;/h3&gt;
&lt;p&gt;Sean $X_1$ y $X_2$ dos variables aleatorias que cumplen las siguientes condiciones:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Su distribución es normal $X_1\sim N(\mu_1,\sigma_1)$ y  $X_2\sim N(\mu_2,\sigma_2)$.&lt;/li&gt;
&lt;li&gt;Sus medias $\mu_1$, $\mu_2$ y varianzas $\sigma_1^2$, $\sigma_2^2$, son desconocidas, pero $\sigma^2_1\not = \sigma^2_2$.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Contraste&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
H_0 &amp;amp;: \mu_1=\mu_2\newline
H_1 &amp;amp;: \mu_1\neq \mu_2
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Estadístico del contraste&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;$$
T=\frac{(\bar{X}_1-\bar{X}_2)-(\mu_1-\mu_2)}{\sqrt{\frac{\hat{S}^2_1}{n_1}+\frac{\hat{S}^2_2}{n_2}}} \sim T(g),
$$&lt;/p&gt;
&lt;p&gt;con $g=n_1+n_2-2-\Delta$ y&lt;/p&gt;
&lt;p&gt;$$\Delta = \frac{(\frac{n_2-1}{n_1}\hat{S}_1^2-\frac{n_1-1}{n_2}\hat{S}_2^2)^2}{\frac{n_2-1}{n_1^2}\hat{S}_1^4+\frac{n_1-1}{n_2^2}\hat{S}_2^4}.
$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Región de aceptación&lt;/strong&gt;: $-t_{\alpha/2}^{g} &amp;lt; T &amp;lt; t_{\alpha/2}^{g}$.&lt;br&gt;
&lt;strong&gt;Región de rechazo&lt;/strong&gt;: $T\leq -t_{\alpha/2}^{g}$ y $T\geq t_{\alpha/2}^{g}$.&lt;/p&gt;
&lt;h3 id=&#34;contraste-de-comparación-de-varianzas-de-dos-poblaciones-normales&#34;&gt;Contraste de comparación de varianzas de dos poblaciones normales&lt;/h3&gt;
&lt;p&gt;Sean $X_1$ y $X_2$ dos variables aleatorias que cumplen las siguientes condiciones:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Su distribución es normal $X_1\sim N(\mu_1,\sigma_1)$ y $X_2\sim N(\mu_2,\sigma_2)$.&lt;/li&gt;
&lt;li&gt;Sus medias $\mu_1$, $\mu_2$ y varianzas $\sigma_1^2$, $\sigma_2^2$ son desconocidas.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Contraste&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
H_0 &amp;amp;: \sigma_1=\sigma_2\newline
H_1 &amp;amp;: \sigma_1\neq \sigma_2
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Estadístico del contraste&lt;/strong&gt;:
$$\left.
\begin{array}{l}
\displaystyle \frac{(n_1-1)\hat{S}_1^2}{\sigma_1^2}\sim \chi^2(n_1-1) \newline
\displaystyle \frac{(n_2-1)\hat{S}_2^2}{\sigma_2^2}\sim \chi^2(n_2-1)
\end{array}
\right\}
\Rightarrow
F= \frac{\frac{\frac{(n_1-1)\hat{S}_1^2}{\sigma_1^2}}{n_1-1}}{\frac{\frac{(n_2-1)\hat{S}_2^2}{\sigma_2^2}}{n_2-1}} =
\frac{\sigma_2^2}{\sigma_1^2}\frac{\hat{S}_1^2}{\hat{S}_2^2}\sim F(n_1-1,n_2-1).
$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Región de aceptación&lt;/strong&gt;: $F_{\alpha/2}^{n_1-1,n_2-1} &amp;lt; F &amp;lt; F_{1-\alpha/2}^{n_1-1,n_2-1}$.&lt;br&gt;
&lt;strong&gt;Región de rechazo&lt;/strong&gt;: $F\leq F_{\alpha/2}^{n_1-1,n_2-1}$ y $F\geq F_{1-\alpha/2}^{n_1-1,n_2-1}$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Ejemplo&lt;/strong&gt;. Siguiendo con el ejemplo de las puntuaciones en dos grupos:&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
X_1 &amp;amp;: 4 - 6 - 8 - 7 - 7 - 6 - 5 - 2 - 5 - 3 \newline
X_2 &amp;amp;: 8 - 9 - 5 - 3 - 8 - 7 - 8 - 6 - 8 - 7 - 5 - 7
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;Si se desea comparar las varianzas, el contraste que se plantea es&lt;/p&gt;
&lt;p&gt;$$
H_0: \sigma_1=\sigma_2\qquad H_1: \sigma_1\neq \sigma_2
$$&lt;/p&gt;
&lt;p&gt;Para realizar el contraste, se tiene&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\bar{X}_1 = \frac{4+\cdots +3}{10}=5.3$ puntos y $\bar{X}_2=\frac{8+\cdots +7}{12}=6.75$ puntos.&lt;/li&gt;
&lt;li&gt;$\hat{S}_1^2= \frac{(4-5.3)^2+\cdots + (3-5.3)^2}{9}=3.5667$ y $\hat{S}_2^2= \frac{(8-6.75)^2+\cdots + (3-6.75)^2}{11}=2.9318$   puntos$^2$.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;El estadístico del contraste vale&lt;/p&gt;
&lt;p&gt;$$
F = \frac{\hat{S}_1^2}{\hat{S}_2^2} = \frac{3.5667}{2.9318}=1.2165,
$$&lt;/p&gt;
&lt;p&gt;y el $p$-valor del contraste es $2P(F(9,11)\leq 1.2165)=0.7468$, por lo que se mantiene la hipótesis de igualdad de varianzas.&lt;/p&gt;
&lt;h3 id=&#34;contraste-de-comparación-de-proporciones-de-dos-poblaciones&#34;&gt;Contraste de comparación de proporciones de dos poblaciones&lt;/h3&gt;
&lt;p&gt;Sean $p_1$ y $p_2$ las respectivas proporciones de individuos que presentan una determinada característica en dos poblaciones.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Contraste&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;$$
H_0: p_1=p_2\qquad H_1: p_1\neq p_2
$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Estadístico del contraste&lt;/strong&gt;: Las variables que miden el número de individuos con la característica en dos muestras aleatorias de tamaños $n_1$ y $n_2$ respectivamente, siguen distribuciones binomiales
$X_1\sim B(n_1,p_1)$ y $X_2\sim B(n_2,p_2)$. Si las muestras son grandes ($n_ip_i\geq 5$ y $n_i(1-p_i)\geq 5$), de acuerdo al teorema central del límite, $X_1\sim N(np_1,\sqrt{np_1(1-p_1)})$ y $X_2\sim N(np_2,\sqrt{np_2(1-p_2)})$, y se cumple&lt;/p&gt;
&lt;p&gt;$$
\left.
\begin{array}{l}
\hat{p}_1=\frac{X_1}{n_1} \sim N\left(p_1,\sqrt{\frac{p_1(1-p_1)}{n_1}}\right)\newline
\hat{p}_2=\frac{X_2}{n_2} \sim N\left(p_2,\sqrt{\frac{p_2(1-p_2)}{n_2}}\right)
\end{array}
\right\}
\Rightarrow Z = \frac{\hat{p}_1-\hat{p_2}}{\sqrt{\frac{p_1(1-p_1)}{n_1}+\frac{p_2(1-p_2)}{n_2}}}\sim N(0,1)
$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Región de aceptación&lt;/strong&gt;: $z_{\alpha/2}&amp;lt; Z &amp;lt; z_{1-\alpha/2}$.&lt;br&gt;
&lt;strong&gt;Región de rechazo&lt;/strong&gt;: $z\leq z_{\alpha/2}$ y $z\geq z_{1-\alpha/2}$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Ejemplo&lt;/strong&gt;. Se quiere comparar los porcentajes de aprobados en dos grupos que han seguido metodologías distintas. En el primer grupo han aprobado 24 alumnos de un total de 40, mientras que en el segundo han aprobado 48 de 60.&lt;/p&gt;
&lt;p&gt;El contraste que se plantea es&lt;/p&gt;
&lt;p&gt;$$
H_0: p_1=p_2\qquad H_1: p_1\neq p_2
$$&lt;/p&gt;
&lt;p&gt;Para realizar el contraste, se tiene $\hat{p}_1=24/40= 0.6$ y $\hat{p}_2=48/60=0.8$, de manera que se cumplen las condiciones $n_1\hat{p}_1=40\cdot 0.6=24\geq 5$,
$n_1(1-\hat{p}_1)=40(1-0.6)=26\geq 5$,
$n_2\hat{p}_2=60\cdot 0.8=48\geq 5$ y
$n_2(1-\hat{p}_2)=60(1-0.8)=12\geq 5$, y el estadístico del contraste vale&lt;/p&gt;
&lt;p&gt;$$
Z = \frac{\hat{p}_1-\hat{p_2}}{\sqrt{\frac{p_1(1-p_1)}{n_1}+\frac{p_2(1-p_2)}{n_2}}} = \frac{0.6-0.8}{\sqrt{\frac{0.6(1-0.6)}{40}+\frac{0.8(1-0.8)}{60}}} = -2.1483,
$$&lt;/p&gt;
&lt;p&gt;y el $p$-valor del contraste es $2P(Z\leq -2.1483)= 0.0317$, de manera que se rechaza la hipótesis nula para $\alpha=0.05$ y se concluye que hay diferencias.&lt;/p&gt;
&lt;h2 id=&#34;realización-de-contrastes-mediante-intervalos-de-confianza&#34;&gt;Realización de contrastes mediante intervalos de confianza&lt;/h2&gt;
&lt;p&gt;Una interesante alternativa a la realización de un contraste&lt;/p&gt;
&lt;p&gt;$$
H_0: \theta=\theta_0\qquad H_1: \theta\neq \theta_0
$$&lt;/p&gt;
&lt;p&gt;con un riesgo $\alpha$, es calcular el intervalo de confianza para $\theta$ con un nivel de confianza
$1-\alpha$, ya que este intervalo se puede interpretar como el conjunto aceptable de hipótesis para $\theta$, de manera que si $\theta_0$ está fuera del intervalo, la hipótesis nula es poco creíble y puede rechazarse, mientras que si está dentro la hipótesis es creíble y se
acepta.&lt;/p&gt;
&lt;p&gt;Cuando el contraste sea unilateral de menor, el contraste se realizaría comparando $\theta_0$ con el límite superior del intervalo de confianza para $\theta$ con un nivel de confianza $1-2\alpha$, mientras que si el
contraste es unilateral de mayor, se comparará con el límite inferior del intervalo.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Contraste&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Intervalo de confianza&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Decisión&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Bilateral&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$[l_i,l_s]$ con nivel de confianza $1-\alpha$&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Rechazar $H_0$ si $\theta_0\not \in [l_i,l_s]$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Unilateral menor&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$[-\infty,l_s]$ con nivel de confianza $1-2\alpha$&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Rechazar $H_0$ si $\theta_0\geq l_s$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Unilateral mayor&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$[l_i,\infty]$ con nivel de confianza $1-2\alpha$&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Rechazar $H_0$ si $\theta_0\leq l_i$&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;Ejemplo&lt;/strong&gt;. Volviendo al contraste para comparar el rendimiento académico de dos grupos de alumnos que han obtenido las siguientes puntuaciones:&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
X_1 &amp;amp;: 4 - 6 - 8 - 7 - 7 - 6 - 5 - 2 - 5 - 3 \newline
X_2 &amp;amp;: 8 - 9 - 5 - 3 - 8 - 7 - 8 - 6 - 8 - 7 - 5 - 7
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;El contraste que se planteaba era&lt;/p&gt;
&lt;p&gt;$$
H_0: \mu_1=\mu_2\qquad H_1: \mu_1\neq \mu_2
$$&lt;/p&gt;
&lt;p&gt;Como se trata de un contraste bilateral, el intervalo de confianza para la diferencia de medias $\mu_1-\mu_2$ con nivel de confianza $1-\alpha=0.95$, suponiendo
varianzas iguales, vale $[-3.0521, 0.1521]$ puntos. Y como según la hipótesis nula $\mu_1-\mu_2=0$, y el 0 cae dentro del intervalo, se acepta la hipótesis nula.&lt;/p&gt;
&lt;p&gt;La ventaja del intervalo es que, además de permitirnos realizar el contraste, nos da una idea de la magnitud de la diferencia entre las medias de los grupos.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>

<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Regression | Aprende con Alf</title><link>/en/tag/regression/</link><atom:link href="/en/tag/regression/index.xml" rel="self" type="application/rss+xml"/><description>Regression</description><generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Wed, 20 Apr 2016 00:00:00 +0000</lastBuildDate><image><url>/images/logo_hude38443eeb2faa5fa84365aba7d86a77_3514_300x300_fit_lanczos_3.png</url><title>Regression</title><link>/en/tag/regression/</link></image><item><title>Regression</title><link>/en/teaching/statistics/manual/regression/</link><pubDate>Wed, 20 Apr 2016 00:00:00 +0000</pubDate><guid>/en/teaching/statistics/manual/regression/</guid><description>&lt;p>In the last chapter we saw how to describe the distribution of a single variable in a sample. However, in most cases, studies require to describe several variables that are often related. For instance, a nutritional study should consider all the variables that could be related to the weight, as height, age, gender, smoking, diet, physic exercise, etc.&lt;/p>
&lt;p>To understand a phenomenon that involve several variables is not enough to study every variable by its own. We have to study all the variables together to describe how they interact and the type of relation among them.&lt;/p>
&lt;p>Usually in a &lt;em>dependency study&lt;/em> there is a &lt;strong>dependent variable&lt;/strong> $Y$ that it is supposed to be influenced by a set of variables $X_1,\ldots,X_n$ known as &lt;strong>independent variables&lt;/strong>. The simpler case is a &lt;em>simple dependency study&lt;/em> when there is only one independent variable, that is the case covered in this chapter.&lt;/p>
&lt;h2 id="joint-distribution">Joint distribution&lt;/h2>
&lt;h3 id="joint-frequencies">Joint frequencies&lt;/h3>
&lt;p>To study the relation between two variables $X$ and $Y$, we have to study the joint distribution of the &lt;strong>two-dimensional variable&lt;/strong> $(X,Y)$, whose values are pairs $(x_i,y_j)$ where the first element is a value of $X$ and the second a value of $Y$.&lt;/p>
&lt;div class="alert alert-def">
&lt;div>
&lt;p>&lt;strong>Definition - Joint sample frequencies&lt;/strong>. Given a sample of $n$ values and a two-dimensional variable $(X,Y)$, for every value of the variable $(x_i,y_j)$ is defined:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Absolute frequency&lt;/strong> $n_{ij}$: Is the number of times that the pair $(x_i,y_j)$ appears in the sample.&lt;/li>
&lt;li>&lt;strong>Relative frequency&lt;/strong> $f_{ij}$: Is the proportion of times that the pair $(x_i,y_j)$ appears in the sample.&lt;/li>
&lt;/ul>
&lt;p>$$f_{ij}=\frac{n_{ij}}{n}.$$&lt;/p>
&lt;/div>
&lt;/div>
&lt;div class="alert alert-warning">
&lt;div>
For two-dimensional variables it make no sense cumulative frequencies.
&lt;/div>
&lt;/div>
&lt;h3 id="joint-frequency-distribution">Joint frequency distribution&lt;/h3>
&lt;p>The values of the two-dimensional variable with their frequencies is known as &lt;strong>joint frequency distribution&lt;/strong>, and is represented in a &lt;strong>joint frequency table&lt;/strong>.&lt;/p>
&lt;p>$$\begin{array}{|c|ccccc|}
\hline
X\backslash Y &amp;amp; y_1 &amp;amp; \cdots &amp;amp; y_j &amp;amp; \cdots &amp;amp; y_q \newline
\hline
x_1 &amp;amp; n_{11} &amp;amp; \cdots &amp;amp; n_{1j} &amp;amp; \cdots &amp;amp; n_{1q} \newline
\vdots &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \vdots \newline
x_i &amp;amp; n_{i1} &amp;amp; \cdots &amp;amp; n_{ij} &amp;amp; \cdots &amp;amp; n_{iq} \newline
\vdots &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \vdots \newline
x_p &amp;amp; n_{p1} &amp;amp; \cdots &amp;amp; n_{pj} &amp;amp; \cdots &amp;amp; n_{pq} \newline
\hline
\end{array}$$&lt;/p>
&lt;p>&lt;strong>Example (grouped data)&lt;/strong>. The height (in cm) and weight (in kg) of a sample of 30 students is:&lt;/p>
&lt;div style="text-align:center">
(179,85), (173,65), (181,71), (170,65), (158,51), (174,66),&lt;br/>
(172,62), (166,60), (194,90), (185,75), (162,55), (187,78),&lt;br/>
(198,109), (177,61), (178,70), (165,58), (154,50), (183,93),&lt;br/>
(166,51), (171,65), (175,70), (182,60), (167,59), (169,62),&lt;br/>
(172,70), (186,71), (172,54), (176,68),(168,67), (187,80).
&lt;/div>
&lt;p>The joint frequency table is&lt;/p>
&lt;p>$$\begin{array}{|c||c|c|c|c|c|c|}
\hline
X/Y &amp;amp; [50,60) &amp;amp; [60,70) &amp;amp; [70,80) &amp;amp; [80,90) &amp;amp; [90,100) &amp;amp; [100,110) \ \newline
\hline\hline
(150,160] &amp;amp; 2 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 \ \newline
\hline
(160,170] &amp;amp; 4 &amp;amp; 4 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 \ \newline
\hline
(170,180] &amp;amp; 1 &amp;amp; 6 &amp;amp; 3 &amp;amp; 1 &amp;amp; 0 &amp;amp; 0 \ \newline
\hline
(180,190] &amp;amp; 0 &amp;amp; 1 &amp;amp; 4 &amp;amp; 1 &amp;amp; 1 &amp;amp; 0 \ \newline
\hline
(190,200] &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 1 &amp;amp; 1 \ \newline
\hline
\end{array}$$&lt;/p>
&lt;h3 id="scatter-plot">Scatter plot&lt;/h3>
&lt;p>The joint frequency distribution can be represented graphically with a &lt;strong>scatter plot&lt;/strong>, where data is displayed as a collections of points on a $XY$ coordinate system.&lt;/p>
&lt;p>Usually the independent variable is represented in the $X$ axis and the dependent variable in the $Y$ axis. For every data pair $(x_i,y_j)$ in the sample a dot is drawn on the plane with those coordinates.&lt;/p>
&lt;img src="../img/regression/scatterplot.svg" alt="Scatter plot" width="300">
&lt;p>The result is a set of points that usually is known as a &lt;em>point cloud&lt;/em>.&lt;/p>
&lt;p>&lt;strong>Example&lt;/strong>. The scatter plot below represent the distribution of heights and weights of the previous sample.&lt;/p>
&lt;img src="../img/regression/height_weight_scatterplot.svg" alt="Scatter plot of heights and weights" width="600">
&lt;div class="alert alert-int">
&lt;div>
The shape of the point cloud in a scatter plot gives information about the type of relation between the variables.&lt;/p>
&lt;img src="../img/regression/scatterplot_different_relations.svg" alt="Scatter plot of different types of relations" width="700">
&lt;/div>
&lt;/div>
&lt;h3 id="marginal-frequency-distributions">Marginal frequency distributions&lt;/h3>
&lt;p>The frequency distributions of each variable of the two-dimensional variable are known as &lt;strong>marginal frequency distributions&lt;/strong>.&lt;/p>
&lt;p>We can get the marginal frequency distributions from the joint frequency table by adding frequencies by rows and columns.&lt;/p>
&lt;p>$$\begin{array}{|c|ccccc|c|}
\hline
X\backslash Y &amp;amp; y_1 &amp;amp; \cdots &amp;amp; y_j &amp;amp; \cdots &amp;amp; y_q &amp;amp; \color{red}{n_x} \newline
\hline
x_1 &amp;amp; n_{11} &amp;amp; \cdots &amp;amp; n_{1j} &amp;amp; \cdots &amp;amp; n_{1q} &amp;amp; \color{red}{n_{x_1}} \newline
\vdots &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \downarrow + &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \color{red}{\vdots} \newline
x_i &amp;amp; n_{i1} &amp;amp; \stackrel{+}{\rightarrow} &amp;amp; n_{ij} &amp;amp; \stackrel{+}{\rightarrow} &amp;amp; n_{iq} &amp;amp; \color{red}{n_{x_i}} \newline
\vdots &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \downarrow + &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \color{red}{\vdots} \newline
x_p &amp;amp; n_{p1} &amp;amp; \cdots &amp;amp; n_{pj} &amp;amp; \cdots &amp;amp; n_{pq} &amp;amp; \color{red}{n_{x_p}} \newline
\hline
\color{red}{n_y} &amp;amp; \color{red}{n_{y_1}} &amp;amp; \color{red}{\cdots} &amp;amp; \color{red}{n_{y_j}} &amp;amp; \color{red}{\cdots} &amp;amp; \color{red}{n_{y_q}} &amp;amp; n \newline
\hline
\end{array}$$&lt;/p>
&lt;p>&lt;strong>Example&lt;/strong>. The marginal frequency distributions for the previous sample of heights and weights are&lt;/p>
&lt;p>$$
\begin{array}{|c||c|c|c|c|c|c|c|}
\hline
X/Y &amp;amp; [50,60) &amp;amp; [60,70) &amp;amp; [70,80) &amp;amp; [80,90) &amp;amp; [90,100) &amp;amp; [100,110) &amp;amp; \color{red}{n_x}\ \newline
\hline\hline
(150,160] &amp;amp; 2 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; \color{red}{2}\ \newline
\hline
(160,170] &amp;amp; 4 &amp;amp; 4 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; \color{red}{8}\ \newline
\hline
(170,180] &amp;amp; 1 &amp;amp; 6 &amp;amp; 3 &amp;amp; 1 &amp;amp; 0 &amp;amp; 0 &amp;amp; \color{red}{11} \ \newline
\hline
(180,190] &amp;amp; 0 &amp;amp; 1 &amp;amp; 4 &amp;amp; 1 &amp;amp; 1 &amp;amp; 0 &amp;amp; \color{red}{7} \ \newline
\hline
(190,200] &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 1 &amp;amp; 1 &amp;amp; \color{red}{2}\ \newline
\hline
\color{red}{n_y} &amp;amp; \color{red}{7} &amp;amp; \color{red}{11} &amp;amp; \color{red}{7} &amp;amp; \color{red}{2} &amp;amp; \color{red}{2} &amp;amp; \color{red}{1} &amp;amp; 30\ \newline
\hline
\end{array}
$$&lt;/p>
&lt;p>and the corresponding statistics are&lt;/p>
&lt;p>$$
\begin{array}{lllll}
\bar x = 174.67 \mbox{ cm} &amp;amp; \quad &amp;amp; s^2_x = 102.06 \mbox{ cm}^2 &amp;amp; \quad &amp;amp; s_x = 10.1 \mbox{ cm} \newline
\bar y = 69.67 \mbox{ Kg} &amp;amp; &amp;amp; s^2_y = 164.42 \mbox{ Kg}^2 &amp;amp; &amp;amp; s_y = 12.82 \mbox{ Kg}
\end{array}
$$&lt;/p>
&lt;h2 id="covariance">Covariance&lt;/h2>
&lt;p>To study the relation between two variables, we have to analyze the joint variation of them.&lt;/p>
&lt;img src="../img/regression/deviations_to_means.svg" alt="Devitations to means in an scatterplot" width="600">
&lt;p>Dividing the point cloud of the scatter plot in 4 quadrants centered in the mean point $(\bar x, \bar y)$, the sign of deviations from the mean is:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align:center">&lt;strong>Quadrant&lt;/strong>&lt;/th>
&lt;th style="text-align:center">$(x_i-\bar x)$&lt;/th>
&lt;th style="text-align:center">$(y_j-\bar y)$&lt;/th>
&lt;th style="text-align:center">$(x_i-\bar x)(y_j-\bar y)$&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:center">1&lt;/td>
&lt;td style="text-align:center">$+$&lt;/td>
&lt;td style="text-align:center">$+$&lt;/td>
&lt;td style="text-align:center">$+$&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">2&lt;/td>
&lt;td style="text-align:center">$-$&lt;/td>
&lt;td style="text-align:center">$+$&lt;/td>
&lt;td style="text-align:center">$-$&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">3&lt;/td>
&lt;td style="text-align:center">$-$&lt;/td>
&lt;td style="text-align:center">$-$&lt;/td>
&lt;td style="text-align:center">$+$&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">4&lt;/td>
&lt;td style="text-align:center">$+$&lt;/td>
&lt;td style="text-align:center">$-$&lt;/td>
&lt;td style="text-align:center">$-$&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;img src="../img/regression/scatterplot_quadrants.svg" alt="Quadrants of a scatter plot" width="400">
&lt;p>If there is an &lt;em>increasing linear&lt;/em> relationship between the variables, most of the points will fall in quadrants 1 and 3, and the sum of the products of deviations from the mean will be positive.&lt;/p>
&lt;p>$$\sum(x_i-\bar x)(y_j-\bar y) &amp;gt; 0$$&lt;/p>
&lt;img src="../img/regression/increasing_linear_scatterplot.svg" alt="Increasing linear scatter plot" width="500">
&lt;p>If there is an &lt;em>decreasing linear&lt;/em> relationship between the variables, most of the points will fall in quadrants 2 and 4, and the sum of the products of deviations from the mean will be negative.&lt;/p>
&lt;p>$$\sum(x_i-\bar x)(y_j-\bar y) &amp;lt; 0$$&lt;/p>
&lt;img src="../img/regression/decreasing_linear_scatterplot.svg" alt="Decreasing linear scatter plot" width="500">
&lt;p>Using the products of deviations from the means we get the following statistic.&lt;/p>
&lt;div class="alert alert-def">
&lt;div>
&lt;strong>Definition - Sample covariance&lt;/strong>. The &lt;em>sample covariance&lt;/em> of a two-dimensional variable $(X,Y)$ is the average of the products of deviations from the respective means.$$s_{xy}=\frac{\sum (x_i-\bar x)(y_j-\bar y)n_{ij}}{n}$$
&lt;/div>
&lt;/div>
&lt;p>It can also be calculated using the formula&lt;/p>
&lt;p>$$s_{xy}=\frac{\sum x_iy_jn_{ij}}{n}-\bar x\bar y.$$&lt;/p>
&lt;div class="alert alert-int">
&lt;div>
&lt;p>The covariance measures the linear relation between two variables:&lt;/p>
&lt;ul>
&lt;li>If $s_{xy}&amp;gt;0$ there exists an increasing linear relation.&lt;/li>
&lt;li>If $s_{xy}&amp;lt;0$ there exists a decreasing linear relation.&lt;/li>
&lt;li>If $s_{xy}=0$ there is no linear relation.&lt;/li>
&lt;/ul>
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Example&lt;/strong>. Using the joint frequency table of the sample of heights and weights&lt;/p>
&lt;p>$$
\begin{array}{|c||c|c|c|c|c|c|c|}
\hline
X/Y &amp;amp; [50,60) &amp;amp; [60,70) &amp;amp; [70,80) &amp;amp; [80,90) &amp;amp; [90,100) &amp;amp; [100,110) &amp;amp; n_x\ \newline
\hline\hline
(150,160] &amp;amp; 2 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 2\ \newline
\hline
(160,170] &amp;amp; 4 &amp;amp; 4 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 8\ \newline
\hline
(170,180] &amp;amp; 1 &amp;amp; 6 &amp;amp; 3 &amp;amp; 1 &amp;amp; 0 &amp;amp; 0 &amp;amp; 11 \ \newline
\hline
(180,190] &amp;amp; 0 &amp;amp; 1 &amp;amp; 4 &amp;amp; 1 &amp;amp; 1 &amp;amp; 0 &amp;amp; 7 \ \newline
\hline
(190,200] &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 1 &amp;amp; 1 &amp;amp; 2\ \newline
\hline
n_y &amp;amp; 7 &amp;amp; 11 &amp;amp; 7 &amp;amp; 2 &amp;amp; 2 &amp;amp; 1 &amp;amp; 30\ \newline
\hline
\end{array}
$$&lt;/p>
&lt;p>$$\bar x = 174.67 \mbox{ cm} \qquad \bar y = 69.67 \mbox{ Kg}$$&lt;/p>
&lt;p>we get that the covariance is equal to&lt;/p>
&lt;p>$$
\begin{aligned}
s_{xy} &amp;amp;=\frac{\sum x_iy_jn_{ij}}{n}-\bar x\bar y = \frac{155\cdot 55\cdot 2 + 165\cdot 55\cdot 4 + \cdots + 195\cdot 105\cdot 1}{30}-174.67\cdot 69.67 = \newline
&amp;amp; = \frac{368200}{30}-12169.26 = 104.07 \mbox{ cm$\cdot$ Kg}.
\end{aligned}
$$&lt;/p>
&lt;p>This means that there is a increasing linear relation between the weight and the height.&lt;/p>
&lt;h2 id="regression">Regression&lt;/h2>
&lt;p>In most cases the goal of a dependency study is not only to detect a relation between two variables, but also to express that relation with a mathematical function, $$y=f(x)$$ in order to predict the dependent variable for every value of the independent one.
The part of Statistics in charge of constructing such a function is called &lt;strong>regression&lt;/strong>, and the function is known as &lt;strong>regression function&lt;/strong> or &lt;strong>regression model&lt;/strong>.&lt;/p>
&lt;h3 id="simple-regression-models">Simple regression models&lt;/h3>
&lt;p>There are a lot of types of regression models. The most common models are shown in the table below.&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align:left">&lt;strong>Model&lt;/strong>&lt;/th>
&lt;th style="text-align:center">&lt;strong>Equation&lt;/strong>&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:left">Linear&lt;/td>
&lt;td style="text-align:center">$y=a+bx$&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">Quadratic&lt;/td>
&lt;td style="text-align:center">$y=a+bx+cx^2$&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">Cubic&lt;/td>
&lt;td style="text-align:center">$y=a+bx+cx^2+dx^3$&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">Potential&lt;/td>
&lt;td style="text-align:center">$y=a\cdot x^b$&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">Exponential&lt;/td>
&lt;td style="text-align:center">$y=e^{a+bx}$&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">Logarithmic&lt;/td>
&lt;td style="text-align:center">$y=a+b\log x$&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">Inverse&lt;/td>
&lt;td style="text-align:center">$y=a+\frac{b}{x}$&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">Sigmoidal&lt;/td>
&lt;td style="text-align:center">$y=e^{a+\frac{b}{x}}$&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>The model choice depends on the shape of the points cloud in the scatterplot.&lt;/p>
&lt;h3 id="residuals-or-predictive-errors">Residuals or predictive errors&lt;/h3>
&lt;p>Once chosen the type of regression model, we have to determine which function of that family explains better the relation between the dependent and the independent variables, that is, the function that predicts better the dependent variable.&lt;/p>
&lt;p>That function is the function that minimizes the distances from the observed values for $Y$ in the sample to the predicted values of the regression function. These distances are known as &lt;em>residuals&lt;/em> or &lt;em>predictive errors&lt;/em>.&lt;/p>
&lt;div class="alert alert-def">
&lt;div>
&lt;strong>Definition - Residuals or predictive errors&lt;/strong>. Given a regression model $y=f(x)$ for a two-dimensional variable $(X,Y)$, the &lt;em>residual&lt;/em> or &lt;em>predictive error&lt;/em> for every pair $(x_i,y_j)$ of the sample is the difference between the observed value of the dependent variable $y_j$ and the predicted value of the regression function for $x_i$,$$e_{ij} = y_j-f(x_i).$$
&lt;/div>
&lt;/div>
&lt;img src="../img/regression/y_residuals.svg" alt="Regression rediduals on Y" width="600">
&lt;h3 id="least-squares-fitting">Least squares fitting&lt;/h3>
&lt;p>A way to get the regression function is the &lt;em>least squares method&lt;/em>, that determines the function that minimizes the squared residuals.&lt;/p>
&lt;p>$$\sum e_{ij}^2.$$&lt;/p>
&lt;p>For a linear model $f(x) = a + bx$, the sum depends on two parameters,the intercept $a$, and the slope $b$ of the straight line,&lt;/p>
&lt;p>$$\theta(a,b) = \sum e_{ij}^2 =\sum (y_j - f(x_i))^2 =\sum (y_j-a-bx_i)^2.$$&lt;/p>
&lt;p>This reduces the problem to determine the values of $a$ and $b$ that minimize this sum.&lt;/p>
&lt;p>To solve the minimization problem, we have to set to zero the partial derivatives with respect to $a$ and $b$.&lt;/p>
&lt;p>$$
\begin{aligned}
\frac{\partial \theta(a,b)}{\partial a} &amp;amp;= \frac{\partial \sum (y_j-a-bx_i)^2 }{\partial a} =0 \newline
\frac{\partial \theta(a,b)}{\partial b} &amp;amp;= \frac{\partial \sum (y_j-a-bx_i)^2 }{\partial b} =0
\end{aligned}
$$&lt;/p>
&lt;p>And solving the equation system, we get&lt;/p>
&lt;p>$$a= \bar y - \frac{s_{xy}}{s_x^2}\bar x \qquad b=\frac{s_{xy}}{s_x^2}$$&lt;/p>
&lt;p>This values minimize the residuals on $Y$ and give us the optimal linear model.&lt;/p>
&lt;h2 id="regression-line">Regression line&lt;/h2>
&lt;div class="alert alert-def">
&lt;div>
&lt;strong>Definition - Regression line&lt;/strong>. Given a sample of a two-dimensional variable $(X,Y)$, the &lt;em>regression line&lt;/em> of $Y$ on $X$ is$$y = \bar y +\frac{s_{xy}}{s_x^2}(x-\bar x).$$
&lt;/div>
&lt;/div>
&lt;div class="alert alert-int">
&lt;div>
The regression line of $Y$ on $X$ is the straight line that minimizes the predictive errors on $Y$, therefore it is the linear regression model that gives better predictions of $Y$.
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Example&lt;/strong>. Using the previous sample of heights ($X$) and weights ($Y$) with the following statistics&lt;/p>
&lt;p>$$
\begin{array}{lllll}
\bar x = 174.67 \mbox{ cm} &amp;amp; \quad &amp;amp; s^2_x = 102.06 \mbox{ cm}^2 &amp;amp; \quad &amp;amp; s_x = 10.1 \mbox{ cm} \newline
\bar y = 69.67 \mbox{ Kg} &amp;amp; &amp;amp; s^2_y = 164.42 \mbox{ Kg}^2 &amp;amp; &amp;amp; s_y = 12.82 \mbox{ Kg} \newline
&amp;amp; &amp;amp; s_{xy} = 104.07 \mbox{ cm$\cdot$ Kg} &amp;amp; &amp;amp;
\end{array}
$$&lt;/p>
&lt;p>the regression line of weight on height is&lt;/p>
&lt;p>$$y = \bar y +\frac{s_{xy}}{s_x^2}(x-\bar x) = 69.67+\frac{104.07}{102.06}(x-174.67) = -108.49 +1.02 x$$&lt;/p>
&lt;p>And the regression line of height on weight is&lt;/p>
&lt;p>$$x = \bar x +\frac{s_{xy}}{s_y^2}(y-\bar y) = 174.67+\frac{104.07}{164.42}(y-69.67) = 130.78 + 0.63 y$$&lt;/p>
&lt;div class="alert alert-warning">
&lt;div>
Observe that the regression lines are different!
&lt;/div>
&lt;/div>
&lt;img src="../img/regression/regression_lines.svg" alt="Regression lines of heights and wieghts" width="600">
&lt;h3 id="relative-position-of-the-regression-lines">Relative position of the regression lines&lt;/h3>
&lt;p>Usually, the regression line of $Y$ on $X$ and the regression line of $X$ on $Y$ are not the same, but they always intersect in the mean point $(\bar x,\bar y)$.&lt;/p>
&lt;p>If there is a perfect linear relation between the variables, then both regression lines are the same, as that line makes both $X$-residuals and $Y$-residuals zero.&lt;/p>
&lt;img src="../img/regression/perfect_linear_regression.svg" alt="Perfect linear regression" width="500">
&lt;p>If there is no linear relation between the variables, then both regression lines are constant and equals to the respective means,&lt;/p>
&lt;p>$$y = \bar y,\quad x = \bar x.$$&lt;/p>
&lt;p>So, they intersect perpendicularly.&lt;/p>
&lt;img src="../img/regression/non_linear_regression.svg" alt="Non linear regression" width="500">
&lt;h3 id="regression-coefficient">Regression coefficient&lt;/h3>
&lt;p>The most important parameter of a regression line is the slope.&lt;/p>
&lt;div class="alert alert-def">
&lt;div>
&lt;strong>Definition - Regression coefficient&lt;/strong> $b_{yx}$. Given a sample of a two-dimensional variable $(X,Y)$, the _regression coefficient_ of the regression line of $Y$ on $X$ is its slope,$$b_{yx} = \frac{s_{xy}}{s_x^2}$$
&lt;/div>
&lt;/div>
&lt;div class="alert alert-warning">
&lt;div>
The regression coefficient has always the same sign as the covariance.
&lt;/div>
&lt;/div>
&lt;div class="alert alert-int">
&lt;div>
It measures how the dependent variable changes in relation to the independent one according to the regression line. In particular, it gives the number of units that the dependent variable increases or decreases for every unit that the independent variable increases.
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Example&lt;/strong>. In the sample of heights and weights, the regression line of weight on height was&lt;/p>
&lt;p>$$y=-108.49 +1.02 x.$$&lt;/p>
&lt;p>Thus, the regression coefficient of weight on height is&lt;/p>
&lt;p>$$b_{yx}= 1.02 \mbox{Kg/cm.}$$&lt;/p>
&lt;p>That means that, according to the regression line of weight on height, the weight will increase $1.02$ Kg for every cm that the height increases.&lt;/p>
&lt;h3 id="regression-predictions">Regression predictions&lt;/h3>
&lt;p>Usually the regression models are used to predict the dependent variable for some values of the independent variable.&lt;/p>
&lt;p>&lt;strong>Example&lt;/strong>. In the sample of heights and weights, to predict the weight of a person with a height of 180 cm, we have to use the regression line of weight on height,&lt;/p>
&lt;p>$$y = -108.49 + 1.02 \cdot 180 = 75.11 \mbox{ Kg}.$$&lt;/p>
&lt;p>But to predict the height of a person with a weight of 79 Kg, we have to use the regression line of height on weight,&lt;/p>
&lt;p>$$x = 130.78 + 0.63\cdot 79 = 180.55 \mbox{ cm}.$$&lt;/p>
&lt;p>&lt;em>However, how reliable are these predictions?&lt;/em>&lt;/p>
&lt;h2 id="correlation">Correlation&lt;/h2>
&lt;p>Once we have a regression model, in order to see if it is a good predictive model we have to assess the goodness of fit of the model and the strength of the of relation set by it. The part of Statistics in charge of this is &lt;strong>correlation&lt;/strong>.&lt;/p>
&lt;p>The correlation study the residuals of a regression model: the smaller the residuals, the greater the goodness of fit, and the stronger the relation set by the model.&lt;/p>
&lt;h3 id="residual-variance">Residual variance&lt;/h3>
&lt;p>To measure the goodness of fit of a regression model is common to use the &lt;em>residual variance&lt;/em>.&lt;/p>
&lt;div class="alert alert-def">
&lt;div>
&lt;p>&lt;strong>Definition - Sample residual variance&lt;/strong> $s_{ry}^2$. Given a regression model $y=f(x)$ of a two-dimensional variable $(X,Y)$, its _sample residual variance_ is the average of the squared residuals,&lt;/p>
&lt;p>$$s_{ry}^2 = \frac{\sum e_{ij}^2n_{ij}}{n} = \frac{\sum (y_j - f(x_i))^2n_{ij}}{n}.$$&lt;/p>
&lt;/div>
&lt;/div>
&lt;div class="alert alert-int">
&lt;div>
&lt;p>The greater the residuals, the greater the residual variance and the smaller the goodness of fit.&lt;/p>
&lt;p>When the linear relation is perfect, the residuals are zero and the residual variance is zero. Conversely, when there are no relation, the residuals coincide with deviations from the mean, and the residual variance is equal to the variance of the dependent variable.&lt;/p>
&lt;p>$$0\leq s_{ry}^2\leq s_y^2$$&lt;/p>
&lt;/div>
&lt;/div>
&lt;h3 id="explained-and-non-explained-variation">Explained and non-explained variation&lt;/h3>
&lt;img src="../img/regression/variation_decomposition.gif" alt="Variation decomposition by regression model" width="600">
&lt;h3 id="coefficient-of-determination">Coefficient of determination&lt;/h3>
&lt;p>From the residual variance is possible to define another correlation statistic easier to interpret.&lt;/p>
&lt;div class="alert alert-def">
&lt;div>
&lt;strong>Definition - Sample coefficient of determination $r^2$&lt;/strong>. Given a regression model $y=f(x)$ of a two-dimensional variable $(X,Y)$, its &lt;em>coefficient of determination&lt;/em> is$$r^2 = 1- \frac{s_{ry}^2}{s_y^2}$$
&lt;/div>
&lt;/div>
&lt;div class="alert alert-warning">
&lt;div>
&lt;p>As the residual variance ranges from 0 to $s_y^2$, we have&lt;/p>
&lt;p>$$0\leq r^2\leq 1$$&lt;/p>
&lt;/div>
&lt;/div>
&lt;div class="alert alert-int">
&lt;div>
&lt;p>The greater $r^2$ is, the greater the goodness of fit of the regression model, and the more reliable will its predictions be. In particular,&lt;/p>
&lt;ul>
&lt;li>If $r^2 =0$ then there is no relation as set by the regression model.&lt;/li>
&lt;li>If $r^2=1$ then the relation set by the model is perfect.&lt;/li>
&lt;/ul>
&lt;/div>
&lt;/div>
&lt;div class="alert alert-warning">
&lt;div>
&lt;p>When the regression model is linear, the coefficient of determination can be computed with this formula&lt;/p>
&lt;p>$$ r^2 = \frac{s_{xy}^2}{s_x^2s_y^2}.$$&lt;/p>
&lt;/div>
&lt;/div>
&lt;div class="spoiler " >
&lt;p>
&lt;a class="btn btn-primary" data-toggle="collapse" href="#spoiler-18" role="button" aria-expanded="false" aria-controls="spoiler-18">
Proof
&lt;/a>
&lt;/p>
&lt;div class="collapse card " id="spoiler-18">
&lt;div class="card-body">
&lt;p>When the fitted model is the regression line, the the residual variance is&lt;/p>
&lt;p>$$
\begin{aligned}
s_{ry}^2 &amp;amp; = \sum e_{ij}^2f_{ij} = \sum (y_j - f(x_i))^2f_{ij} = \sum \left(y_j - \bar y -\frac{s_{xy}}{s_x^2}(x_i-\bar x) \right)^2f_{ij}= \newline
&amp;amp; = \sum \left((y_j - \bar y)^2 +\frac{s_{xy}^2}{s_x^4}(x_i-\bar x)^2 - 2\frac{s_{xy}}{s_x^2}(x_i-\bar x)(y_j -\bar y)\right)f_{ij} = \newline
&amp;amp; = \sum (y_j - \bar y)^2f_{ij} +\frac{s_{xy}^2}{s_x^4}\sum (x_i-\bar x)^2f_{ij}- 2\frac{s_{xy}}{s_x^2}\sum (x_i-\bar x)(y_j -\bar y)f_{ij}= \newline
&amp;amp; = s_y^2 + \frac{s_{xy}^2}{s_x^4}s_x^2 - 2 \frac{s_{xy}}{s_x^2}s_{xy} = s_y^2 - \frac{s_{xy}^2}{s_x^2}.
\end{aligned}
$$&lt;/p>
&lt;p>and the coefficient of determination is&lt;/p>
&lt;p>$$
\begin{aligned}
r^2 &amp;amp;= 1- \frac{s_{ry}^2}{s_y^2} = 1- \frac{s_y^2 - \frac{s_{xy}^2}{s_x^2}}{s_y^2} = 1 - 1 + \frac{s_{xy}^2}{s_x^2s_y^2} = \frac{s_{xy}^2}{s_x^2s_y^2}.
\end{aligned}
$$&lt;/p>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Example&lt;/strong>. In the sample of heights and weights, we had&lt;/p>
&lt;p>$$
\begin{array}{lll}
\bar x = 174.67 \mbox{ cm} &amp;amp; \quad &amp;amp; s^2_x = 102.06 \mbox{ cm}^2 \newline
\bar y = 69.67 \mbox{ Kg} &amp;amp; &amp;amp; s^2_y = 164.42 \mbox{ Kg}^2 \newline
s_{xy} = 104.07 \mbox{ cm$\cdot$ Kg}
\end{array}
$$&lt;/p>
&lt;p>Thus, the linear coefficient of determination is&lt;/p>
&lt;p>$$r^2 = \frac{s_{xy}^2}{s_x^2s_y^2} = \frac{(104.07 \mbox{ cm\cdot Kg})^2}{102.06 \mbox{ cm}^2 \cdot 164.42 \mbox{ Kg}^2} = 0.65.$$&lt;/p>
&lt;p>This means that the linear model of weight on height explains the 65% of the variation of weight, and the linear model of height on weight also explains 65% of the variation of height.&lt;/p>
&lt;h3 id="correlation-coefficient">Correlation coefficient&lt;/h3>
&lt;div class="alert alert-def">
&lt;div>
&lt;strong>Definition - Sample correlation coefficient $r$&lt;/strong>. Given a sample of a two-dimensional variable $(X,Y)$, the &lt;em>sample correlation coefficient&lt;/em> is the square root of the linear coefficient of determination, with the sign of the covariance,$$r = \dfrac{s_{xy}}{s_xs_y}.$$
&lt;/div>
&lt;/div>
&lt;div class="alert alert-warning">
&lt;div>
&lt;p>As $r^2$ ranges from 0 to 1, $r$ ranges from -1 to 1,&lt;/p>
&lt;p>$$-1\leq r\leq 1.$$&lt;/p>
&lt;/div>
&lt;/div>
&lt;div class="alert alert-int">
&lt;div>
&lt;p>The correlation coefficient measures not only the strength of the linear association but also its direction (increasing or decreasing):&lt;/p>
&lt;ul>
&lt;li>If $r=0$ then there is no linear relation.&lt;/li>
&lt;li>Si $r=1$ then there is a perfect increasing linear relation.&lt;/li>
&lt;li>Si $r=-1$ then there is a perfect decreasing linear relation.&lt;/li>
&lt;/ul>
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Example&lt;/strong>. In the sample of heights and weights, we had&lt;/p>
&lt;p>$$\begin{array}{lll}
\bar x = 174.67 \mbox{ cm} &amp;amp; \quad &amp;amp; s^2_x = 102.06 \mbox{ cm}^2 \newline
\bar y = 69.67 \mbox{ Kg} &amp;amp; &amp;amp; s^2_y = 164.42 \mbox{ Kg}^2 \newline
s_{xy} = 104.07 \mbox{ cm$\cdot$ Kg}
\end{array}
$$&lt;/p>
&lt;p>Thus, the correlation coefficient is&lt;/p>
&lt;p>$$r = \frac{s_{xy}}{s_xs_y} = \frac{104.07 \mbox{ cm\cdot Kg}}{10.1 \mbox{ cm} \cdot 12.82 \mbox{ Kg}} = +0.8.$$&lt;/p>
&lt;p>This means that there is a rather strong linear, increasing, relation between height and weight.&lt;/p>
&lt;h3 id="different-linear-correlations">Different linear correlations&lt;/h3>
&lt;p>The scatter plots below show linear regression models with differents correlations.&lt;/p>
&lt;img src="../img/regression/different_correlations.svg" alt="Linear regression models with different correlations" width="700">
&lt;h3 id="reliability-of-regression-predictions">Reliability of regression predictions&lt;/h3>
&lt;p>The coefficient of determination explains the goodness of fit of a regression model, but there are other factors that influence the reliability of regression predictions:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>The coefficient of determination: The greater $r^2$, the greater the goodness of fit and the more reliable the predictions are.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>The variability of the population distribution: The greater the variation, the more difficult to predict and the less reliable the predictions are.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>The sample size: The greater the sample size, the more information we have and the more reliable the predictions are.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;div class="alert alert-warning">
&lt;div>
In addition, we have to take into account that a regression model is only valid for the range of values observed in the sample. That means that, as we don’t have any information outside that range, we must not do predictions for values far from that range.
&lt;/div>
&lt;/div>
&lt;h2 id="non-linear-regression">Non-linear regression&lt;/h2>
&lt;p>The fit of a non-linear regression can be also done by the least square fitting method.&lt;/p>
&lt;p>However, in some cases the fitting of a non-linear model can be reduced to the fitting of a linear model applying a simple transformation to the variables of the model.&lt;/p>
&lt;h3 id="transformations-of-non-linear-regression-models">Transformations of non-linear regression models&lt;/h3>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>Logarithmic&lt;/strong>: A logarithmic model $y = a+b \log x$ can be transformed in a linear model with the change $t=\log x$:&lt;/p>
&lt;p>$$y=a+b\log x = a+bt.$$&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Exponential&lt;/strong>: An exponential model $y = e^{a+bx}$ can be transformed in a linear model with the change $z = \log y$:&lt;/p>
&lt;p>$$z = \log y = \log(e^{a+bx}) = a+bx.$$&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Potential&lt;/strong>: A potential model $y = ax^b$ can be transformed in a linear model with the changes $t=\log x$ and $z=\log y$:&lt;/p>
&lt;p>$$z = \log y = \log(ax^b) = \log a + b \log x = a^\prime+bt.$$&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Inverse&lt;/strong>: An inverse model $y = a+b/x$ can be transformed in a linear model with the change $t=1/x$:&lt;/p>
&lt;p>$$y = a + b(1/x) = a+bt.$$&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Sigmoidal&lt;/strong>: A sigmoidal model $y = e^{a+b/x}$ can be transformed in a linear model with the changes $t=1/x$ and $z=\log y$:&lt;/p>
&lt;p>$$z = \log y = \log (e^{a+b/x}) = a+b(1/x) = a+bt.$$&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h3 id="exponential-relation">Exponential relation&lt;/h3>
&lt;p>&lt;strong>Example&lt;/strong>. The number of bacteria in a culture evolves with time according to the table below.&lt;/p>
&lt;p>$$\begin{array}{c|c}
\mbox{Hours} &amp;amp; \mbox{Bacteria} \newline
\hline
0 &amp;amp; 25 \newline
1 &amp;amp; 28 \newline
2 &amp;amp; 47 \newline
3 &amp;amp; 65 \newline
4 &amp;amp; 86 \newline
5 &amp;amp; 121 \newline
6 &amp;amp; 190 \newline
7 &amp;amp; 290 \newline
8 &amp;amp; 362
\end{array}
$$&lt;/p>
&lt;p>The scatter plot of the sample is showed below.&lt;/p>
&lt;img src="../img/regression/bacteria_evolution.svg" alt="Scatter plot of the evolution of a bacteria culture" width="500">
&lt;p>Fitting a linear model we get&lt;/p>
&lt;p>$$\mbox{Bacteria} = -30.18+41,27,\mbox{Hours, with } r^2=0.85.$$&lt;/p>
&lt;img src="../img/regression/linear_regression_bacteria.svg" alt="Linear regression of the bacteria evolution" width="500">
&lt;p>&lt;em>Is a good model?&lt;/em>&lt;/p>
&lt;p>Although the linear model is not bad, according to the shape of the point cloud of the scatter plot, an exponential model looks more suitable.&lt;/p>
&lt;p>To construct an exponential model $y = e^{a+bx}$ we can apply the transformation $z=\log y$, that is, applying a logarithmic transformation to the dependent variable.&lt;/p>
&lt;p>$$\begin{array}{c|c|c}
\mbox{Hours} &amp;amp; \mbox{Bacteria} &amp;amp; \mbox{$\log$(Bacteria)} \newline
\hline
0 &amp;amp; 25 &amp;amp; 3.22 \newline
1 &amp;amp; 28 &amp;amp; 3.33 \newline
2 &amp;amp; 47 &amp;amp; 3.85 \newline
3 &amp;amp; 65 &amp;amp; 4.17 \newline
4 &amp;amp; 86 &amp;amp; 4.45 \newline
5 &amp;amp; 121 &amp;amp; 4.80 \newline
6 &amp;amp; 190 &amp;amp; 5.25 \newline
7 &amp;amp; 290 &amp;amp; 5.67 \newline
8 &amp;amp; 362 &amp;amp; 5.89
\end{array}
$$&lt;/p>
&lt;img src="../img/regression/log_bacteria_evolution.svg" alt="Scatter plot of the evolution of the logarithm of bacteria culture" width="500">
&lt;p>Now it only remains to compute the regression line of the logarithm of bacteria on hours,&lt;/p>
&lt;p>$$\mbox{$\log$(Bacteria)} = 3.107 + 0.352, \mbox{Horas},$$&lt;/p>
&lt;p>and, undoing the change of variable,&lt;/p>
&lt;p>$$\mbox{Bacteria} = e^{3.107+0.352,\mbox{Hours}}, \mbox{ with } r^2=0.99.$$&lt;/p>
&lt;img src="../img/regression/exponential_regression_bacteria.svg" alt="Exponential regression of the bacteria evolution" width="500">
&lt;p>Thus, the exponential model fits much better than the linear model.&lt;/p>
&lt;h2 id="regression-risks">Regression risks&lt;/h2>
&lt;h3 id="lack-of-fit-does-not-mean-independence">Lack of fit does not mean independence&lt;/h3>
&lt;p>It is important to note that every regression model has its own coefficient of determination.&lt;/p>
&lt;div class="alert alert-warning">
&lt;div>
Thus, a coefficient of determination near zero means that there is no relation as set by the model, but &lt;em>that does not mean that the variables are independent&lt;/em>, because there could be a different type of relation.
&lt;/div>
&lt;/div>
&lt;div class="center">
&lt;img src="../img/regression/linear_regression_parabolic_relation.svg" alt="Linear regression on a cuadratic relation" width="500">
&lt;img src="../img/regression/parabolic_regression.svg" alt="Cuadratic regression on a cuadratic relation" width="500">
&lt;/div>
&lt;h3 id="outliers-influence-in-regression">Outliers influence in regression&lt;/h3>
&lt;p>Outliers in regression studies are points that clearly do not follow the tendency of the rest of points, even if the values of the pair are not outliers for every variable separately.&lt;/p>
&lt;img src="../img/regression/scatterplot_with_outliers.svg" alt="Scatter plot with an outlier" width="500">
&lt;p>Outliers in regression studies can provoke drastic changes in the regression models.&lt;/p>
&lt;div class="center">
&lt;img src="../img/regression/linear_regression_with_outliers.svg" alt="Linear regression with an outlier" width="500"> &lt;img src="../img/regression/linear_regression_without_outliers.svg" alt="Linear regression without outliers" width="500">
&lt;/div>
&lt;h3 id="the-simpsons-paradox">The Simpson&amp;rsquo;s paradox&lt;/h3>
&lt;p>Sometimes a trend can disappears or even reverses when we split the sample into groups according to a qualitative variable that is related to the dependent variable.
This is known as the &lt;em>Simpson&amp;rsquo;s paradox&lt;/em>.&lt;/p>
&lt;p>&lt;strong>Example&lt;/strong>. The scatterplot below shows an inverse relation between the study hours and the score in an exam.&lt;/p>
&lt;div class="center">
&lt;img src="../img/regression/simpson_paradox_1.svg" alt="Simpon's paradox. Inverse relation between study hours and the score in an exam." width="500">
&lt;/div>
&lt;p>But if we split the sample in two groups (good and bad students) we get different trends and now the relation is direct, which makes more sense.&lt;/p>
&lt;div class="center">
&lt;img src="../img/regression/simpson_paradox_2.svg" alt="Simpon's paradox. Direct relation between study hours and the score in an exam."" width="500">
&lt;/div></description></item><item><title>Problems of Linear Regression</title><link>/en/teaching/statistics/problems/linear_regression/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/en/teaching/statistics/problems/linear_regression/</guid><description>&lt;h2 id="exercise-1">Exercise 1&lt;/h2>
&lt;p>Give some examples of:&lt;/p>
&lt;ol>
&lt;li>Non related variables.&lt;/li>
&lt;li>Variables that are increasingly related.&lt;/li>
&lt;li>Variables that are decreasingly related.&lt;/li>
&lt;/ol>
&lt;div class="spoiler " >
&lt;p>
&lt;a class="btn btn-primary" data-toggle="collapse" href="#spoiler-0" role="button" aria-expanded="false" aria-controls="spoiler-0">
Solution
&lt;/a>
&lt;/p>
&lt;div class="collapse card " id="spoiler-0">
&lt;div class="card-body">
&lt;ol>
&lt;li>The daily averge temperature and the daily number of births in a city.&lt;/li>
&lt;li>The hours preparing an exam and the score.&lt;/li>
&lt;li>The weight of a person and the time require to run 100 meters.&lt;/li>
&lt;/ol>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;h2 id="exercise-2">Exercise 2&lt;/h2>
&lt;p>In a study about the effect of different doses of a medicament, 2 patients got 2 mg and took 5 days to cure, 4 patients got 2 mg and took 6 days to cure, 2 patients got 3 mg ant took 3 days to cure, 4 patients got 3 mg and took 5 days to cure, 1 patient got 3 mg and took 6 days to cure, 5 patients got 4 mg and took 3 days to cure and 2 patients got 4 mg and took 5 days to cure.&lt;/p>
&lt;ol>
&lt;li>Construct the joint frequency table.&lt;/li>
&lt;li>Get the marginal frequency distributions and compute the main statistics for each variable.&lt;/li>
&lt;li>Compute the covariance and interpret it.&lt;/li>
&lt;/ol>
&lt;div class="spoiler " >
&lt;p>
&lt;a class="btn btn-primary" data-toggle="collapse" href="#spoiler-1" role="button" aria-expanded="false" aria-controls="spoiler-1">
Solution
&lt;/a>
&lt;/p>
&lt;div class="collapse card " id="spoiler-1">
&lt;div class="card-body">
&lt;ol>
&lt;li>&lt;/li>
&lt;/ol>
&lt;p>$$
\begin{array}{c|c|c|c}
\hline
\mbox{dose/days} &amp;amp; 3 &amp;amp; 5 &amp;amp; 6\newline
\hline
2 &amp;amp; 0 &amp;amp; 2 &amp;amp; 4\newline
\hline
3 &amp;amp; 2 &amp;amp; 4 &amp;amp; 1\newline
\hline
4 &amp;amp; 5 &amp;amp; 2 &amp;amp; 0\newline
\hline
\end{array}
$$&lt;/p>
&lt;ol start="2">
&lt;li>&lt;/li>
&lt;/ol>
&lt;p>$$
\begin{array}{c|c|c|c|c}
\hline
\mbox{dose/days} &amp;amp; 3 &amp;amp; 5 &amp;amp; 6 &amp;amp; \mbox{Sum}\newline
\hline
2 &amp;amp; 0 &amp;amp; 2 &amp;amp; 4 &amp;amp; 6\newline
\hline
3 &amp;amp; 2 &amp;amp; 4 &amp;amp; 1 &amp;amp; 7\newline
\hline
4 &amp;amp; 5 &amp;amp; 2 &amp;amp; 0 &amp;amp; 7\newline
\hline
\mbox{Sum} &amp;amp; 7 &amp;amp; 8 &amp;amp; 5 &amp;amp; 20\newline
\hline
\end{array}
$$&lt;/p>
&lt;p>Dose: $\bar x=3.05$ mg, $s_x^2=0.6475$ mg$^2$, $s_x=0.8047$ mg.&lt;br/>
Days: $\bar y=4.55$ days, $s_y^2=1.4475$ days$^2$, $s_y=1.2031$ days.&lt;br/>
3. $s_{xy}=-0.6775$ mg$\cdot$days.&lt;/p>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;h2 id="exercise-3">Exercise 3&lt;/h2>
&lt;p>The table below shows the two-dimensional frequency distribution of a sample of 80 persons in a study about the relation between the blood cholesterol ($X$) in mg/dl and the high blood pressure ($Y$).&lt;/p>
&lt;p>$$
\begin{array}{|c||c|c|c||c|}
\hline
X\setminus Y &amp;amp; [110,130) &amp;amp; [130,150) &amp;amp; [150,170) &amp;amp; n_x \newline
\hline\hline
[170,190) &amp;amp; &amp;amp; 4 &amp;amp; &amp;amp; 12\newline
\hline
[190,210) &amp;amp; 10 &amp;amp; 12 &amp;amp; 4 &amp;amp; \newline
\hline
[210,230) &amp;amp; 7 &amp;amp; &amp;amp; 8 &amp;amp; \newline
\hline
[230,250) &amp;amp; 1 &amp;amp; &amp;amp; &amp;amp; 18\newline
\hline\hline
n_y &amp;amp; &amp;amp; 30 &amp;amp; 24 &amp;amp; \newline
\hline
\end{array}
$$&lt;/p>
&lt;ol>
&lt;li>Complete the table.&lt;/li>
&lt;li>Construct the linear regression model of cholesterol on pressure.&lt;/li>
&lt;li>Use the linear model to calculate the expected cholesterol for a person with pressure 160 mmHg.&lt;/li>
&lt;li>According to the linear model, what is the expected pressure for a person with cholesterol 270 mg/dl?&lt;/li>
&lt;/ol>
&lt;p>Use the following sums: $\sum x_i=16960$ mg/dl, $\sum y_j=11160$ mmHg, $\sum x_i^2=3627200$ (mg/dl)$^2$, $\sum y_j^2=1576800$ mmHg$^2$ y $\sum x_iy_j=2378800$ mg/dl$\cdot$mmHg.&lt;/p>
&lt;div class="spoiler " >
&lt;p>
&lt;a class="btn btn-primary" data-toggle="collapse" href="#spoiler-2" role="button" aria-expanded="false" aria-controls="spoiler-2">
Solution
&lt;/a>
&lt;/p>
&lt;div class="collapse card " id="spoiler-2">
&lt;div class="card-body">
&lt;ol>
&lt;li>&lt;/li>
&lt;/ol>
&lt;p>$$
\begin{array}{|c||c|c|c||c|}
\hline
X\setminus Y &amp;amp; [110,130) &amp;amp; [130,150) &amp;amp; [150,170) &amp;amp; n_x \newline
\hline\hline
[170,190) &amp;amp; 8 &amp;amp; 4 &amp;amp; 0 &amp;amp; 12\newline
\hline
[190,210) &amp;amp; 10 &amp;amp; 12 &amp;amp; 4 &amp;amp; 26 \newline
\hline
[210,230) &amp;amp; 7 &amp;amp; 9 &amp;amp; 8 &amp;amp; 24 \newline
\hline
[230,250) &amp;amp; 1 &amp;amp; 5 &amp;amp; 12 &amp;amp; 18\newline
\hline\hline
n_y &amp;amp; 26 &amp;amp; 30 &amp;amp; 24 &amp;amp; 80\newline
\hline
\end{array}
$$&lt;/p>
&lt;ol start="2">
&lt;li>&lt;/li>
&lt;/ol>
&lt;p>$\bar x=212$ mg/dl, $s_x^2=396$ (mg/dl)$^2$.&lt;br/>
$\bar y=139.5$ mmHg, $s_y^2=249.75$ mmHg$^2$.&lt;br/>
$s_{xy}=161$ mg/dl$\cdot$mmHg.&lt;br/>
Regression line of cholesterol on blood pressure: $x=122.0721 + 0.6446y$.&lt;br/>
3. $x(160)=225.2152$ mg/dl.&lt;br/>
4.&lt;/p>
&lt;p>Regression line of blood pressure on cholesterol: $y=53.3081 + 0.4066x$.&lt;br/>
$y(270)=163.0808$ mmHg.&lt;/p>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;h2 id="exercise-4">Exercise 4&lt;/h2>
&lt;p>A research study has been conducted to determine the loss of activity of a drug. The table below shows the results of the experiment.&lt;/p>
&lt;p>$$
\begin{array}{lrrrrr}
\hline
\mbox{Time (in years)} &amp;amp; 1 &amp;amp; 2 &amp;amp; 3 &amp;amp; 4 &amp;amp; 5 \newline
\mbox{Activity (%)} &amp;amp; 96 &amp;amp; 84 &amp;amp; 70 &amp;amp; 58 &amp;amp; 52 \newline
\hline
\end{array}
$$&lt;/p>
&lt;ol>
&lt;li>Construct the linear regression model of activity on time.&lt;/li>
&lt;li>According to the linear model, when will the activity be 80%? When will the drug have lost all activity?&lt;/li>
&lt;/ol>
&lt;div class="spoiler " >
&lt;p>
&lt;a class="btn btn-primary" data-toggle="collapse" href="#spoiler-3" role="button" aria-expanded="false" aria-controls="spoiler-3">
Solution
&lt;/a>
&lt;/p>
&lt;div class="collapse card " id="spoiler-3">
&lt;div class="card-body">
&lt;ol>
&lt;li>$\bar x=3$ years, $s_x^2=2$ years$^2$.&lt;br/>
$\bar y=72$ %, $s_y^2=264$ %$^2$.&lt;br/>
$s_{xy}=-22.8$ years$\cdot$%.&lt;br/>
Regression line of activity on time: $y=106.2 + -11.4x$.&lt;br/>&lt;/li>
&lt;li>&lt;/li>
&lt;/ol>
&lt;p>Regression line of time on activity: $x=9.2182 + -0.0864y$.&lt;br/>
$x(80)=2.3091$ years and $x(0)=9.2182$ years.
&lt;/div>
&lt;/div>
&lt;/div>
&lt;h2 id="exercise-5">Exercise 5&lt;/h2>
&lt;p>A basketball team is testing a new stretching program to reduce the injuries during the league. The data below show the daily number of minutes doing stretching exercises and the number of injuries along the league.&lt;/p>
&lt;p>$$
\begin{array}{lrrrrrrrr}
\hline
\mbox{Stretching minutes} &amp;amp; 0 &amp;amp; 30 &amp;amp; 10 &amp;amp; 15 &amp;amp; 5 &amp;amp; 25 &amp;amp; 35 &amp;amp; 40\newline
\mbox{Injuries} &amp;amp; 4 &amp;amp; 1 &amp;amp; 2 &amp;amp; 2 &amp;amp; 3 &amp;amp; 1 &amp;amp; 0 &amp;amp; 1\newline
\hline
\end{array}
$$&lt;/p>
&lt;ol>
&lt;li>Construct the regression line of the number of injuries on the time of stretching.&lt;/li>
&lt;li>How much is the reduction of injuries for every minute of stretching?&lt;/li>
&lt;li>How many minutes of stretching are require for having no injuries? Is reliable this prediction?&lt;/li>
&lt;/ol>
&lt;p>Use the following sums ($X$=Number of minutes stretching, and $Y$=Number of injuries): $\sum x_i =160$ min, $\sum y_j=14$ injuries, $\sum x_i^2=4700$ min$^2$, $\sum y_j^2=36$ injuries$^2$ and $\sum x_iy_j=160$ min$\cdot$injuries.&lt;/p>
&lt;div class="spoiler " >
&lt;p>
&lt;a class="btn btn-primary" data-toggle="collapse" href="#spoiler-4" role="button" aria-expanded="false" aria-controls="spoiler-4">
Solution
&lt;/a>
&lt;/p>
&lt;div class="collapse card " id="spoiler-4">
&lt;div class="card-body">
&lt;ol>
&lt;li>$\bar x=20$ min, $s_x^2=187.5$ min$^2$.&lt;br/>
$\bar y=1.75$ injuries, $s_y^2=1.4375$ injuries$^2$.&lt;br/>
$s_{xy}=-15$ min$\cdot$injuries.&lt;br/>
Regression line of injuries on time of stetching: $y=3.35 + -0.08x$.&lt;br/>&lt;/li>
&lt;li>$0.08$ injuries/min.&lt;br/>&lt;/li>
&lt;li>&lt;/li>
&lt;/ol>
&lt;p>Regression line of time of stretching on injuries: $x=38.2609 + -10.4348y$.&lt;br/>
$x(0)=38.2609$ min.&lt;br/>
$r^2=0.8348$.
&lt;/div>
&lt;/div>
&lt;/div>
&lt;h2 id="exercise-6">Exercise 6&lt;/h2>
&lt;p>For two variables $X$ and $Y$ we have&lt;/p>
&lt;ul>
&lt;li>The regression line of $Y$ on $X$ is $y-x-2=0$.&lt;/li>
&lt;li>The regression line of $X$ on $Y$ is $y-4x+22=0$.&lt;/li>
&lt;/ul>
&lt;p>Calculate:&lt;/p>
&lt;ol>
&lt;li>The means $\bar x$ and $\bar y$.&lt;/li>
&lt;li>The correlation coefficient.&lt;/li>
&lt;/ol>
&lt;div class="spoiler " >
&lt;p>
&lt;a class="btn btn-primary" data-toggle="collapse" href="#spoiler-5" role="button" aria-expanded="false" aria-controls="spoiler-5">
Solution
&lt;/a>
&lt;/p>
&lt;div class="collapse card " id="spoiler-5">
&lt;div class="card-body">
&lt;ol>
&lt;li>$\bar x=8$ and $\bar y=10$.&lt;br/>&lt;/li>
&lt;li>$r=0.5$.&lt;/li>
&lt;/ol>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;h2 id="exercise-7">Exercise 7&lt;/h2>
&lt;p>The means of two variables $X$ and $Y$ are $\bar x=2$ and $\bar y=1$, and the correlation coefficient is 0.&lt;/p>
&lt;ol>
&lt;li>Predict the value of $Y$ for $x=10$.&lt;/li>
&lt;li>Predict the value of $X$ for $y=5$.&lt;/li>
&lt;li>Plot both regression lines.&lt;/li>
&lt;/ol>
&lt;div class="spoiler " >
&lt;p>
&lt;a class="btn btn-primary" data-toggle="collapse" href="#spoiler-6" role="button" aria-expanded="false" aria-controls="spoiler-6">
Solution
&lt;/a>
&lt;/p>
&lt;div class="collapse card " id="spoiler-6">
&lt;div class="card-body">
&lt;ol>
&lt;li>$y(10)=1$.&lt;br/>&lt;/li>
&lt;li>$x(5)=2$.&lt;/li>
&lt;/ol>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;h2 id="exercise-8">Exercise 8&lt;/h2>
&lt;p>A study to determine the relation between the age and the physical strength gave the scatter plot below.
&lt;img src="../img/age_physical_strength_scatterplot-1.svg" title="plot of chunk age_physical_strength_scatterplot" alt="plot of chunk age_physical_strength_scatterplot" style="display: block; margin: auto;" />&lt;/p>
&lt;ol>
&lt;li>Calculate the linear coefficient of determination for the whole sample.&lt;/li>
&lt;li>Calculate the linear coefficient of determination for the sample of people younger than 25 years old.&lt;/li>
&lt;li>Calculate the linear coefficient of determination for the sample of people older than 25 years old.&lt;/li>
&lt;li>For which age group the relation between age and strength is stronger?&lt;/li>
&lt;/ol>
&lt;p>Use the following sums ($X$=Age and $Y=$Weight lifted).&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Whole sample: $\sum x_i=431$ years, $\sum y_j=769$ Kg, $\sum x_i^2=13173$ years$^2$, $\sum y_j^2=39675$ Kg$^2$ and $\sum x_iy_j=21792$ years$\cdot$Kg.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Young people: $\sum x_i=123$ years, $\sum y_j=294$ Kg, $\sum x_i^2=2339$ years$^2$, $\sum y_j^2=14418$ Kg$^2$ and $\sum x_iy_j=5766$ years$\cdot$Kg.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Old people: $\sum x_i=308$ years, $\sum y_j=475$ Kg, $\sum x_i^2=10834$ years$^2$, $\sum y_j^2=25257$ Kg$^2$ and $\sum x_iy_j=16026$ years$\cdot$Kg.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;div class="spoiler " >
&lt;p>
&lt;a class="btn btn-primary" data-toggle="collapse" href="#spoiler-7" role="button" aria-expanded="false" aria-controls="spoiler-7">
Solution
&lt;/a>
&lt;/p>
&lt;div class="collapse card " id="spoiler-7">
&lt;div class="card-body">
&lt;ol>
&lt;li>$\bar x=26.9375$ years, $s_x^2=97.6836$ years$^2$.&lt;br/>
$\bar y=48.0625$ kg, $s_y^2=169.6836$ kg$^2$.&lt;br/>
$s_{xy}=67.3164$ years$\cdot$kg.&lt;br/>
$r^2=0.2734$.&lt;br/>&lt;/li>
&lt;li>$\bar x=17.5714$ years, $s_x^2=25.3878$ years$^2$.&lt;br/>
$\bar y=42$ kg, $s_y^2=295.7143$ kg$^2$.&lt;br/>
$s_{xy}=85.7143$ years$\cdot$kg.&lt;br/>
$r^2=0.9786$.&lt;br/>&lt;/li>
&lt;li>$\bar x=34.2222$ years, $s_x^2=32.6173$ years$^2$.&lt;br/>
$\bar y=52.7778$ kg, $s_y^2=20.8395$ kg$^2$.&lt;br/>
$s_{xy}=-25.5062$ years$\cdot$kg.&lt;br/>
$r^2=0.9571$.&lt;br/>&lt;/li>
&lt;li>The linear relation between the age and the physical strength is a little bit stronger in the group of young people.&lt;/li>
&lt;/ol>
&lt;/div>
&lt;/div>
&lt;/div></description></item><item><title>Problems of Non Linear Regression</title><link>/en/teaching/statistics/problems/non_linear_regression/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/en/teaching/statistics/problems/non_linear_regression/</guid><description>&lt;h2 id="exercise-1">Exercise 1&lt;/h2>
&lt;p>A dietary center is testing a new diet in sample of 12 persons. The data below are the number of days of diet and the weight loss (in kg) until them for every person.&lt;/p>
&lt;pre>&lt;code>(33,3.9) (51,5.9) (30,3.2) (55,6) (38,4.9) (62,6.2) (35,4.5) (60,6.1) (44,5.6) (69,6.2) (47,5.8) (40,5.3)
&lt;/code>&lt;/pre>
&lt;ol>
&lt;li>Draw the scatter plot. According to the point cloud, what type of regression model explains better the relation between the weight loss and the days of diet?&lt;/li>
&lt;li>Construct the linear regression model and the logarithmic regression model of the weight loss on the number of days of diet.&lt;/li>
&lt;li>Use the best model to predict the weight that will lose a person after 40 and 100 days of diet. Are these predictions reliable?&lt;/li>
&lt;/ol>
&lt;p>Use the following sums ($X$=days of diet and $Y$=weight loss):
$\sum x_i=564$ days, $\sum \log(x_i)=45.8086$ $\log(\mbox{days})$, $\sum y_j=63.6$ kg, $\sum x_i^2=28234$ days$^2$, $\sum \log(x_i)^2=175.6603$ $\log(\mbox{days})^2$, $\sum y_j^2=347.7$ kg$^2$, $\sum x_iy_j=3108.5$ days$\cdot$kg, $\sum \log(x_i)y_j=245.4738$ $\log(\mbox{days})\cdot$kg.&lt;/p>
&lt;div class="spoiler " >
&lt;p>
&lt;a class="btn btn-primary" data-toggle="collapse" href="#spoiler-0" role="button" aria-expanded="false" aria-controls="spoiler-0">
Solution
&lt;/a>
&lt;/p>
&lt;div class="collapse card " id="spoiler-0">
&lt;div class="card-body">
&lt;ol>
&lt;li>&lt;/li>
&lt;/ol>
&lt;p>&lt;img src="../img/diet_scatterplot-1.svg" title="plot of chunk diet_scatterplot" alt="plot of chunk diet_scatterplot" style="display: block; margin: auto;" />
2. Linear model&lt;br/>
$\bar x=47$ days, $s_x^2=143.8333$ days$^2$.&lt;br/>
$\bar y=5.3$ kg, $s_y^2=0.885$ kg$^2$.&lt;br/>
$s_{xy}=9.9417$ days$\cdot$kg.&lt;br/>
Regression line of weight loss on days of diet: $y=2.0514 + 0.0691x$.&lt;br/>
$r^2=0.7765$.&lt;br/>&lt;/p>
&lt;p>Logartihmic model&lt;br/>
$\overline{\log(x)}=3.8174$ log(days), $s_{\log(x)}^2=0.0659$ log(days)$^2$.&lt;br/>
$s_{\log(x)y}=0.224$ log(days)$\cdot$kg.&lt;br/>
Logartihmic model of weight loss on days of diet: $y=-7.6678 + 3.397\log(x)$.&lt;br/>
$r^2=0.8599$.&lt;br/>
3. $y(40)=4.8635$ kg and $y(100)=7.9761$ kg. The predictions are reliable because the coefficient of determination is close to 1, but the last one is less reiable as 100 is far from the observed range of values in the sample.&lt;/p>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;h2 id="exercise-2">Exercise 2&lt;/h2>
&lt;p>The concentration of a drug in blood, in mg/dl, depends on time, in hours, according to the data below.&lt;/p>
&lt;p>$$
\begin{array}{lrrrrrrr}
\hline
\mbox{Time} &amp;amp; 2 &amp;amp; 3 &amp;amp; 4 &amp;amp; 5 &amp;amp; 6 &amp;amp; 7 &amp;amp; 8\newline
\mbox{Drug concentration} &amp;amp; 25 &amp;amp; 36 &amp;amp; 48 &amp;amp; 64 &amp;amp; 86 &amp;amp; 114 &amp;amp; 168\newline
\hline
\end{array}
$$&lt;/p>
&lt;ol>
&lt;li>Construct the linear regression model of drug concentration on time.&lt;/li>
&lt;li>Construct the exponential regression model of drug concentration on time.&lt;/li>
&lt;li>Use the best regression model to predict the drug concentration after $4.8$ hours? Is this prediction reliable? Justify your answer.&lt;/li>
&lt;/ol>
&lt;p>Use the following sums ($C$=Drug concentration and $T$=time): $\sum t_i=35$ h, $\sum \log(t_i)=10.6046$ $\log(\mbox{h})$, $\sum c_j=541$ mg/dl, $\sum \log(c_j)= 29.147$ $\log(\mbox{mg/dl})$, $\sum t_i^2=203$ h$^2$, $\sum \log(t_i)^2=17.5206$ $\log(\mbox{h})^2$, $\sum c_j^2=56937$ (mg/dl)$^2$, $\sum \log(c_j)^2=124.0131$ $\log(\mbox{mg/dl})^2$, $\sum t_ic_j=3328$ h$\cdot$mg/dl, $\sum t_i\log(c_j)=154.3387$ h$\cdot\log(\mbox{mg/dl})$, $\sum \log(t_i)c_j=951.6961$ $\log(\mbox{h})\cdot$mg/dl, $\sum\log(t_i)\log(c_j)=46.08046$ $\log(\mbox{h})\cdot\log(\mbox{mg/dl})$.&lt;/p>
&lt;div class="spoiler " >
&lt;p>
&lt;a class="btn btn-primary" data-toggle="collapse" href="#spoiler-1" role="button" aria-expanded="false" aria-controls="spoiler-1">
Solution
&lt;/a>
&lt;/p>
&lt;div class="collapse card " id="spoiler-1">
&lt;div class="card-body">
&lt;ol>
&lt;li>$\bar x=5$ hours, $s_x^2=4$ hours$^2$.&lt;br/>
$\bar y=77.2857$ mg/dl, $s_y^2=2160.7755$ (mg/dl)$^2$.&lt;br/>
$s_{xy}=89$ hours$\cdot$mg/dl.&lt;br/>
Regression line of drug concentration on time: $y=-33.9643 + 22.25x$.&lt;br/>
$r^2=0.9165$.&lt;br/>&lt;/li>
&lt;li>&lt;/li>
&lt;/ol>
&lt;p>$\overline{\log(y)}=4.1639$ log(mg/dl), $s_{\log(y)}^2=0.3785$ log(mg/dl)$^2$.&lt;br/>
$s_{x\log(y)}=1.2291$ hours$\cdot$log(mg/dl).&lt;br/>
Exponential model of drug concentration on time: $y=e^{2.6275 + 0.3073x}$.&lt;br/>
$r^2=0.9979$.&lt;br/>
3. $y(4.8)=60.4853$ mg/dl.
&lt;/div>
&lt;/div>
&lt;/div>
&lt;h2 id="exercise-3">Exercise 3&lt;/h2>
&lt;p>A researcher is studying the relation between the obesity and the response to pain. The obesity is measured as the percentage over the ideal weight, and the response to pain as the nociceptive flexion pain threshold. The results of the study appears in the table below.&lt;/p>
&lt;p>$$
\begin{array}{lrrrrrrrrrr}
\hline
\mbox{Obesity} &amp;amp; 89 &amp;amp; 90 &amp;amp; 77 &amp;amp; 30 &amp;amp; 51 &amp;amp; 75 &amp;amp; 62 &amp;amp; 45 &amp;amp; 90 &amp;amp; 20\newline
\mbox{Pain threshold} &amp;amp; 10 &amp;amp; 12 &amp;amp; 11.5 &amp;amp; 4.5 &amp;amp; 5.5 &amp;amp; 7 &amp;amp; 9 &amp;amp; 8 &amp;amp; 15 &amp;amp; 3\newline
\hline
\end{array}
$$&lt;/p>
&lt;ol>
&lt;li>According to the scatter plot, what model explains better the relation of the response to pain on the obesity?&lt;/li>
&lt;li>According to the best regression model, what is the response to pain expected for a person with an obesity of 50%? Is this prection reliable?&lt;/li>
&lt;li>According to the best regression model, what is the expected obesity for a person with a pain threshold of 10? Is this prediction reliable?&lt;/li>
&lt;/ol>
&lt;p>Use the following sums ($X$=Obesity and $Y$=Pain threshold):
$\sum x_i=629$, $\sum \log(x_i)=40.4121$, $\sum y_j=92.2$, $\sum \log(y_j)=21.339$, $\sum x_i^2=45445$, $\sum \log(x_i)^2=165.6795$, $\sum y_j^2=960.14$, $\sum \log(y_j)^2=47.6231$, $\sum x_iy_j=6537.7$, $\sum x_i\log(y_j)=1443.1275$, $\sum \log(x_i)y_j=387.5728$, $\sum \log(x_i)\log(y_j)=88.3696$.&lt;/p>
&lt;div class="spoiler " >
&lt;p>
&lt;a class="btn btn-primary" data-toggle="collapse" href="#spoiler-2" role="button" aria-expanded="false" aria-controls="spoiler-2">
Solution
&lt;/a>
&lt;/p>
&lt;div class="collapse card " id="spoiler-2">
&lt;div class="card-body">
&lt;ol>
&lt;li>&lt;/li>
&lt;/ol>
&lt;p>&lt;img src="../img/obesity_pain_scatterplot-1.svg" title="plot of chunk obesity_pain_scatterplot" alt="plot of chunk obesity_pain_scatterplot" style="display: block; margin: auto;" />
2. Linear model&lt;br/>
$\bar x=62.9$, $s_x^2=588.09$.&lt;br/>
$\bar y=9.22$, $s_y^2=11.0056$.&lt;br/>
$s_{xy}=82.0356$.&lt;br/>
Regression line of pain threshold on obesity: $y=1.3232 + 0.1255x$.&lt;br/>
$r^2=0.8422$.&lt;/p>
&lt;p>Logartihmic model&lt;br/>
$\overline{\log(x)}=4.0412$, $s_{\log(x)}^2=0.2366$.&lt;br/>
$s_{\log(x)y}=1.4973$.&lt;br/>
Logartihmic model of pain threshold on obesity: $y=-16.3578 + 6.3293\log(x)$.&lt;br/>
$r^2=0.8611$.&lt;br/>
$y(50)=8.4023$.&lt;br/>
3.&lt;/p>
&lt;p>Exponential model of obesity on pain threshold: $x=e^{2.7868 + 0.1361y}$.&lt;br/>
$x(10)=63.2648$.&lt;/p>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;h2 id="exercise-4">Exercise 4&lt;/h2>
&lt;p>A blood bank keeps plasma at a temperature of 0ºF.
When it is required for a blood transfusion, it is heated in an oven at a constant temperature of 120ºF.
In an experiment it has been measured the temperature of plasma at different times during the heating.
The results are in the table below.&lt;/p>
&lt;p>$$
\begin{array}{lrrrrrrrr}
\hline
\mbox{Time (min)} &amp;amp; 5 &amp;amp; 8 &amp;amp; 15 &amp;amp; 25 &amp;amp; 30 &amp;amp; 37 &amp;amp; 45 &amp;amp; 60\newline
\mbox{Temperature (ºF)} &amp;amp; 25 &amp;amp; 50 &amp;amp; 86 &amp;amp; 102 &amp;amp; 110 &amp;amp; 114 &amp;amp; 118 &amp;amp; 120\newline
\hline
\end{array}
$$&lt;/p>
&lt;ol>
&lt;li>Plot the scatter plot.
Which type of regression model do you think explains better relationship between temperature and time?&lt;/li>
&lt;li>Which transformation should we apply to the variables to have a linear relationship?&lt;/li>
&lt;li>Compute the logarithmic regression of the temperature on time.&lt;/li>
&lt;li>According to the logarithmic model, what will the temperature of the plasma be after 15 minutes of heating?
Is this prediction reliable? Justify your answer.&lt;/li>
&lt;/ol>
&lt;p>Use the following sums ($X$=Time and $Y$=Temperature):
$\sum x_i=225$ min, $\sum \log(x_i)=24.5289$ log(min), $\sum y_j=725$ ºF, $\sum \log(y_j)=35.2051$ log(ºF), $\sum x_i^2=8833$ min², $\sum \log(x_i)^2=80.4703$ log²(min), $\sum y_j^2=74345$ ºF², $\sum \log(y_j)^2=157.1023$ log²(ºF), $\sum x_iy_j=24393$ min⋅ºF, $\sum x_i\log(y_j)=1048.0142$ min⋅log(ºF), $\sum \log(x_i)y_j=2431.7096$ log(min)⋅ºF, $\sum \log(x_i)\log(y_j)=111.1165$ log(min)log(ºF).&lt;/p>
&lt;div class="spoiler " >
&lt;p>
&lt;a class="btn btn-primary" data-toggle="collapse" href="#spoiler-3" role="button" aria-expanded="false" aria-controls="spoiler-3">
Solution
&lt;/a>
&lt;/p>
&lt;div class="collapse card " id="spoiler-3">
&lt;div class="card-body">
&lt;ol>
&lt;li>&lt;/li>
&lt;/ol>
&lt;p>&lt;img src="../img/temperature_time_scatterplot-1.svg" title="plot of chunk temperature_time_scatterplot" alt="plot of chunk temperature_time_scatterplot" style="display: block; margin: auto;" />
A logarithmic model.&lt;br/>
2. Apply a logarithmic transformation to time $z=\log(x)$.&lt;/p>
&lt;ol start="3">
&lt;li>$\bar z=28.125$ log(min), $s_z^2=0.6577$ log²(min).&lt;br/>
$\bar y=90.625$ ºF, $s_y^2=1080.2344$ ºF².&lt;br/>
$s_{zy}=26.0969$ log(min)ºF.&lt;br/>
Logarithmic model of temperature on time: $y=-31.0325 + 39.6781\log(x)$.&lt;br/>&lt;/li>
&lt;li>$y(15)=76.4176$ ºF.&lt;br/>
$r^2=0.9586$, that is close to 1, so the prediction is reliable.&lt;/li>
&lt;/ol>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;h2 id="exercise-5">Exercise 5&lt;/h2>
&lt;p>The activity of a radioactive substance depends on time according to the data in the table below.&lt;/p>
&lt;p>$$
\begin{array}{lrrrrrrrr}
\hline
t\mbox{ (hours)} &amp;amp; 0 &amp;amp; 10 &amp;amp; 20 &amp;amp; 30 &amp;amp; 40 &amp;amp; 50 &amp;amp; 60 &amp;amp; 70 \newline
A\mbox{ ($10^7$ disintegrations/s)} &amp;amp; 25.9 &amp;amp; 8.16 &amp;amp; 2.57 &amp;amp; 0.81 &amp;amp; 0.25 &amp;amp; 0.08 &amp;amp; 0.03 &amp;amp; 0.01\newline
\hline
\end{array}
$$&lt;/p>
&lt;ol>
&lt;li>Represent graphically the data of radioactivity as a function of time.
Which type of regression model explains better the relationship between radioactivity and time?&lt;/li>
&lt;li>Represent graphically the data of radioactivity as a function of time in a semi-logarithmic paper.&lt;/li>
&lt;li>Compute the regression line of the logarithm of radioactivity on time.&lt;/li>
&lt;li>Taking into account that radioactivity decay follows the formula
\newline[
A(t) = A_0 e^{-\lambda t}
\newline]
where $A_0$ is the number of disintegrations at the begining and $\lambda$ is a disintegration constant, different for each radioactive substance, use the slope of the previous regression line to compute the disintegration constant for the substance.&lt;/li>
&lt;/ol>
&lt;p>Use the following sums ($X$=Time and $Y$=Radioactivity):
$\sum x_i=280$ hours, $\sum y_j=37.81$ 10⁷ disintegrations/s, $\sum \log(y_j)=-5.9371$ log(10⁷ disintegrations/s), $\sum x_i^2=14000$ hours², $\sum y_j^2=744.7265$ 10⁷ disintegrations/s², $\sum \log(y_j)^2=57.7369$ log²(10⁷ disintegrations/s), $\sum x_iy_j=173.8$ hours⋅10⁷ disintegrations/s, $\sum x_i\log(y_j)=-680.9447$ hours⋅log(10⁷ disintegrations/s).&lt;/p>
&lt;div class="spoiler " >
&lt;p>
&lt;a class="btn btn-primary" data-toggle="collapse" href="#spoiler-4" role="button" aria-expanded="false" aria-controls="spoiler-4">
Solution
&lt;/a>
&lt;/p>
&lt;div class="collapse card " id="spoiler-4">
&lt;div class="card-body">
&lt;ol>
&lt;li>&lt;/li>
&lt;/ol>
&lt;p>&lt;img src="../img/radioactivity_time_scatterplot-1.svg" title="plot of chunk radioactivity_time_scatterplot" alt="plot of chunk radioactivity_time_scatterplot" style="display: block; margin: auto;" />
2.
&lt;img src="../img/log_radioactivity_time_scatterplot-1.svg" title="plot of chunk log_radioactivity_time_scatterplot" alt="plot of chunk log_radioactivity_time_scatterplot" style="display: block; margin: auto;" />&lt;/p>
&lt;ol start="3">
&lt;li>$\bar x=35$ hours, $s_x^2=525$ hours².&lt;br/>
$\bar z=-0.7421$ log(10⁷ disintegrations/s), $s_z^2=6.6664$ log(10⁷ disintegrations/s)^2.&lt;br/>
$s_{xz}=-59.1434$ hours⋅log(10⁷ disintegrations/s)&lt;br/>
Regression line of logarithm of radioactivity on time: $z=3.2008 + -0.1127x$.&lt;br/>&lt;/li>
&lt;li>$\lambda=0.1127$.&lt;/li>
&lt;/ol>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;h2 id="exercise-6">Exercise 6&lt;/h2>
&lt;p>For oscillations of small amplitude, the oscillation period $T$ of a pendulum is given by the formula
\newline[
T = 2\pi\sqrt{\frac{L}{g}}
\newline]
where $L$ is the length of the pendulum and $g$ is the gravitational constant. In order to check if the previous formula is satisfied, an experiment has been conducted where it has been measured the oscillation period for different lengths of the pendulum.The measurements are shown in the table below.&lt;/p>
&lt;p>$$
\begin{array}{lrrrrr}
\hline
L\text{ (cm)} &amp;amp; 52.5 &amp;amp; 68.0 &amp;amp; 99.0 &amp;amp; 116.0 &amp;amp; 146.0 \newline
P\text{ (seg)} &amp;amp; 1.449 &amp;amp; 1.639 &amp;amp; 1.999 &amp;amp; 2.153 &amp;amp; 2.408\newline
\hline
\end{array}
$$&lt;/p>
&lt;ol>
&lt;li>Represent graphically the data of the period versus the length of the pendulum.&lt;br>
Does a linear model fit well to the points cloud?&lt;/li>
&lt;li>Represent graphically the data of the period versus the length in a logarithmic paper.
Which type of model fits better to the points cloud?&lt;/li>
&lt;li>Compute the regression line of the logarithm of period on the logarithm of length.&lt;/li>
&lt;li>Taking in to account the independent term of the previous regression line, compute the value of $g$.&lt;/li>
&lt;/ol>
&lt;div class="spoiler " >
&lt;p>
&lt;a class="btn btn-primary" data-toggle="collapse" href="#spoiler-5" role="button" aria-expanded="false" aria-controls="spoiler-5">
Solution
&lt;/a>
&lt;/p>
&lt;div class="collapse card " id="spoiler-5">
&lt;div class="card-body">
&lt;ol>
&lt;li>&lt;/li>
&lt;/ol>
&lt;p>&lt;img src="../img/period_length_scatterplot-1.svg" title="plot of chunk period_length_scatterplot" alt="plot of chunk period_length_scatterplot" style="display: block; margin: auto;" />
The linear model fits well to the points cloud.&lt;br>
&lt;img src="../img/log_period_length_scatterplot-1.svg" title="plot of chunk log_period_length_scatterplot" alt="plot of chunk log_period_length_scatterplot" style="display: block; margin: auto;" />
2. The model that best fits the points cloud is linear.&lt;br>
3. Let $X$ be the logarithm of length and $Y$ to the logarithm of period,&lt;/p>
&lt;p>$\bar x=4.5025$ log(cm), $s_x^2=0.1353$ log(cm)².&lt;br/>
$\bar y=0.6407$ log(s), $s_y^2=0.0339$ log(s)².&lt;br/>
$s_{xy}=0.0677$ log(cm)log(s)&lt;br>
Regression line of Y on X: $y=-1.6132 + 0.5006x$.&lt;br/>
4. $g=994.4579 cm/s².&lt;/p>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;h2 id="exercise-7">Exercise 7&lt;/h2>
&lt;p>A study tries to determine the relationship between two substances $X$ and $Y$ in blood.
The concentrations of these substances have been measured in seven individuals (in $\mu$g/dl) and the results are shown in the table below.&lt;/p>
&lt;p>$$
\begin{array}{rrrrrrrr}
\hline
X &amp;amp; 2.1 &amp;amp; 4.9 &amp;amp; 9.8 &amp;amp; 11.7 &amp;amp; 5.9 &amp;amp; 8.4 &amp;amp; 9.2 \newline
Y &amp;amp; 1.3 &amp;amp; 1.5 &amp;amp; 1.7 &amp;amp; 1.8 &amp;amp; 1.5 &amp;amp; 1.7 &amp;amp; 1.7 \newline
\hline
\end{array}
$$&lt;/p>
&lt;ol>
&lt;li>Are $Y$ and $X$ linearly related?&lt;/li>
&lt;li>Are $Y$ and $X$ potentially related?&lt;/li>
&lt;li>Use the best of the previous regression models to predict the concentration in blood of $Y$ for $x=8$ $\mu$gr/dl.Is this prediction reliable. Justify your answer.&lt;/li>
&lt;/ol>
&lt;p>Use the following sums:
$\sum x_i=52$ μg/dl, $\sum \log(x_i)=13.1955$ log(μg/dl), $\sum y_j=11.2$ μg/dl, $\sum \log(y_j)=3.253$ log(μg/dl), $\sum x_i^2=451.36$ (μg/dl)², $\sum \log(x_i)^2=26.9397$ log(μg/dl)², $\sum y_j^2=18.1$ (μg/dl)², $\sum \log(y_j)^2=1.5878$ log(μg/dl)², $\sum x_iy_j=86.57$ (μg/dl)², $\sum x_i\log(y_j)=26.3463$ μg/dl⋅log(μg/dl), $\sum \log(x_i)y_j=21.7087$ log(μg/dl)⋅μg/dl, $\sum \log(x_i)\log(y_j)=6.5224$ log(μg/dl)².&lt;/p>
&lt;div class="spoiler " >
&lt;p>
&lt;a class="btn btn-primary" data-toggle="collapse" href="#spoiler-6" role="button" aria-expanded="false" aria-controls="spoiler-6">
Solution
&lt;/a>
&lt;/p>
&lt;div class="collapse card " id="spoiler-6">
&lt;div class="card-body">
&lt;ol>
&lt;li>&lt;/li>
&lt;/ol>
&lt;p>$\bar x=7.4286$ μg/dl, $s_x^2=9.2963$ (μg/dl)².&lt;br/>
$\bar z=-0.7421$ μg/dl, $s_z^2=6.6664$ (μg/dl)².&lt;br/>
$s_{xz}=-0.4147$ (μg/dl)²&lt;br>
Linear relation: $r^2=0.9696$, that is close to 1, so there is a strong linear relation.&lt;br>
2. Naming $u=\log(x)$ and $v=\log(y)$,&lt;/p>
&lt;p>$\bar u=1.8851$ log(μg/dl), $s_u^2=0.295$ log(μg/dl)².&lt;br/>
$\bar v=0.4647$ log(μg/dl), $s_v^2=0.0109$ log(μg/dl)².&lt;br/>
$s_{uv}=0.0558$ (μg/dl)²&lt;br>
Potential relation: $r^2=0.9688$, that is close to 1, so there is a strong potential relation, although the linear relation is a little bit stronger.&lt;br>
3. Regression line of $Y$ on $X$: $y=1.2153 + 0.0518x$.&lt;br/>
$y(8)=1.6296$ μg/dl. The prediction is reliable since the linear coefficient of determination is close to 1.&lt;/p>
&lt;/div>
&lt;/div>
&lt;/div></description></item></channel></rss>
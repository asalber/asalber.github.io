<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Tangent Line | Aprende con Alf</title><link>/en/tag/tangent-line/</link><atom:link href="/en/tag/tangent-line/index.xml" rel="self" type="application/rss+xml"/><description>Tangent Line</description><generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><image><url>/images/logo_hude38443eeb2faa5fa84365aba7d86a77_3514_300x300_fit_lanczos_3.png</url><title>Tangent Line</title><link>/en/tag/tangent-line/</link></image><item><title>One variable differential calculus</title><link>/en/teaching/calculus/manual/derivatives-one-variable/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/en/teaching/calculus/manual/derivatives-one-variable/</guid><description>&lt;h2 id="concept-of-derivative">Concept of derivative&lt;/h2>
&lt;h3 id="increment">Increment&lt;/h3>
&lt;div class="alert alert-def">
&lt;div>
&lt;strong>Definition - Increment of a variable&lt;/strong>. An &lt;em>increment of a variable&lt;/em> $x$ is a change in the value of the variable; it is denoted $\Delta x$. The increment of a variable $x$ along an interval $[a,b]$ is given by $$\Delta x = b-a.$$
&lt;/div>
&lt;/div>
&lt;div class="alert alert-def">
&lt;div>
&lt;strong>Definition - Increment of a function&lt;/strong>. The &lt;em>increment of a function&lt;/em> $y=f(x)$ along an interval $[a,b]\subseteq Dom(f)$ is given by $$\Delta y = f(b)-f(a).$$
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Example&lt;/strong>. The increment of $x$ along the interval $[2,5]$ is $\Delta x=5-2=3$, and the increment of the function $y=x^2$ along the same interval is $\Delta y=5^2-2^2=21$.&lt;/p>
&lt;h3 id="average-rate-of-change">Average rate of change&lt;/h3>
&lt;p>The study of a function $y=f(x)$ requires to understand how the function changes, that is, how the dependent variable $y$ changes when we change the independent variable $x$.&lt;/p>
&lt;div class="alert alert-def">
&lt;div>
&lt;strong>Definition - Average rate of change&lt;/strong>. The &lt;em>average rate of change&lt;/em> of a function $y=f(x)$ in an interval $[a,a+\Delta x]\subseteq Dom(f)$, is the quotient between the increment of $y$ and the increment of $x$ in that interval; it is denoted by $$\mbox{ARC}\;f[a,a+\Delta x]=\frac{\Delta y}{\Delta x}=\frac{f(a+\Delta x)-f(a)}{\Delta x}.$$
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Example - Area of a square&lt;/strong>. Let $y=x^2$ be the function that measures the area of a metallic square of side length $x$.&lt;/p>
&lt;p>If at any given time the side of the square is $a$, and we heat the square uniformly increasing the side by dilatation a quantity $\Delta x$, how much will increase the area of the square?&lt;/p>
&lt;p>$$
\Delta y = f(a+\Delta x)-f(a)=(a+\Delta x)^2-a^2=
a^2+2a\Delta x+\Delta x^2-a^2=2a\Delta x+\Delta x^2.
$$&lt;/p>
&lt;p>What is the average rate of change in the interval $[a,a+\Delta x]$? $$\mbox{ARC}\;f[a,a+\Delta x]=\frac{\Delta y}{\Delta x}=\frac{2a\Delta x+\Delta x^2}{\Delta x}=2a+\Delta x.$$&lt;/p>
&lt;img src="../img/derivatives1/square_area_variation.svg" alt="Variation of the area of a square" width="300">
&lt;h3 id="geometric-interpretation-of-the-average-rate-of-change">Geometric interpretation of the average rate of change&lt;/h3>
&lt;p>The average rate of change of a function $y=f(x)$ in an interval $[a,a+\Delta x]$ is the slope of the &lt;em>secant&lt;/em> line to the graph of $f$ through the points $(a,f(a))$ and $(a+\Delta x,f(a+\Delta x))$.&lt;/p>
&lt;img src="../img/derivatives1/secant_line.svg" alt="Secant line to a function" width="500">
&lt;h3 id="instantaneous-rate-of-change">Instantaneous rate of change&lt;/h3>
&lt;p>Often it is interesting to study the rate of change of a function, not in an interval, but in a point.&lt;/p>
&lt;p>Knowing the tendency of change of a function in an instant can be used to predict the value of the function in nearby instants.&lt;/p>
&lt;div class="alert alert-def">
&lt;div>
&lt;p>&lt;strong>Definition - Instantaneous rate of change and derivative&lt;/strong>. The &lt;em>instantaneous rate of change&lt;/em> of a function $f$ in a point $a$, is the limit of the average rate of change of $f$ in the interval $[a,a+\Delta x]$, when $\Delta x$ approaches 0; it is denoted by&lt;/p>
&lt;p>$$
\begin{aligned}
\textrm{IRC}\;f (a) &amp;amp;= \lim_{\Delta x\rightarrow 0} \textrm{ARC}\; f[a,a+\Delta x]=\lim_{\Delta x\rightarrow 0}\frac{\Delta y}{\Delta x}=\newline
&amp;amp;= \lim_{\Delta x\rightarrow 0}\frac{f(a+\Delta x)-f(a)}{\Delta x}.
\end{aligned}
$$&lt;/p>
&lt;p>When this limit exists, the function $f$ is said to be &lt;em>differentiable&lt;/em> at the point $a$, and its value is called the &lt;em>derivative&lt;/em> of $f$ at $a$, and it is denoted $f'(a)$ (Lagrange’s notation) or $\frac{df}{dx}(a)$ (Leibniz’s notation).&lt;/p>
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Example - Area of a square&lt;/strong>. Let us take again the function $y=x^2$ that measures the area of a metallic square of side $x$.&lt;/p>
&lt;p>If at any given time the side of the square is $a$, and we heat the square uniformly increasing the side, what is the tendency of change of the area in that moment?&lt;/p>
&lt;p>$$\begin{aligned}
\textrm{IRC}\;f(a)&amp;amp;=\lim_{\Delta x\rightarrow 0}\frac{\Delta y}{\Delta x} = \lim_{\Delta x\rightarrow 0}\frac{f(a+\Delta x)-f(a)}{\Delta x} =\newline
&amp;amp;= \lim_{\Delta x\rightarrow 0}\frac{2a\Delta x+\Delta x^2}{\Delta x}=\lim_{\Delta x\rightarrow 0} 2a+\Delta x= 2a.
\end{aligned}
$$&lt;/p>
&lt;p>Thus, $$f'(a)=\frac{df}{dx}(a)=2a,$$ indicating that the area of the square tends to increase the double of the side.&lt;/p>
&lt;h3 id="interpretation-of-the-derivative">Interpretation of the derivative&lt;/h3>
&lt;div class="alert alert-int">
&lt;div>
&lt;p>The derivative of a function $f'(a)$ shows the growth rate of $f$ at point $a$:&lt;/p>
&lt;ul>
&lt;li>$f'(a)&amp;gt;0$ indicates an increasing tendency ($y$ increases as $x$ increases).&lt;/li>
&lt;li>$f'(a)&amp;lt;0$ indicates a decreasing tendency ($y$ decreases as $x$ increases).&lt;/li>
&lt;/ul>
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Example&lt;/strong>. A derivative $f'(a)=3$ indicates that $y$ tends to increase triple of $x$ at point $a$. A derivative $f'(a)=-0.5$ indicates that $y$ tends to decrease half of $x$ at point $a$.&lt;/p>
&lt;h3 id="geometric-interpretation-of-the-derivative">Geometric interpretation of the derivative&lt;/h3>
&lt;p>We have seen that the average rate of change of a function $y=f(x)$ in an interval $[a,a+\Delta x]$ is the slope of the secant line, but when $\Delta x$ approaches $0$, the secant line becomes the tangent line.&lt;/p>
&lt;p>The instantaneous rate of change or derivative of a function $y=f(x)$ at $x=a$ is the slope of the &lt;em>tangent line&lt;/em> to the graph of $f$ at point $(a,f(a))$. Thus, the equation of the tangent line to the graph of $f$ at the point $(a,f(a))$ is $$y-f(a) = f'(a)(x-a) \Leftrightarrow y = f(a)+f'(a)(x-a)$$&lt;/p>
&lt;img src="../img/derivatives1/tangent_line.svg" alt="Tangent line to a function" width="450">
&lt;h3 id="kinematic-applications-linear-motion">Kinematic applications: Linear motion&lt;/h3>
&lt;p>Assume that the function $y=f(t)$ describes the position of an object moving in the real line at time $t$. Taking as reference the coordinates origin $O$ and the unitary vector $\mathbf{i}=(1)$, we can represent the position of the moving object $P$ at every moment $t$ with a vector $\vec{OP}=x\mathbf{i}$ where $x=f(t)$.&lt;/p>
&lt;img src="../img/derivatives1/linear_motion.svg" alt="Linear motion" width="500">
&lt;p>&lt;strong>Remark&lt;/strong>. It also makes sense when $f$ measures other magnitudes as the temperature of a body, the concentration of a gas, or the quantity of substance in a chemical reaction at every moment $t$.&lt;/p>
&lt;h3 id="kinematic-interpretation-of-the-average-rate-of-change">Kinematic interpretation of the average rate of change&lt;/h3>
&lt;p>In this context, if we take the instants $t=t_0$ and $t=t_0+\Delta t$, both in $\mbox{Dom}(f)$, the vector $$\mathbf{v}_m=\frac{f(t_0+\Delta t)-f(t_0)}{\Delta t}$$ is known as the &lt;em>average velocity&lt;/em> of the trajectory $f$ in the interval $[t_0, t_0+\Delta t]$.&lt;/p>
&lt;p>&lt;strong>Example&lt;/strong>. A vehicle makes a trip from Madrid to Barcelona. Let $f(t)$ be the function that determine the position of the vehicle at every moment $t$. If the vehicle departs from Madrid (km 0) at 8:00 and arrives at Barcelona (km 600) at 14:00, then the average velocity of the vehicle in the path is $$\mathbf{v}_m=\frac{f(14)-f(8)}{14-8}=\frac{600-0}{6} = 100 km/h.$$&lt;/p>
&lt;h3 id="kinematic-interpretation-of-the-derivative">Kinematic interpretation of the derivative&lt;/h3>
&lt;p>In the same context of the linear motion, the derivative of the function $f(t)$ at the moment $t_0$ is the vector&lt;/p>
&lt;p>$$\mathbf{v}=f'(t_0)=\lim_{\Delta t\rightarrow 0}\frac{f(t_0+\Delta t)-f(t_0)}{\Delta t},$$&lt;/p>
&lt;p>that is known, as long as the limit exists, as the &lt;em>instantaneous velocity&lt;/em> or simply &lt;em>velocity&lt;/em> of the trajectory $f$ at moment $t_0$.&lt;/p>
&lt;p>That is, the derivative of the object position with respect to time is a vector field that is called &lt;em>velocity along the trajectory $f$&lt;/em>.&lt;/p>
&lt;p>&lt;strong>Example&lt;/strong>. Following with the previous example, what indicates the speedometer at any instant is the modulus of the instantaneous velocity vector at that moment.&lt;/p>
&lt;h2 id="algebra-of-derivatives">Algebra of derivatives&lt;/h2>
&lt;h3 id="properties-of-the-derivative">Properties of the derivative&lt;/h3>
&lt;p>If $y=c$, is a constant function, then $y'=0$ at any point.&lt;/p>
&lt;p>If $y=x$, is the identity function, then $y'=1$ at any point.&lt;/p>
&lt;p>If $u=f(x)$ and $v=g(x)$ are two differentiable functions, then&lt;/p>
&lt;ul>
&lt;li>$(u+v)'=u'+v'$&lt;/li>
&lt;li>$(u-v)'=u'-v'$&lt;/li>
&lt;li>$(u\cdot v)'=u'\cdot v+ u\cdot v'$&lt;/li>
&lt;li>$\left(\dfrac{u}{v}\right)'=\dfrac{u'\cdot v-u\cdot v'}{v^2}$&lt;/li>
&lt;/ul>
&lt;h3 id="derivative-of-a-composite-function">Derivative of a composite function&lt;/h3>
&lt;div class="alert alert-def">
&lt;div>
&lt;strong>Theorem - Chain rule&lt;/strong>. If the function $y=f\circ g$ is the composition of two functions $y=f(z)$ and $z=g(x)$, then $$(f\circ g)'(x)=f'(g(x))g'(x).$$
&lt;/div>
&lt;/div>
&lt;div class="spoiler " >
&lt;p>
&lt;a class="btn btn-primary" data-toggle="collapse" href="#spoiler-6" role="button" aria-expanded="false" aria-controls="spoiler-6">
Proof
&lt;/a>
&lt;/p>
&lt;div class="collapse card " id="spoiler-6">
&lt;div class="card-body">
It is easy to proof this fact using the Leibniz notation $$\frac{dy}{dx}=\frac{dy}{dz}\frac{dz}{dx}=f'(z)g'(x)=f'(g(x))g'(x).$$
&lt;/div>
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Example&lt;/strong>. If $f(z)=\sin z$ and $g(x)=x^2$, then $f\circ g(x)=\sin(x^2)$. Applying the chain rule the derivative of the composite function is
$$(f\circ g)'(x)=f'(g(x))g'(x) = \cos(g(x)) 2x = \cos(x^2)2x.$$&lt;/p>
&lt;p>On the other hand, $g\circ f(z)= (\sin z)^2$, and applying the chain rule again, its derivative is
$$(g\circ f)'(z)=g'(f(z))f'(z) = 2f(z)\cos z = 2\sin z\cos z.$$&lt;/p>
&lt;h3 id="derivative-of-the-inverse-of-a-function">Derivative of the inverse of a function&lt;/h3>
&lt;div class="alert alert-def">
&lt;div>
&lt;strong>Theorem - Derivative of the inverse function&lt;/strong>. Given a function $y=f(x)$ with inverse $x=f^{-1}(y)$, then $$\left(f^{-1}\right)'(y)=\frac{1}{f'(x)}=\frac{1}{f'(f^{-1}(y))},$$
provided that $f$ is differentiable at $f^{-1}(y)$ and $f'(f^{-1}(y))\neq 0$.
&lt;/div>
&lt;/div>
&lt;div class="spoiler " >
&lt;p>
&lt;a class="btn btn-primary" data-toggle="collapse" href="#spoiler-8" role="button" aria-expanded="false" aria-controls="spoiler-8">
Proof
&lt;/a>
&lt;/p>
&lt;div class="collapse card " id="spoiler-8">
&lt;div class="card-body">
It is easy to prove this equality using the Leibniz notation $$\frac{dx}{dy}=\frac{1}{dy/dx}=\frac{1}{f'(x)}=\frac{1}{f'(f^{-1}(y))}$$
&lt;/div>
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Example&lt;/strong>. The inverse of the exponential function $y=f(x)=e^x$ is the natural logarithm $x=f^{-1}(y)=\ln y$, so we can compute the derivative of the natural logarithm using the previous theorem and we get $$\left(f^{-1}\right)'(y)=\frac{1}{f'(x)}=\frac{1}{e^x}=\frac{1}{e^{\ln y}}=\frac{1}{y}.$$&lt;/p>
&lt;p>Sometimes it is easier to apply the chain rule to compute the derivative of the inverse of a function. In this example, as $\ln x$ is the inverse of $e^x$, we know that $e^{\ln x}=x$, so differentiating both sides and applying the chain rule to the left side we get $$(e^{\ln x})'=x' \Leftrightarrow e^{\ln x}(\ln(x))' = 1 \Leftrightarrow (\ln(x))'=\frac{1}{e^{\ln x}}=\frac{1}{x}.$$&lt;/p>
&lt;h2 id="analysis-of-functions">Analysis of functions&lt;/h2>
&lt;h3 id="analysis-of-functions-increase-and-decrease">Analysis of functions: increase and decrease&lt;/h3>
&lt;p>The main application of derivatives is to determine the variation (increase or decrease) of functions. For that we use the sign of the first derivative.&lt;/p>
&lt;div class="alert alert-def">
&lt;div>
&lt;p>&lt;strong>Theorem&lt;/strong>. Let $f(x)$ be a function with first derivative in an interval $I\subseteq \mathbb{R}$.&lt;/p>
&lt;ul>
&lt;li>If $\forall x\in I\ f'(x)&amp;gt; 0$ then $f$ is increasing on $I$.&lt;/li>
&lt;li>If $\forall x\in I\ f'(x)&amp;lt; 0$ then $f$ is decreasing on $I$.&lt;/li>
&lt;/ul>
&lt;/div>
&lt;/div>
&lt;p>If $f'(x_0)=0$ then $x_0$ is known as a &lt;em>critical point&lt;/em> or &lt;em>stationary point&lt;/em>. At this point the function can be increasing, decreasing or neither increasing nor decreasing.&lt;/p>
&lt;p>&lt;strong>Example&lt;/strong> The function $f(x)=x^2$ has derivative $f'(x)=2x$; it is decreasing on $\mathbb{R}^-$ as $f'(x)&amp;lt; 0$ $\forall x\in \mathbb{R}^-$ and increasing on $\mathbb{R}^+$ as $f'(x)&amp;gt; 0$ $\forall x\in \mathbb{R}^+$.
It has a critical point at $x=0$, as $f'(0)=0$; at this point the function is neither increasing nor decreasing.&lt;/p>
&lt;div class="alert alert-warning">
&lt;div>
A function can be increasing or decreasing on an interval and not have first derivative.
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Example&lt;/strong>. Let us analyze the increase and decrease of the function $f(x)=x^4-2x^2+1$. Its first derivative is $f'(x)=4x^3-4x$.&lt;/p>
&lt;img src="../img/derivatives1/increase_analysis.svg" alt="Inrease or decrease analysis of a function" width="550">
&lt;h3 id="analysis-of-functions-relative-extrema">Analysis of functions: relative extrema&lt;/h3>
&lt;p>As a consequence of the previous result we can also use the first derivative to determine the relative extrema of a function.&lt;/p>
&lt;div class="alert alert-def">
&lt;div>
&lt;p>&lt;strong>Theorem - First derivative test&lt;/strong>. Let $f(x)$ be a function with first derivative in an interval $I\subseteq \mathbb{R}$ and let $x_0\in I$ be a critical point of $f$ ($f'(x_0)=0$).&lt;/p>
&lt;ul>
&lt;li>If $f'(x)&amp;gt;0$ on an open interval extending left from $x_0$ and $f'(x)&amp;lt;0$ on an open interval extending right from $x_0$, then $f$ has a &lt;em>relative maximum&lt;/em> at $x_0$.&lt;/li>
&lt;li>If $f'(x)&amp;lt;0$ on an open interval extending left from $x_0$ and $f'(x)&amp;gt;0$ on an open interval extending right from $x_0$, then $f$ has a &lt;em>relative minimum&lt;/em> at $x_0$.&lt;/li>
&lt;li>If $f'(x)$ has the same sign on both an open interval extending left from $x_0$ and an open interval extending right from $x_0$, then $f$ has an &lt;em>inflection point&lt;/em> at $x_0$.&lt;/li>
&lt;/ul>
&lt;/div>
&lt;/div>
&lt;div class="alert alert-warning">
&lt;div>
A vanishing derivative is a necessary but not sufficient condition for the function to have a relative extrema at a point.
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Example&lt;/strong>. The function $f(x)=x^3$ has derivative $f'(x)=3x^2$; it has a critical point at $x=0$. However it does not have a relative extrema at that point, but an inflection point.&lt;/p>
&lt;p>&lt;strong>Example&lt;/strong>. Consider again the function $f(x)=x^4-2x^2+1$ and let us analyze its relative extrema now. Its first derivative is $f'(x)=4x^3-4x$.&lt;/p>
&lt;img src="../img/derivatives1/extrema_analysis.svg" alt="Extrema analysis of a function" width="550">
&lt;h3 id="analysis-of-functions-concavity">Analysis of functions: concavity&lt;/h3>
&lt;p>The concavity of a function can be determined by de second derivative.&lt;/p>
&lt;div class="alert alert-def">
&lt;div>
&lt;p>&lt;strong>Theorem&lt;/strong>. Let $f(x)$ be a function with second derivative in an interval $I\subseteq \mathbb{R}$.&lt;/p>
&lt;ul>
&lt;li>If $\forall x\in I\ f''(x)&amp;gt; 0$ then $f$ is concave up (convex) on $I$.&lt;/li>
&lt;li>If $\forall x\in I\ f''(x)&amp;lt; 0$ then $f$ is concave down (concave) on $I$.&lt;/li>
&lt;/ul>
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Example&lt;/strong>. The function $f(x)=x^2$ has second derivative $f''(x)=2&amp;gt;0$ $\forall x\in \mathbb{R}$, so it is concave up in all $\mathbb{R}$.&lt;/p>
&lt;div class="alert alert-warning">
&lt;div>
A function can be concave up or down and not have second derivative.
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Example&lt;/strong>. Let us analyze the concavity of the same function of previous examples $f(x)=x^4-2x^2+1$. Its second derivative is $f''(x)=12x^2-4$.&lt;/p>
&lt;img src="../img/derivatives1/concavity_analysis.svg" alt="Concavity analysis of a function" width="550">
&lt;h2 id="function-approximation">Function approximation&lt;/h2>
&lt;h3 id="approximating-a-function-with-the-derivative">Approximating a function with the derivative&lt;/h3>
&lt;p>The tangent line to the graph of a function $f(x)$ at $x=a$ can be used to approximate $f$ in a neighbourhood of $a$.&lt;/p>
&lt;p>Thus, the increment of a function $f(x)$ in an interval $[a,a+\Delta x]$ can be approximated multiplying the derivative of $f$ at $a$ by the increment of $x$ $$\Delta y \approx f'(a)\Delta x$$&lt;/p>
&lt;img src="../img/derivatives1/tangent_line_approximation.gif" alt="Approximation of a function with the tangent line" width="700">
&lt;p>&lt;strong>Example - Area of a square&lt;/strong>. In the previous example of the function $y=x^2$ that measures the area of a metallic square of side $x$, if the side of the square is $a$ and we increment it by a quantity $\Delta x$, then the increment on the area will be approximately $$\Delta y \approx f'(a)\Delta x = 2a\Delta x.$$
In the figure below we can see that the error of this approximation is $\Delta x^2$, which is smaller than $\Delta x$ when $\Delta x$ approaches to 0.&lt;/p>
&lt;img src="../img/derivatives1/square_area_variation_approximation.svg" alt="Approximation of the variation of a square area" width="300">
&lt;h3 id="approximating-a-function-by-a-polynomial">Approximating a function by a polynomial&lt;/h3>
&lt;p>Another useful application of the derivative is the approximation of functions by polynomials.&lt;/p>
&lt;p>Polynomials are functions easy to calculate (sums and products) with very good properties:&lt;/p>
&lt;ul>
&lt;li>Defined in all the real numbers.&lt;/li>
&lt;li>Continuous.&lt;/li>
&lt;li>Differentiable of all orders with continuous derivatives.&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Goal&lt;/strong> Approximate a function $f(x)$ by a polynomial $p(x)$ near a point $x=a$.&lt;/p>
&lt;h3 id="approximating-a-function-by-a-polynomial-of-order-0">Approximating a function by a polynomial of order 0&lt;/h3>
&lt;p>A polynomial of degree 0 has equation
$$p(x) = c_0,$$
where $c_0$ is a constant.&lt;/p>
&lt;p>As the polynomial should coincide with the function at $a$, it must satisfy
$$p(a) = c_0 = f(a).$$&lt;/p>
&lt;p>Therefore, the polynomial of degree 0 that best approximate $f$ near $a$ is
$$p(x) = f(a).$$&lt;/p>
&lt;img src="../img/derivatives1/approximation_polynomial_0.gif" alt="Approximation of a function by a polynomial of order 0" width="550">
&lt;h3 id="approximating-a-function-by-a-polynomial-of-order-1">Approximating a function by a polynomial of order 1&lt;/h3>
&lt;p>A polynomial of order 1 has equation
$$p(x) = c_0+c_1x,$$ but it can also be written as $$p(x) = c_0+c_1(x-a).$$&lt;/p>
&lt;p>Among all the polynomials of degree 1, the one that best approximates $f(x)$ near $a$ is that which meets the following conditions&lt;/p>
&lt;ol>
&lt;li>$p$ and $f$ coincide at $a$: $p(a) = f(a)$,&lt;/li>
&lt;li>$p$ and $f$ have the same rate of change at $a$: $p'(a) = f'(a)$.&lt;/li>
&lt;/ol>
&lt;p>The last condition guarantees that $p$ and $f$ have approximately the same tendency, but it requires the function $f$ to be differentiable at $a$.&lt;/p>
&lt;p>Imposing the previous conditions we have&lt;/p>
&lt;ol>
&lt;li>$p(x)=c_0+c_1(x-a) \Rightarrow p(a)=c_0+c_1(a-a)=c_0=f(a)$,&lt;/li>
&lt;li>$p'(x)=c_1 \Rightarrow p'(a)=c_1=f'(a)$.&lt;/li>
&lt;/ol>
&lt;p>Therefore, the polynomial of degree 1 that best approximates $f$ near $a$ is
$$p(x) = f(a)+f &amp;lsquo;(a)(x-a),$$
which turns out to be the tangent line to $f$ at $(a,f(a))$.&lt;/p>
&lt;img src="../img/derivatives1/approximation_polynomial_1.gif" alt="Approximation of a function by a polynomial of order 1" width="650">
&lt;h3 id="approximating-a-function-by-a-polynomial-of-order-2">Approximating a function by a polynomial of order 2&lt;/h3>
&lt;p>A polynomial of order 2 is a parabola with equation $$p(x) = c_0+c_1x+c_2x^2,$$ but it can also be written as
$$p(x) = c_0+c_1(x-a)+c_2(x-a)^2.$$&lt;/p>
&lt;p>Among all the polynomials of degree 2, the one that best approximate $f(x)$ near $a$ is that which meets the following conditions&lt;/p>
&lt;ol>
&lt;li>$p$ and $f$ coincide at $a$: $p(a) = f(a)$,&lt;/li>
&lt;li>$p$ and $f$ have the same rate of change at $a$: $p&amp;rsquo;(a) = f'(a)$.&lt;/li>
&lt;li>$p$ and $f$ have the same concavity at $a$: $p''(a)=f''(a)$.&lt;/li>
&lt;/ol>
&lt;p>The last condition requires the function $f$ to be differentiable twice at $a$.&lt;/p>
&lt;p>Imposing the previous conditions we have&lt;/p>
&lt;ol>
&lt;li>$p(x)=c_0+c_1(x-a) \Rightarrow p(a)=c_0+c_1(a-a)=c_0=f(a)$,&lt;/li>
&lt;li>$p'(x)=c_1 \Rightarrow p'(a)=c_1=f'(a)$.&lt;/li>
&lt;li>$p''(x)=2c_2 \Rightarrow p''(a)=2c_2=f''(a) \Rightarrow c_2=\frac{f''(a)}{2}$.&lt;/li>
&lt;/ol>
&lt;p>Therefore, the polynomial of degree 2 that best approximates $f$ near $a$ is
$$p(x) = f(a)+f'(a)(x-a)+\frac{f''(a)}{2}(x-a)^2.$$&lt;/p>
&lt;img src="../img/derivatives1/approximation_polynomial_2.gif" alt="Approximation of a function by a polynomial of order 2" width="750">
&lt;h3 id="approximating-a-function-by-a-polynomial-of-order-n">Approximating a function by a polynomial of order $n$&lt;/h3>
&lt;p>A polynomial of order $n$ has equation
$$p(x) = c_0+c_1x+c_2x^2+\cdots +c_nx^n,$$ but it can also be written as $$p(x) = c_0+c_1(x-a)+c_2(x-a)^2+\cdots +c_n(x-a)^n.$$&lt;/p>
&lt;p>Among all the polynomials of degree $n$, the one that best approximate $f(x)$ near $a$ is that which meets the following $n+1$ conditions:&lt;/p>
&lt;ol>
&lt;li>$p(a) = f(a)$,&lt;/li>
&lt;li>$p'(a) = f'(a)$,&lt;/li>
&lt;li>$p''(a)=f''(a)$,&lt;/li>
&lt;li>$\cdots$&lt;/li>
&lt;li>$p^{(n)}(a)=f^{(n)}(a)$.&lt;/li>
&lt;/ol>
&lt;p>The successive derivatives of $p$ are&lt;/p>
&lt;p>$$
\begin{aligned}
p(x) &amp;amp;= c_0+c_1(x-a)+c_2(x-a)^2+\cdots +c_n(x-a)^n,\newline
p'(x)&amp;amp; = c_1+2c_2(x-a)+\cdots +nc_n(x-a)^{n-1},\newline
p''(x)&amp;amp; = 2c_2+\cdots +n(n-1)c_n(x-a)^{n-2},\newline
\vdots
\newline
p^{(n)}(x)&amp;amp;= n(n-1)(n-2)\cdots 1 c_n=n!c_n.
\end{aligned}
$$&lt;/p>
&lt;p>Imposing the previous conditions we have&lt;/p>
&lt;ol>
&lt;li>$p(a) = c_0+c_1(a-a)+c_2(a-a)^2+\cdots +c_n(a-a)^n=c_0=f(a)$,&lt;/li>
&lt;li>$p'(a) = c_1+2c_2(a-a)+\cdots +nc_n(a-a)^{n-1}=c_1=f'(a)$,&lt;/li>
&lt;li>$p''(a) = 2c_2+\cdots +n(n-1)c_n(a-a)^{n-2}=2c_2=f''(a)\Rightarrow c_2=f''(a)/2$,&lt;/li>
&lt;li>$\cdots$&lt;/li>
&lt;li>$p^{(n)}(a)=n!c_n=f^{(n)}(a)=c_n=\frac{f^{(n)}(a)}{n!}$.&lt;/li>
&lt;/ol>
&lt;h3 id="taylor-polynomial-of-order-n">Taylor polynomial of order $n$&lt;/h3>
&lt;div class="alert alert-def">
&lt;div>
&lt;p>&lt;strong>Definition - Taylor polynomial&lt;/strong>. Given a function $f(x)$ differentiable $n$ times at $x=a$, the &lt;em>Taylor polynomial&lt;/em> of order $n$ of $f$ at $a$ is the polynomial with equation&lt;/p>
&lt;p>$$
\begin{aligned}
p_{f,a}^n(x) &amp;amp;= f(a) + f'(a)(x-a) + \frac{f''(a)}{2}(x-a)^2 + \cdots + \frac{f^{(n)}(a)}{n!}(x-a)^n = \newline
&amp;amp;= \sum_{i=0}^{n}\frac{f^{(i)}(a)}{i!}(x-a)^i.
\end{aligned}
$$&lt;/p>
&lt;/div>
&lt;/div>
&lt;div class="alert alert-int">
&lt;div>
The Taylor polynomial of order $n$ of $f$ at $a$ is the $n$th degree polynomial that best approximates $f$ near $a$, as is the only one that meets the previous conditions.
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Example&lt;/strong>. Let us approximate the function $f(x)=\log x$ near the value $1$ by a polynomial of order $3$.&lt;/p>
&lt;p>The equation of the Taylor polynomial of order $3$ of $f$ at $a=1$ is $$p_{f,1}^3(x)=f(1)+f'(1)(x-1)+\frac{f''(1)}{2}(x-1)^2+\frac{f'''(1)}{3!}(x-1)^3.$$ The derivatives of $f$ at $1$ up to order $3$ are&lt;/p>
&lt;p>$$
\begin{array}{lll}
f(x)=\log x &amp;amp; \quad &amp;amp; f(1)=\log 1 =0,\newline
f'(x)=1/x &amp;amp; &amp;amp; f'(1)=1/1=1,\newline
f''(x)=-1/x^2 &amp;amp; &amp;amp; f''(1)=-1/1^2=-1,\newline
f'''(x)=2/x^3 &amp;amp; &amp;amp; f'''(1)=2/1^3=2.
\end{array}
$$&lt;/p>
&lt;p>And substituting into the polynomial equation we get $$p_{f,1}^3(x)=0+1(x-1)+\frac{-1}{2}(x-1)^2+\frac{2}{3!}(x-1)^3= \frac{2}{3}x^3-\frac{3}{2}x^2+3x-\frac{11}{6}.$$&lt;/p>
&lt;img src="../img/derivatives1/taylor_polynomials_logarithm.gif" alt="Taylor polynomials of the logarithm function" width="650">
&lt;h3 id="maclaurin-polynomial-of-order-n">Maclaurin polynomial of order $n$&lt;/h3>
&lt;p>The Taylor polynomial equation has a simpler form when the polynomial is calculated at $0$. This special case of Taylor polynomial at $0$ is known as the &lt;em>Maclaurin polynomial&lt;/em>.&lt;/p>
&lt;div class="alert alert-def">
&lt;div>
&lt;p>&lt;strong>Definition - Maclaurin polynomial&lt;/strong>. Given a function $f(x)$ differentiable $n$ times at $0$, the &lt;em>Maclaurin polynomial&lt;/em> of order $n$ of $f$ is the polynomial with equation&lt;/p>
&lt;p>$$
\begin{aligned}
p_{f,0}^n(x)&amp;amp;=f(0)+f'(0)x+\frac{f''(0)}{2}x^2+\cdots +\frac{f^{(n)}(0)}{n!}x^n = \newline
&amp;amp;=\sum_{i=0}^{n}\frac{f^{(i)}(0)}{i!}x^i.
\end{aligned}
$$&lt;/p>
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Example&lt;/strong>. Let us approximate the function $f(x)=\sin x$ near the value $0$ by a polynomial of order $3$.&lt;/p>
&lt;p>The Maclaurin polynomial equation of order $3$ of $f$ is $$p_{f,0}^3(x)=f(0)+f'(0)x+\frac{f''(0)}{2}x^2+\frac{f'''(0)}{3!}x^3.$$
The derivatives of $f$ at $0$ up to order $3$ are&lt;/p>
&lt;p>$$\begin{array}{lll}
f(x)=\sin x &amp;amp; \quad &amp;amp; f(0)=\sin 0 =0,\newline
f'(x)=\cos x &amp;amp; &amp;amp; f'(0)=\cos 0=1,\newline
f''(x)=-\sin x &amp;amp; &amp;amp; f''(0)=-\sin 0=0,\newline
f'''(x)=-\cos x &amp;amp; &amp;amp; f'''(0)=-\cos 0=-1.
\end{array}
$$&lt;/p>
&lt;p>And substituting into the polynomial equation we get
$$p_{f,0}^3(x)=0+1\cdot x+\frac{0}{2}x^2+\frac{-1}{3!}x^3= x-\frac{x^3}{6}.$$&lt;/p>
&lt;img src="../img/derivatives1/maclaurin_polynomials_sine.gif" alt="Macalurin polynomials of the sine function" width="650">
&lt;h3 id="maclaurin-polynomials-of-elementary-functions">Maclaurin polynomials of elementary functions&lt;/h3>
&lt;p>$$
\renewcommand{\arraystretch}{2.5}
\begin{array}{cc}
\hline
f(x) &amp;amp; p_{f,0}^n(x) \newline
\hline
\sin x &amp;amp; \displaystyle x - \frac{x^3}{3!} + \frac{x^5}{5!} - \cdots + (-1)^k\frac{x^{2k-1}}{(2k-1)!} \mbox{ if $n=2k$ or $n=2k-1$}\newline
\cos x &amp;amp; \displaystyle 1 - \frac{x^2}{2!} + \frac{x^4}{4!} - \cdots + (-1)^k\frac{x^{2k}}{(2k)!} \mbox{ if $n=2k$ or $n=2k+1$}\newline
\arctan x &amp;amp; \displaystyle x - \frac{x^3}{3} + \frac{x^5}{5} - \cdots + (-1)^k\frac{x^{2k-1}}{(2k-1)} \mbox{ if $n=2k$ or $n=2k-1$}\newline
e^x &amp;amp; \displaystyle 1 + x + \frac{x^2}{2!} + \frac{x^3}{3!} + \cdots + \frac{x^n}{n!}\newline
\log(1+x) &amp;amp; \displaystyle x - \frac{x^2}{2} + \frac{x^3}{3} - \cdots + (-1)^{n-1}\frac{x^n}{n}\newline
\hline
\end{array}
$$&lt;/p>
&lt;h3 id="taylor-remainder-and-taylor-formula">Taylor remainder and Taylor formula&lt;/h3>
&lt;p>Taylor polynomials allow to approximate a function in a neighborhood of a value $a$, but most of the times there is an error in the approximation.&lt;/p>
&lt;div class="alert alert-def">
&lt;div>
&lt;p>&lt;strong>Definition - Taylor remainder&lt;/strong>. Given a function $f(x)$ and its Taylor polynomial of order $n$ at $a$, $p_{f,a}^n(x)$, the &lt;em>Taylor remainder&lt;/em> of order $n$ of $f$ at $a$ is the difference between the function and the polynomial,&lt;/p>
&lt;p>$$r_{f,a}^n(x)=f(x)-p_{f,a}^n(x).$$&lt;/p>
&lt;/div>
&lt;/div>
&lt;p>The Taylor remainder measures the error int the approximation of $f(x)$ by the Taylor polynomial and allow us to express the function as the Taylor polynomial plus the Taylor remainder&lt;/p>
&lt;p>$$f(x)=p_{f,a}^n(x) + r_{f,a}^n(x).$$&lt;/p>
&lt;p>This expression is known as the &lt;em>Taylor formula&lt;/em> of order $n$ or $f$ at $a$.&lt;/p>
&lt;p>It can be proved that&lt;/p>
&lt;p>$$\lim_{h\rightarrow 0}\frac{r_{f,a}^n(a+h)}{h^n}=0,$$&lt;/p>
&lt;p>which means that the remainder $r_{f,a}^n(a+h)$ is much smaller than $h^n$.&lt;/p></description></item><item><title>Several variables differentiable calculus</title><link>/en/teaching/calculus/manual/derivatives-n-variables/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/en/teaching/calculus/manual/derivatives-n-variables/</guid><description>&lt;h2 id="vector-functions-of-a-single-real-variable">Vector functions of a single real variable&lt;/h2>
&lt;div class="alert alert-def">
&lt;div>
&lt;p>&lt;strong>Definition - Vector function of a single real variable&lt;/strong>. A &lt;em>vector function of a single real variable&lt;/em> or &lt;em>vector field of a scalar variable&lt;/em> is a function that maps every scalar value $t\in D\subseteq \mathbb{R}$ into a vector $(x_1(t),\ldots,x_n(t))$ in $\mathbb{R}^n$:&lt;/p>
&lt;p>$$
\begin{array}{rccl}
f: &amp;amp; \mathbb{R} &amp;amp; \longrightarrow &amp;amp; \mathbb{R}^n\newline
&amp;amp; t &amp;amp; \longrightarrow &amp;amp; (x_1(t),\ldots, x_n(t))
\end{array}
$$&lt;/p>
&lt;p>where $x_i(t)$, $i=1,\ldots,n$, are real function of a single real variable known as &lt;em>coordinate functions&lt;/em>.&lt;/p>
&lt;/div>
&lt;/div>
&lt;p>The most common vector field of scalar variable are in the the real plane $\mathbb{R}^2$, where usually they are represented as&lt;/p>
&lt;p>$$f(t)=x(t)\mathbf{i}+y(t)\mathbf{j},$$&lt;/p>
&lt;p>and in the real space $\mathbb{R}^3$, where usually they are represented as&lt;/p>
&lt;p>$$f(t)=x(t)\mathbf{i}+y(t)\mathbf{j}+z(t)\mathbf{k},$$&lt;/p>
&lt;h3 id="graphic-representation-of-vector-fields">Graphic representation of vector fields&lt;/h3>
&lt;p>The graphic representation of a vector field in $\mathbb{R}^2$ is a trajectory in the real plane.&lt;/p>
&lt;img src="../img/derivativesn/trajectory_plane.svg" alt="Trajectory of a vector function in the plane." width="300">
&lt;p>The graphic representation of a vector field in $\mathbb{R}^3$ is a trajectory in the real space.&lt;/p>
&lt;img src="../img/derivativesn/trajectory_space.svg" alt="Trajectory of a vector function in the space." width="350">
&lt;h2 id="derivative-of-a-vector-field">Derivative of a vector field&lt;/h2>
&lt;p>The concept of derivative as the limit of the average rate of change of a function can be extended easily to vector fields.&lt;/p>
&lt;div class="alert alert-def">
&lt;div>
&lt;p>&lt;strong>Definition - Derivative of a vectorial field&lt;/strong>. A vectorial field $f(t)=(x_1(t),\ldots,x_n(t))$ is &lt;em>differentiable&lt;/em> at a point $t=a$ if the limit&lt;/p>
&lt;p>$$\lim_{\Delta t\rightarrow 0} \frac{f(a+\Delta t)-f(a)}{\Delta t}.$$&lt;/p>
&lt;p>exists. In such a case, the value of the limit is known as the &lt;em>derivative&lt;/em> of the vector field at $a$, and it is written $f'(a)$.&lt;/p>
&lt;/div>
&lt;/div>
&lt;p>Many properties of real functions of a single real variable can be extended to vector fields through its component functions. Thus, for instance, the derivative of a vector field can be computed from the derivatives of its component functions.&lt;/p>
&lt;div class="alert alert-theo">
&lt;div>
&lt;p>&lt;strong>Theorem&lt;/strong>. Given a vector field $f(t)=(x_1(t),\ldots,x_n(t))$, if $x_i(t)$ is differentiable at $t=a$ for all $i=1,\ldots,n$, then $f$ is differentiable at $a$ and its derivative is&lt;/p>
&lt;p>$$f'(a)=(x_1'(a),\ldots,x_n'(a))$$&lt;/p>
&lt;/div>
&lt;/div>
&lt;div class="spoiler " >
&lt;p>
&lt;a class="btn btn-primary" data-toggle="collapse" href="#spoiler-3" role="button" aria-expanded="false" aria-controls="spoiler-3">
Proof
&lt;/a>
&lt;/p>
&lt;div class="collapse card " id="spoiler-3">
&lt;div class="card-body">
&lt;p>The proof for a vectorial field in $\mathbb{R}^2$ is easy.&lt;/p>
&lt;p>$$\begin{aligned}
f'(a)&amp;amp;=\lim_{\Delta t\rightarrow 0} \frac{f(a+\Delta t)-f(a)}{\Delta t} = \lim_{\Delta t\rightarrow 0} \frac{(x(a+\Delta t),y(a+\Delta t))-(x(a),y(a))}{\Delta t} =\newline
&amp;amp;= \lim_{\Delta t\rightarrow 0} \left(\frac{x(a+\Delta t)-x(a)}{\Delta t},\frac{y(a+\Delta t)-y(a)}{\Delta t}\right) =\newline
&amp;amp;= \left(\lim_{\Delta t\rightarrow 0}\frac{x(a+\Delta t)-x(a)}{\Delta t},\lim_{\Delta t\rightarrow 0}\frac{y(a+\Delta t)-y(a)}{\Delta t}\right) =
(x'(a),y'(a)).
\end{aligned}
$$&lt;/p>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;h3 id="kinematics-curvilinear-motion">Kinematics: Curvilinear motion&lt;/h3>
&lt;p>The notion of derivative as a velocity along a trajectory in the real line can be generalized to a trajectory in any euclidean space $\mathbb{R}^n$.&lt;/p>
&lt;p>In case of a two dimensional space $\mathbb{R}^2$, if $f(t)$ describes the position of a moving object in the real plane at any time $t$, taking as reference the coordinates origin $O$ and the unitary vectors ${\mathbf{i}=(1,0),\mathbf{j}=(0,1)}$, we can represent the position of the moving object $P$ at every moment $t$ with a vector $\vec{OP}=x(t)\mathbf{i}+y(t)\mathbf{j}$, where the coordinates&lt;/p>
&lt;p>$$
\begin{cases}
x=x(t)\newline
y=y(t)
\end{cases}
\quad
t\in \mbox{Dom}(f)
$$&lt;/p>
&lt;p>are the &lt;em>coordinate functions&lt;/em> of $f$.&lt;/p>
&lt;img src="../img/derivativesn/curvilinear_motion.svg" alt="Trajectory of a curvilinear motion in the plane." width="500">
&lt;div class="alert alert-int">
&lt;div>
In this context the derivative of a trajectory $f'(a)=(x_1'(a),\ldots,x_n'(a))$ is the &lt;em>velocity&lt;/em> vector of the trajectory $f$ at moment $t=a$.
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Example&lt;/strong>. Given the trajectory $f(t) = (\cos t,\sin t)$, $t\in \mathbb{R}$, whose image is the unit circumference centred in the coordinate origin, its coordinate functions are $x(t) = \cos t$, $y(t) = \sin t$, $t\in \mathbb{R}$, and its velocity is&lt;/p>
&lt;p>$$\mathbf{v}=f'(t)=(x'(t),y'(t))=(-\sin t, \cos t).$$&lt;/p>
&lt;p>In the moment $t=\pi/4$, the object is in position $f(\pi/4) = (\cos(\pi/4),\sin(\pi/4)) =(\sqrt{2}/2,\sqrt{2}/2)$ and it is moving with a velocity $\mathbf{v}=f'(\pi/4)=(-\sin(\pi/4),\cos(\pi/4))=(-\sqrt{2}/2,\sqrt{2}/2)$.&lt;/p>
&lt;img src="../img/derivativesn/circumference_trajectory.svg" alt="Trajectory of a vector function in the space." width="300">
&lt;p>Observe that the module of the velocity vector is always 1 as $\vert\mathbf{v}\vert=\sqrt{(-\sin t)^2+(\cos t)^2}=1$.&lt;/p>
&lt;h3 id="tangent-line-to-a-trajectory">Tangent line to a trajectory&lt;/h3>
&lt;h4 id="tangent-line-to-a-trajectory-in-the-plane">Tangent line to a trajectory in the plane&lt;/h4>
&lt;h5 id="vectorial-equation">Vectorial equation&lt;/h5>
&lt;p>Given a trajectory $f(t)$ in the real plane, the vectors that are parallel to the velocity $\mathbf{v}$ at a moment $a$ are called &lt;em>tangent vectors&lt;/em> to the trajectory $f$ at the moment $a$, and the line passing through $P=f(a)$ directed by $\mathbf{v}$ is the tangent line to the graph of $f$ at the moment $a$.&lt;/p>
&lt;div class="alert alert-def">
&lt;div>
&lt;p>&lt;strong>Definition - Tangent line to a trajectory&lt;/strong>. Given a trajectory $f(t)$ in the real plane $\mathbb{R}^2$, the &lt;em>tangent line&lt;/em> to to the graph of $f$ at $a$ is the line with equation&lt;/p>
&lt;p>$$
\begin{aligned}
l:(x,y) &amp;amp;= f(a)+tf'(a) = (x(a),y(a))+t(x'(a),y'(a))\newline
&amp;amp; = (x(a)+tx'(a),y(a)+ty'(a)).
\end{aligned}
$$&lt;/p>
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Example&lt;/strong>. We have seen that for the trajectory $f(t) = (\cos t,\sin t)$, $t\in \mathbb{R}$, whose image is the unit circumference centred at the coordinate origin, the object position at the moment $t=\pi/4$ is $f(\pi/4)=(\sqrt{2}/2,\sqrt{2}/2)$ and its velocity $\mathbf{v}=(-\sqrt{2}/2,\sqrt{2}/2)$. Thus the equation of the tangent line to $f$ at that moment is&lt;/p>
&lt;p>$$
\begin{aligned}
l: (x,y) &amp;amp; = f(\pi/4)+t\mathbf{v} =
\left(\frac{\sqrt{2}}{2},\frac{\sqrt{2}}{2}\right)+t\left(\frac{-\sqrt{2}}{2},\frac{\sqrt{2}}{2}\right) =\newline
&amp;amp; =\left(\frac{\sqrt{2}}{2}-t\frac{\sqrt{2}}{2},\frac{\sqrt{2}}{2}+t\frac{\sqrt{2}}{2}\right).
\end{aligned}
$$&lt;/p>
&lt;h5 id="cartesian-and-point-slope-equations">Cartesian and point-slope equations&lt;/h5>
&lt;p>From the vectorial equation of the tangent to a trajectory $f(t)$ at the moment $t=a$ we can get the coordinate functions&lt;/p>
&lt;p>$$
\begin{cases}
x=x(a)+tx'(a)\newline
y=y(a)+ty'(a)
\end{cases}
\quad t\in \mathbb{R},
$$&lt;/p>
&lt;p>and solving for $t$ and equalling both equations we get the &lt;em>Cartesian equation&lt;/em> of the tangent&lt;/p>
&lt;p>$$\frac{x-x(a)}{x'(a)}=\frac{y-y(a)}{y'(a)},$$&lt;/p>
&lt;p>if $x'(a)\neq 0$ and $y'(a)\neq 0$.&lt;/p>
&lt;p>From this equation it is easy to get the &lt;em>point-slope equation&lt;/em> of the tangent&lt;/p>
&lt;p>$$y-y(a)=\frac{y'(a)}{x'(a)}(x-x(a)).$$&lt;/p>
&lt;p>&lt;strong>Example&lt;/strong>. Using the vectorial equation of the tangent of the previous example&lt;/p>
&lt;p>$$l: (x,y)=\left(\frac{\sqrt{2}}{2}-t\frac{\sqrt{2}}{2},\frac{\sqrt{2}}{2}+t\frac{\sqrt{2}}{2}\right),$$&lt;/p>
&lt;p>its Cartesian equation is $$\frac{x-\sqrt{2}/2}{-\sqrt{2}/2} = \frac{y-\sqrt{2}/2}{\sqrt{2}/2}$$
and the point-slope equation is&lt;/p>
&lt;p>$$y-\sqrt{2}/2 = \frac{-\sqrt{2}/2}{\sqrt{2}/2}(x-\sqrt{2}/2) \Rightarrow y=-x+\sqrt{2}.$$&lt;/p>
&lt;h3 id="normal-line-to-a-trajectory-in-the-plane">Normal line to a trajectory in the plane&lt;/h3>
&lt;p>We have seen that the tangent line to a trajectory $f(t)$ at $a$ is the line passing through the point $P=f(a)$ directed by the velocity vector $\mathbf{v}=f'(a)=(x'(a),y'(a))$. If we take as direction vector a vector orthogonal to $\mathbf{v}$, we get another line that is known as &lt;em>normal line&lt;/em> to the trajectory.&lt;/p>
&lt;div class="alert alert-def">
&lt;div>
&lt;p>&lt;strong>Definition - Normal line to a trajectory&lt;/strong>. Given a trajectory $f(t)$ in the real plane $\mathbb{R}^2$, the &lt;em>normal line&lt;/em> to the graph of $f$ at moment $t=a$ is the line with equation&lt;/p>
&lt;p>$$l: (x,y)=(x(a),y(a))+t(y'(a),-x'(a)) = (x(a)+ty'(a),y(a)-tx'(a)).$$&lt;/p>
&lt;/div>
&lt;/div>
&lt;p>The Cartesian equation is&lt;/p>
&lt;p>$$\frac{x-x(a)}{y'(a)} = \frac{y-y(a)}{-x'(a)},$$&lt;/p>
&lt;p>and the point-slope equation is&lt;/p>
&lt;p>$$y-y(a) = \frac{-x'(a)}{y'(a)}(x-x(a)).$$&lt;/p>
&lt;div class="alert alert-int">
&lt;div>
The normal line is always perpendicular to the tangent line as their direction vectors are orthogonal.
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Example&lt;/strong>. Considering again the trajectory of the unit circumference $f(t) = (\cos t,\sin t)$, $t\in \mathbb{R}$, the normal line to the graph of $f$ at moment $t=\pi/4$ is&lt;/p>
&lt;p>$$
\begin{aligned}
l: (x,y)&amp;amp;=(\cos(\pi/2),\sin(\pi/2))+t(\cos(\pi/2),\sin(\pi/2)) =\newline
&amp;amp;= \left(\frac{\sqrt{2}}{2},\frac{\sqrt{2}}{2}\right)+t\left(\frac{\sqrt{2}}{2},\frac{\sqrt{2}}{2}\right)
=\left(\frac{\sqrt{2}}{2}+t\frac{\sqrt{2}}{2},\frac{\sqrt{2}}{2}+t\frac{\sqrt{2}}{2}\right),
\end{aligned}
$$&lt;/p>
&lt;p>the Cartesian equation is&lt;/p>
&lt;p>$$\frac{x-\sqrt{2}/2}{\sqrt{2}/2} = \frac{y-\sqrt{2}/2}{\sqrt{2}/2},$$
and the point-slope equation is
$$y-\sqrt{2}/2 = \frac{\sqrt{2}/2}{\sqrt{2}/2}(x-\sqrt{2}/2) \Rightarrow y=x.$$&lt;/p>
&lt;h3 id="tangent-and-normal-lines-to-a-function">Tangent and normal lines to a function&lt;/h3>
&lt;p>A particular case of tangent and normal lines to a trajectory are the tangent and normal lines to a function of one real variable. For every function $y=f(x)$, the trajectory that trace its graph is&lt;/p>
&lt;p>$$g(x) = (x,f(x)) \quad x\in \mathbb{R},$$&lt;/p>
&lt;p>and its velocity is&lt;/p>
&lt;p>$$g'(x) = (1,f'(x)),$$&lt;/p>
&lt;p>so that the tangent line to $g$ at the moment $a$ is&lt;/p>
&lt;p>$$\frac{x-a}{1} = \frac{y-f(a)}{f'(a)} \Rightarrow y-f(a) = f'(a)(x-a),$$&lt;/p>
&lt;p>and the normal line is&lt;/p>
&lt;p>$$\frac{x-a}{f'(a)} = \frac{y-f(a)}{-1} \Rightarrow y-f(a) = \frac{-1}{f'(a)}(x-a).$$&lt;/p>
&lt;p>&lt;strong>Example&lt;/strong>. Given the function $y=x^2$, the trajectory that traces its graph is $g(x)=(x,x^2)$ and its velocity is $g'(x)=(1,2x)$. At the moment $x=1$ the trajectory passes through the point $(1,1)$ with a velocity $(1,2)$. Thus, the tangent line at that moment is&lt;/p>
&lt;p>$$\frac{x-1}{1} = \frac{y-1}{2} \Rightarrow y-1 = 2(x-1) \Rightarrow y = 2x-1,$$&lt;/p>
&lt;p>and the normal line is&lt;/p>
&lt;p>$$\frac{x-1}{2} = \frac{y-1}{-1} \Rightarrow y-1 = \frac{-1}{2}(x-1) \Rightarrow y = \frac{-x}{2}+\frac{3}{2}.$$&lt;/p>
&lt;h3 id="tangent-line-to-a-trajectory-in-the-space">Tangent line to a trajectory in the space&lt;/h3>
&lt;p>The concept of tangent line to a trajectory can be easily extended from the real plane to the three-dimensional space $\mathbb{R}^3$.&lt;/p>
&lt;p>If $f(t)=(x(t),y(t),z(t))$, $t\in \mathbb{R}$, is a trajectory in the real space $\mathbb{R}^3$, then at the moment $a$, the moving object that follows this trajectory will be at the position $P=(x(a),y(a),z(a))$ with a velocity $\mathbf{v}=f'(t)=(x'(t),y'(t),z'(t))$. Thus, the tangent line to $f$ at this moment have the following vectorial equation&lt;/p>
&lt;p>$$
\begin{aligned}
l&amp;amp;: (x,y,z)=(x(a),y(a),z(a))+t(x'(a),y'(a),z'(a)) =\newline
&amp;amp;= (x(a)+tx'(a),y(a)+ty'(a),z(a)+tz'(a)),
\end{aligned}
$$&lt;/p>
&lt;p>and the Cartesian equations are
$$\frac{x-x(a)}{x'(a)}=\frac{y-y(a)}{y'(a)}=\frac{z-z(a)}{z'(a)},$$
provided that $x'(a)\neq 0$, $y'(a)\neq 0$ y $z'(a)\neq 0$.&lt;/p>
&lt;p>&lt;strong>Example&lt;/strong>. Given the trajectory $f(t)=(\cos t, \sin t, t)$, $t\in \mathbb{R}$ in the real space, at the moment $t=\pi/2$ the trajectory passes through the point&lt;/p>
&lt;p>$$f(\pi/2)=(\cos(\pi/2),\sin(\pi/2),\pi/2)=(0,1,\pi/2),$$&lt;/p>
&lt;p>with velocity&lt;/p>
&lt;p>$$\mathbf{v}=f'(\pi/2)=(-\sin(\pi/2),\cos(\pi/2), 1)=(-1,0,1),$$&lt;/p>
&lt;p>and the tangent line to the graph of $f$ at that moment is&lt;/p>
&lt;p>$$l:(x,y,z)=(0,1,\pi/2)+t(-1,0,1) = (-t,1,t+\pi/2).$$&lt;/p>
&lt;img src="../img/derivativesn/tangent_trajectory_space.svg" alt="Tangent line to a trajectory in the space." width="400">
&lt;p>&lt;img src="/media/logo-geogebra.png#left" /> &lt;strong>&lt;a href="https://ggbm.at/Q2C7EfBn" target="_blank">Interactive Example&lt;/a>&lt;/strong>&lt;/p>
&lt;h3 id="normal-plane-to-a-trajectory-in-the-space">Normal plane to a trajectory in the space&lt;/h3>
&lt;p>In the three-dimensional space $\mathbb{R}^3$, the normal line to a trajectory is not unique. There are an infinite number of normal lines and all of them are in the normal plane.&lt;/p>
&lt;p>If $f(t)=(x(t),y(t),z(t))$, $t\in \mathbb{R}$, is a trajectory in the real space $\mathbb{R}^3$, then at the moment $a$, the moving object that follows this trajectory will be at the position $P=(x(a),y(a),z(a))$ with a velocity $\mathbf{v}=f'(t)=(x'(t),y'(t),z'(t))$. Thus, using the velocity vector as normal vector the normal plane to $f$ at this moment have the following vectorial equation&lt;/p>
&lt;p>$$
\begin{aligned}
\Pi &amp;amp;: (x-x(a),y-y(a),z-z(a))(x'(a),y'(a),z'(a)) = 0\newline
&amp;amp;= x'(a)(x-x(a))+y'(a)(y-y(a))+z'(a)(z-z(a))=0.
\end{aligned}
$$&lt;/p>
&lt;p>&lt;strong>Example&lt;/strong>. For the trajectory of the previous example $f(t)=(\cos t, \sin t, t)$, $t\in \mathbb{R}$, at the moment $t=\pi/2$ the trajectory passes through the point&lt;/p>
&lt;p>$$f(\pi/2)=(\cos(\pi/2),\sin(\pi/2),\pi/2)=(0,1,\pi/2),$$&lt;/p>
&lt;p>with velocity&lt;/p>
&lt;p>$$\mathbf{v}=f'(\pi/2)=(-\sin(\pi/2),\cos(\pi/2), 1)=(-1,0,1),$$
and normal plane to the graph of $f$ at that moment is&lt;/p>
&lt;p>$$\Pi:\left(x-0,y-1,z-\frac{\pi}{2}\right)(-1,0,1) =0 \Leftrightarrow -x+z-\frac{\pi}{2}=0.$$&lt;/p>
&lt;img src="../img/derivativesn/normal_plane_trajectory_space.svg" alt="Normal plane to a trajectory in the space." width="400">
&lt;p>&lt;img src="/media/logo-geogebra.png#left" /> &lt;strong>&lt;a href="https://ggbm.at/Q2C7EfBn" target="_blank">Interactive Example&lt;/a>&lt;/strong>&lt;/p>
&lt;h2 id="functions-of-several-variables">Functions of several variables&lt;/h2>
&lt;p>A lot of problems in Geometry, Physics, Chemistry, Biology, etc. involve a variable that depend on two or more variables:&lt;/p>
&lt;ul>
&lt;li>The area of a triangle depends on two variables that are the base and height lengths.&lt;/li>
&lt;li>The volume of a perfect gas depends on two variables that are the pressure and the temperature.&lt;/li>
&lt;li>The way travelled by an object free falling depends on a lot of variables: the time, the area of the cross section of the object, the latitude and longitude of the object, the height above the sea level, the air pressure, the air temperature, the speed of wind, etc.&lt;/li>
&lt;/ul>
&lt;p>These dependencies are expressed with functions of several variables.&lt;/p>
&lt;div class="alert alert-def">
&lt;div>
&lt;p>&lt;strong>Definition - Functions of several real variables&lt;/strong>. A &lt;em>function of $n$ real variables&lt;/em> or a &lt;em>scalar field&lt;/em> from a set $A_1\times \cdots \times A_n\subseteq \mathbb{R}^n$ in a set $B\subseteq \mathbb{R}$, is a relation that maps any tuple $(a_1,\ldots,a_n)\in A_1\times \cdots\times A_n$ into a unique element of $B$, denoted by $f(a_1,\ldots,a_n)$, that is knwon as the &lt;em>image&lt;/em> of $(a_1,\ldots,a_n)$ by $f$.&lt;/p>
&lt;p>$$
\begin{array}{lccc}
f: &amp;amp; A_1\times\cdots\times A_n &amp;amp; \longrightarrow &amp;amp; B\newline
&amp;amp;(a_1,\ldots,a_n) &amp;amp; \longrightarrow &amp;amp; f(a_1,\ldots,a_n)
\end{array}
$$&lt;/p>
&lt;/div>
&lt;/div>
&lt;ul>
&lt;li>The area of a triangle is a real function of two real variables&lt;/li>
&lt;/ul>
&lt;p>$$f(x,y)=\frac{xy}{2}.$$&lt;/p>
&lt;ul>
&lt;li>The volume of a perfect gas is a real function of two real variables&lt;/li>
&lt;/ul>
&lt;p>$$v=f(t,p)=\frac{nRt}{p},\quad \mbox{with $n$ and $R$ constants.}$$&lt;/p>
&lt;h3 id="graph-of-a-function-of-two-variables">Graph of a function of two variables&lt;/h3>
&lt;p>The graph of a function of two variables $f(x,y)$ is a surface in the real space $\mathbb{R}^3$ where every point of the surface has coordinates $(x,y,z)$, with $z=f(x,y)$.&lt;/p>
&lt;img src="../img/derivativesn/paraboloid.svg" alt="Graph of a two-variables function" width="300">
&lt;p>&lt;strong>Example&lt;/strong>. The function $f(x,y)=\dfrac{xy}{2}$ that measures the area of a triangle of base $x$ and height $y$ has the graph below.&lt;/p>
&lt;img src="../img/derivativesn/area_triangle_graph.svg" alt="Graph of the function that measures the area of a triangle." width="400">
&lt;p>The function $\displaystyle f(x,y)=\frac{\sin(x^2+y^2)}{\sqrt{x^2+y^2}}$ has the peculiar graph below.&lt;/p>
&lt;img src="../img/derivativesn/water_drop_graph.svg" alt="Surface of a drop of water." width="400">
&lt;h3 id="level-set-of-a-scalar-field">Level set of a scalar field&lt;/h3>
&lt;div class="alert alert-def">
&lt;div>
&lt;p>&lt;strong>Definition - Level set&lt;/strong> Given a scalar field $f:\mathbb{R}^n\rightarrow \mathbb{R}$, the &lt;em>level set&lt;/em> $c$ of $f$ is the set&lt;/p>
&lt;p>$$C_{f,c}={(x_1,\ldots,x_n): f(x_1,\ldots,x_n)=c},$$&lt;/p>
&lt;p>that is, a set where the function takes on the constant value $c$.&lt;/p>
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Example&lt;/strong>. Given the scalar field $f(x,y)=x^2+y^2$ and the point $P=(1,1)$, the level set of $f$ that includes $P$ is&lt;/p>
&lt;p>$$C_{f,2} = {(x,y): f(x,y)=f(1,1)=2} = {(x,y): x^2+y^2=2},$$&lt;/p>
&lt;p>that is the circumference of radius $\sqrt{2}$ centred at the origin.&lt;/p>
&lt;p>Level sets are common in applications like topographic maps, where the level curves correspond to points with the same height above the sea level,&lt;/p>
&lt;img src="../img/derivativesn/mountain_level_curves.svg" alt="Level curves of a topograhic map." width="400">
&lt;p>and weather maps (&lt;em>isobars&lt;/em>), where level curves correspond to points with the same atmospheric pressure.&lt;/p>
&lt;img src="../img/derivativesn/isobars.png" alt="Isobars of a weather map." width="400">
&lt;h3 id="partial-functions">Partial functions&lt;/h3>
&lt;div class="alert alert-def">
&lt;div>
&lt;p>&lt;strong>Definition - Partial function&lt;/strong>. Given a scalar field $f:\mathbb{R}^n\rightarrow \mathbb{R}$, an $i$-th &lt;em>partial function&lt;/em> of $f$ is any function $f_i:\mathbb{R}\rightarrow \mathbb{R}$ that results of substituting all the variables of $f$ by constants, except the $i$-th variable, that is:&lt;/p>
&lt;p>$$f_i(x)=f(c_1,\ldots,c_{i-1},x,c_{i+1},\ldots,c_{n}),$$&lt;/p>
&lt;p>with $c_j$ $(j=1,\ldots, n,\ j\neq i)$ constants.&lt;/p>
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Example&lt;/strong>. If we take the function that measures the area of a triangle&lt;/p>
&lt;p>$$f(x,y)=\frac{xy}{2},$$
and set the value of the base to $x=c$, then we the area of the triangle depends only of the height, and $f$ becomes a function of one variable, that is the partial function&lt;/p>
&lt;p>$$f_1(y)=f(c,y)=\frac{cy}{2},\quad \mbox{with $c$ constant}.$$&lt;/p>
&lt;h2 id="partial-derivative-notion">Partial derivative notion&lt;/h2>
&lt;h3 id="variation-of-a-function-with-respect-to-a-variable">Variation of a function with respect to a variable&lt;/h3>
&lt;p>We can measure the variation of a scalar field with respect to each of its variables in the same way that we measured the variation of a one-variable function.&lt;/p>
&lt;p>Let $z=f(x,y)$ be a scalar field of $\mathbb{R}^2$. If we are at point $(x_0,y_0)$ and we increase the value of $x$ a quantity $\Delta x$, then we move in the direction of the $x$-axis from the point $(x_0,y_0)$ to the point $(x_0+\Delta x,y_0)$, and the variation of the function is
$$\Delta z=f(x_0+\Delta x,y_0)-f (x_0,y_0).$$&lt;/p>
&lt;p>Thus, the rate of change of the function with respect to $x$ along the interval $[x_0,x_0+\Delta x]$ is given by the quotient&lt;/p>
&lt;p>$$\frac{\Delta z}{\Delta x}=\frac{f(x_0+\Delta x,y_0)-f(x_0,y_0)}{\Delta x}.$$&lt;/p>
&lt;h3 id="instantaneous-rate-of-change-of-a-scalar-field-with-respect-to-a-variable">Instantaneous rate of change of a scalar field with respect to a variable&lt;/h3>
&lt;p>If instead o measuring the rate of change in an interval, we measure the rate of change in a point, that is, when $\Delta x$ approaches 0, then we get the instantaneous rate of change that is the partial derivative with respect to $x$.&lt;/p>
&lt;p>$$\lim_{\Delta x\rightarrow 0}\frac{\Delta z}{\Delta x}=\lim_{\Delta x \rightarrow 0}\frac{f(x_0+\Delta x,y_0)-f(x_0,y_0)}{\Delta x}.$$&lt;/p>
&lt;p>The value of this limit, if exists, it is known as the &lt;em>partial derivative&lt;/em> of $f$ with respect to the variable $x$ at the point $(x_0,y_0)$; it is written as
$$\frac{\partial f}{\partial x}(x_0,y_0).$$&lt;/p>
&lt;p>This partial derivative measures the instantaneous rate of change of $f$ at the point $P=(x_0,y_0)$ when $P$ moves in the $x$-axis direction.&lt;/p>
&lt;h3 id="geometric-interpretation-of-partial-derivatives">Geometric interpretation of partial derivatives&lt;/h3>
&lt;p>Geometrically, a two-variable function $z=f(x,y)$ defines a surface. If we cut this surface with a plane of equation $y=y_0$ (that is, the plane where $y$ is the constant $y_0$) the intersection is a curve, and the partial derivative of $f$ with respect to to $x$ at $(x_0,y_0)$ is the slope of the tangent line to that curve at $x=x_0$.&lt;/p>
&lt;img src="../img/derivativesn/partial_tangent_surface.svg" alt="Geometric interpretation of the partial derivative." width="350">
&lt;p>&lt;img src="/media/logo-geogebra.png#left" /> &lt;strong>&lt;a href="https://ggbm.at/K3xnQRY8" target="_blank">Interactive Example&lt;/a>&lt;/strong>&lt;/p>
&lt;h3 id="partial-derivative">Partial derivative&lt;/h3>
&lt;p>The concept of partial derivative can be extended easily from two-variable function to $n$-variables functions.&lt;/p>
&lt;div class="alert alert-def">
&lt;div>
&lt;p>&lt;strong>Definition - Partial derivative&lt;/strong>. Given a $n$-variables function $f(x_1,\ldots,x_n)$, $f$ is &lt;em>partially differentiable&lt;/em> with respect to the variable $x_i$ at the point $a=(a_1,\ldots,a_n)$ if exists the limit&lt;/p>
&lt;p>$$\lim_{\Delta x_i\rightarrow 0} \frac{f(a_1,\ldots,a_{i-1},a_i+\Delta x_i,a_{i+1},\ldots,a_n)-f(a_1,\ldots,a_{i-1},a_i,a_{i+1},\ldots,a_n)} {h}.$$&lt;/p>
&lt;p>In such a case, the value of the limit is known as &lt;em>partial derivative&lt;/em> of $f$ with respect to $x_i$ at $a$; it is denoted&lt;/p>
&lt;p>$$f'_{x_i}(a)=\frac{\partial f}{\partial x_i}(a).$$&lt;/p>
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Remark&lt;/strong>. The definition of derivative for one-variable functions is a particular case of this definition for $n=1$.&lt;/p>
&lt;h3 id="partial-derivatives-computation">Partial derivatives computation&lt;/h3>
&lt;p>When we measure the variation of $f$ with respect to a variable $x_i$ at the point $a=(a_1,\ldots,a_n)$, the other variables remain constant. Thus, if we can consider the $i$-th partial function
$$f_i(x_i)=f(a_1,\ldots,a_{i-1},x_i,a_{i+1},\ldots,a_n),$$&lt;/p>
&lt;p>the partial derivative of $f$ with respect to $x_i$ can be computed differentiating this function:&lt;/p>
&lt;p>$$\frac{\partial f}{\partial x_i}(a)=f_i'(a_i).$$&lt;/p>
&lt;div class="alert alert-warning">
&lt;div>
To differentiate partially $f(x_1,\ldots,x_n)$ with respect to the variable $x_i$, you have to differentiate $f$ as a function of the variable $x_i$, considering the other variables as constants.
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Example of a perfect gas&lt;/strong>. Consider the function that measures the volume of a perfect gas
$$v(t,p)=\frac{nRt}{p},$$ where $t$ is the temperature, $p$ the pressure and $n$ and $R$ are constants.&lt;/p>
&lt;p>The instantaneous rate of change of the volume with respect to the pressure is the partial derivative of $v$ with respect to $p$. To compute this derivative we have to think in $t$ as a constant and differentiate $v$ as if the unique variable was $p$:&lt;/p>
&lt;p>$$\frac{\partial v}{\partial p}(t,p)=\frac{d}{dp}\left(\frac{nRt}{p}\right)_{\mbox{$t=$cst}}=\frac{-nRt}{p^2}.$$&lt;/p>
&lt;p>In the same way, the instantaneous rate of change of the volume with respect to the temperature is the partial derivative of $v$ with respect to $t$:&lt;/p>
&lt;p>$$\frac{\partial v}{\partial t}(t,p)=\frac{d}{dt}\left(\frac{nRt}{p}\right)_{\mbox{$p=$cst}}=\frac{nR}{p}.$$&lt;/p>
&lt;h2 id="gradient">Gradient&lt;/h2>
&lt;div class="alert alert-def">
&lt;div>
&lt;p>&lt;strong>Definition - Gradient&lt;/strong>. Given a scalar field $f(x_1,\ldots,x_n)$, the &lt;em>gradient&lt;/em> of $f$, denoted by $\nabla f$, is a function that maps every point $a=(a_1,\ldots,a_n)$ to a vector with coordinates the partial derivatives of $f$ at $a$,&lt;/p>
&lt;p>$$\nabla f(a)=\left(\frac{\partial f}{\partial x_1}(a),\ldots,\frac{\partial f}{\partial x_n}(a)\right).$$&lt;/p>
&lt;/div>
&lt;/div>
&lt;div class="alert alert-int">
&lt;div>
Later we will show that the gradient in a point is a vector with the magnitude and direction of the maximum rate of change of the function in that point. Thus, $\nabla f(a)$ points to direction of maximum increase of $f$ at $a$, while $-\nabla f(a)$ points to the direction of maximum decrease of $f$ at $a$.
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Example&lt;/strong>. After heating a surface, the temperature $t$ (in $^\circ$C) at each point $(x,y,z)$ (in m) of the surface is given by the function&lt;/p>
&lt;p>$$t(x,y,z)=\frac{x}{y}+z^2.$$&lt;/p>
&lt;p>In what direction will increase the temperature faster at point $(2,1,1)$ of the surface? What magnitude will the maximum increase of temperature have?&lt;/p>
&lt;p>The direction of maximum increase of the temperature is given by the gradient&lt;/p>
&lt;p>$$\nabla t(x,y,z)=\left(\frac{\partial t}{\partial x}(x,y,z),\frac{\partial t}{\partial y}(x,y,z),\frac{\partial t}{\partial z}(x,y,z)\right)=\left(\frac{1}{y},\frac{-x}{y^2},2z\right).$$&lt;/p>
&lt;p>At point $(2,1,1)$ de direction is given by the vector&lt;/p>
&lt;p>$$\nabla t(2,1,1)=\left(\frac{1}{1},\frac{-2}{1^2},2\cdot 1\right)=(1,-2,2),$$&lt;/p>
&lt;p>and its magnitude is&lt;/p>
&lt;p>$$|\nabla f(2,1,1)|=|\sqrt{1^2+(-2)^2+2^2}|=|\sqrt{9}|=3 \mbox{ $^\circ$C/m}.$$&lt;/p>
&lt;h2 id="composition-of-a-vectorial-field-with-a-scalar-field">Composition of a vectorial field with a scalar field&lt;/h2>
&lt;h3 id="multivariate-chain-rule">Multivariate chain rule&lt;/h3>
&lt;p>If $f:\mathbb{R}^n\rightarrow \mathbb{R}$ is a scalar field and $g:\mathbb{R}\rightarrow \mathbb{R}^n$ is a vectorial function, then it is possible to compound $g$ with $f$, so that $f\circ g:\mathbb{R}\rightarrow \mathbb{R}$ is a one-variable function.&lt;/p>
&lt;div class="alert alert-def">
&lt;div>
&lt;p>&lt;strong>Theorem - Chain rule&lt;/strong>. If $g(t)=(x_1(t),\ldots,x_n(t))$ is a vectorial function differentiable at $t$ and $f(x_1,\ldots,x_n)$ is a scalar field differentiable at the point $g(t)$, then $f\circ g(t)$ is differentiable at $t$ and&lt;/p>
&lt;p>$$(f\circ g)'(t) = \nabla f(g(t))\cdot g'(t)=\frac{\partial f}{\partial x_1}\frac{dx_1}{dt}+ \cdots + \frac{\partial f}{\partial x_n}\frac{dx_n}{dt}$$&lt;/p>
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Example&lt;/strong>. Let us consider the scalar field $f(x,y)=x^2y$ and the vectorial function $g(t)=(\cos t,\sin t)$ $t\in [0,2\pi]$ in the real plane, then&lt;/p>
&lt;p>$$\nabla f(x,y) = (2xy, x^2) \quad \mbox{and} \quad g'(t) = (-\sin t, \cos t),$$&lt;/p>
&lt;p>and&lt;/p>
&lt;p>$$
\begin{aligned}
(f\circ g)'(t) &amp;amp;= \nabla f(g(t))\cdot g'(t) = (2\cos t\sin t,\cos^2 t)\cdot (-\sin t,\cos t) =\newline
&amp;amp;= -2\cos t\sin^2 t+\cos^3 t.
\end{aligned}
$$&lt;/p>
&lt;p>We can get the same result differentiating the composed function directly&lt;/p>
&lt;p>$$(f\circ g)(t) = f(g(t)) = f(\cos t, \sin t) = \cos^2 t\sin t,$$&lt;/p>
&lt;p>and its derivative is&lt;/p>
&lt;p>$$(f\circ g)'(t) = 2\cos t(-\sin t)\sin t+\cos^2 t \cos t = -2\cos t\sin^2 t+\cos^3 t.$$&lt;/p>
&lt;p>The chain rule for the composition of a vectorial function with a scalar field allow us to get the algebra of derivatives for one-variable functions easily:&lt;/p>
&lt;p>$$
\begin{aligned}
(u+v)' &amp;amp;= u'+v'\newline
(uv)' &amp;amp;= u&amp;rsquo;v+uv'\newline
\left(\frac{u}{v}\right)' &amp;amp;= \frac{u&amp;rsquo;v-uv'}{v^2}\newline
(u\circ v)' &amp;amp;= u'(v)v'
\end{aligned}
$$&lt;/p>
&lt;p>To infer the derivative of the sum of two functions $u$ and $v$, we can take the scalar field $f(x,y)=x+y$ and the vectorial function $g(t)=(u(t),v(t))$. Applying the chain rule we get&lt;/p>
&lt;p>$$(u+v)'(t) = (f\circ g)'(t) = \nabla f(g(t))\cdot g'(t) = (1,1)\cdot (u',v') = u'+v'.$$&lt;/p>
&lt;p>To infer the derivative of the quotient of two functions $u$ and $v$, we can take the scalar field $f(x,y)=x/y$ and the vectorial function $g(t)=(u(t),v(t))$.&lt;/p>
&lt;p>$$\left(\frac{u}{v}\right)'(t) = (f\circ g)'(t) = \nabla f(g(t))\cdot g'(t) = \left(\frac{1}{v},-\frac{u}{v^2}\right)\cdot (u',v') = \frac{u&amp;rsquo;v-uv'}{v^2}.$$&lt;/p>
&lt;h2 id="tangent-plane-and-normal-line-to-a-surface">Tangent plane and normal line to a surface&lt;/h2>
&lt;div class="alert alert-def">
&lt;div>
&lt;p>Let $C$ be the level set of a scalar field $f$ that includes a point $P$. If $\mathbf{v}$ is the velocity at $P$ of a trajectory following $C$, then&lt;/p>
&lt;p>$$\nabla f(P) \cdot \mathbf{v} = 0.$$&lt;/p>
&lt;/div>
&lt;/div>
&lt;div class="spoiler " >
&lt;p>
&lt;a class="btn btn-primary" data-toggle="collapse" href="#spoiler-17" role="button" aria-expanded="false" aria-controls="spoiler-17">
Proof
&lt;/a>
&lt;/p>
&lt;div class="collapse card " id="spoiler-17">
&lt;div class="card-body">
&lt;p>If we take the trajectory $g(t)$ that follows the level set $C$ and passes through $P$ at time $t=t_0$, that is $P=g(t_0)$, so $\mathbf{v}=g'(t_0)$, then&lt;/p>
&lt;p>$$(f\circ g)(t) = f(g(t)) = f(P),$$&lt;/p>
&lt;p>that is constant at any $t$. Thus, applying the chain rule we have&lt;/p>
&lt;p>$$(f\circ g)'(t) = \nabla f(g(t))\cdot g'(t) = 0,$$&lt;/p>
&lt;p>and, particularly, at $t=t_0$, we have&lt;/p>
&lt;p>$$\nabla f(P)\cdot \mathbf{v} = 0.$$&lt;/p>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;p>That means that the gradient of $f$ at $P$ is normal to $C$ at $P$, provided that the gradient is not zero.&lt;/p>
&lt;h3 id="normal-and-tangent-line-to-curve-in-the-plane">Normal and tangent line to curve in the plane&lt;/h3>
&lt;div class="alert alert-tool">
&lt;div>
&lt;p>&lt;strong>Normal line to a curve in the plane&lt;/strong>. According to the previous result, the normal line to a curve with equation $f(x,y)=0$ at point $P=(x_0,y_0)$, has equation&lt;/p>
&lt;p>$$P+t\nabla f(P) = (x_0,y_0)+t\nabla f(x_0,y_0).$$&lt;/p>
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Example&lt;/strong>. Given the scalar field $f(x,y)=x^2+y^2-25$, and the point $P=(3,4)$, the level set of $f$ that passes through $P$, that satisfies $f(x,y)=f(P)=0$, is the circle with radius 5 centred at the origin of coordinates. Thus, taking as a normal vector the gradient of $f$&lt;/p>
&lt;p>$$\nabla f(x,y) = (2x,2y),$$&lt;/p>
&lt;p>at the point $P=(3,4)$ is $\nabla f(3,4) = (6,8)$, and the normal line to the circle at $P$ is&lt;/p>
&lt;p>$$P+t\nabla f(P) = (3,4)+t(6,8) = (3+6t,4+8t),$$&lt;/p>
&lt;p>On the other hand, the tangent line to the circle at $P$ is&lt;/p>
&lt;p>$$((x,y)-P)\cdot \nabla f(P) = ((x,y)-(3,4))\cdot (6,8) = (x-3,y-4)\cdot(6,8) = 6x+8y=50.$$&lt;/p>
&lt;h3 id="normal-line-and-tangent-plane-to-a-surface-in-the-space">Normal line and tangent plane to a surface in the space&lt;/h3>
&lt;div class="alert alert-tool">
&lt;div>
&lt;p>&lt;strong>Normal line to a surface in the space&lt;/strong>. if we have a surface with equation $f(x,y,z)=0$, at the point $P=(x_0,y_0,z_0)$ the normal line has equation&lt;/p>
&lt;p>$$P+t\nabla f(P) = (x_0,y_0,z_0)+t\nabla f(x_0,y_0,z_0).$$&lt;/p>
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Example&lt;/strong>. Given the scalar field $f(x,y,z)=x^2+y^2-z$, and the point $P=(1,1,2)$, the level set of $f$ that passes through $P$, that satisfies $f(x,y)=f(P)=0$, is the paraboloid $z=x^2+y^2$. Thus, taking as a normal vector the gradient of $f$&lt;/p>
&lt;p>$$\nabla f(x,y,z) = (2x,2y,-1),$$&lt;/p>
&lt;p>at the point $P=(1,1,2)$ is $\nabla f(1,1,2) = (2,2,-1)$, and the normal line to the paraboloid at $P$ is&lt;/p>
&lt;p>$$
\begin{aligned}
P+t\nabla f(P)&amp;amp;= (1,1,2)+t\nabla f(1,1,2) = (1,1,2)+t(2,2,-1)\newline
&amp;amp;= (1+2t,1+2t,2-t).
\end{aligned}
$$&lt;/p>
&lt;p>On the other hand, the tangent plane to the paraboloid at $P$ is&lt;/p>
&lt;p>$$\begin{aligned}
((x,y,z)-P)\cdot \nabla f(P) &amp;amp;= ((x,y,z)-(1,1,2))(2,2,-1) = (x-1,y-1,z-2)(2,2,-1)=\newline
&amp;amp;= 2(x-1)+2(y-1)-(z-2) = 2x+2y-z-2= 0.
\end{aligned}$$&lt;/p>
&lt;p>The graph of the paraboloid $f(x,y,z)=x^2+y^2-z=0$ and the normal line and the tangent plane to the graph of $f$ at the point $P=(1,1,2)$ are below.&lt;/p>
&lt;img src="../img/derivativesn/tangent_plane_paraboloid.svg" alt="Geometric interpretation of the partial derivative." width="350">
&lt;p>&lt;img src="/media/logo-geogebra.png#left" /> &lt;strong>&lt;a href="https://ggbm.at/wTh7KKd3" target="_blank">Interactive Example&lt;/a>&lt;/strong>&lt;/p>
&lt;h2 id="directional-derivative">Directional derivative&lt;/h2>
&lt;p>For a scalar field $f(x,y)$, we have seen that the partial derivative $\dfrac{\partial f}{\partial x}(x_0,y_0)$ is the instantaneous rate of change of $f$ with respect to $x$ at point $P=(x_0,y_0)$, that is, when we move along the $x$-axis.&lt;/p>
&lt;p>In the same way, $\dfrac{\partial f}{\partial y}(x_0,y_0)$ is the instantaneous rate of change of $f$ with respect to $y$ at the point $P=(x_0,y_0)$, that is, when we move along the $y$-axis.&lt;/p>
&lt;p>But, &lt;em>what happens if we move along any other direction?&lt;/em>&lt;/p>
&lt;p>The instantaneous rate of change of $f$ at the point $P=(x_0,y_0)$ along the direction of a unitary vector $u$ is known as &lt;em>directional derivative&lt;/em>.&lt;/p>
&lt;div class="alert alert-def">
&lt;div>
&lt;p>&lt;strong>Definition - Directional derivative&lt;/strong>. Given a scalar field $f$ of $\mathbb{R}^n$, a point $P$ and a unitary vector $\mathbf{u}$ in that space, we say that $f$ is differentiable at $P$ along the direction of $\mathbf{u}$ if exists the limit&lt;/p>
&lt;p>$$f^\prime_{\mathbf{u}}(P) = \lim_{h\rightarrow 0}\frac{f(P+h\mathbf{u})-f(P)}{h}.$$&lt;/p>
&lt;p>In such a case, the value of the limit is known as &lt;em>directional derivative&lt;/em> of $f$ at the point $P$ along the direction of $\mathbf{u}$.&lt;/p>
&lt;/div>
&lt;/div>
&lt;div class="alert alert-theo">
&lt;div>
&lt;p>&lt;strong>Theorem - Directional derivative&lt;/strong> . Given a scalar field $f$ of $\mathbb{R}^n$, a point $P$ and a unitary vector $\mathbf{u}$ in that space, the directional derivative of $f$ at the point $P$ along the direction of $\mathbf{u}$ can be computed as the dot product of the gradient of $f$ at $P$ and the unitary vector $\mathbf{u}$:&lt;/p>
&lt;p>$$f^\prime_{\mathbf{u}}(P) = \nabla f(P)\cdot \mathbf{u}.$$&lt;/p>
&lt;/div>
&lt;/div>
&lt;div class="spoiler " >
&lt;p>
&lt;a class="btn btn-primary" data-toggle="collapse" href="#spoiler-22" role="button" aria-expanded="false" aria-controls="spoiler-22">
Proof
&lt;/a>
&lt;/p>
&lt;div class="collapse card " id="spoiler-22">
&lt;div class="card-body">
&lt;p>If we consider a unitary vector $\mathbf{u}$, the trajectory that passes through $P$, following the direction of $\mathbf{u}$, has equation&lt;/p>
&lt;p>$$g(t)=P+t\mathbf{u},\ t\in\mathbb{R}.$$&lt;/p>
&lt;p>For $t=0$, this trajectory passes through the point $P=g(0)$ with velocity $\mathbf{u}=g'(0)$.&lt;/p>
&lt;p>Thus, the directional derivative of $f$ at the point $P$ along the direction of $\mathbf{u}$ is&lt;/p>
&lt;p>$$(f\circ g)'(0) = \nabla f(g(0))\cdot g'(0) = \nabla f(P)\cdot \mathbf{u}.$$&lt;/p>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;div class="alert alert-warning">
&lt;div>
The partial derivatives are the directional derivatives along the vectors of the canonical basis.
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Example&lt;/strong>. Given the function $f(x,y) = x^2+y^2$, its gradient is&lt;/p>
&lt;p>$$\nabla f(x,y) = (2x,2y).$$&lt;/p>
&lt;p>The directional derivative of $f$ at the point $P=(1,1)$, along the unit vector $\mathbf{u}=(1/\sqrt{2},1/\sqrt{2})$ is&lt;/p>
&lt;p>$$f_{\mathbf{u}}'(P) = \nabla f(P)\cdot \mathbf{u} = (2,2)\cdot(1/\sqrt{2},1/\sqrt{2}) = \frac{2}{\sqrt{2}}+\frac{2}{\sqrt{2}} = \frac{4}{\sqrt{2}}.$$&lt;/p>
&lt;p>To compute the directional derivative along a non-unitary vector $\mathbf{v}$, we have to use the unitary vector that results from normalizing $v$ with the transformation&lt;/p>
&lt;p>$$\mathbf{v'}=\frac{\mathbf{v}}{|\mathbf{v}|}.$$&lt;/p>
&lt;h3 id="geometric-interpretation-of-the-directional-derivative">Geometric interpretation of the directional derivative&lt;/h3>
&lt;p>Geometrically, a two-variable function $z=f(x,y)$ defines a surface. If we cut this surface with a plane of equation $a(y-y_0)=b(x-x_0)$ (that is, the vertical plane that passes through the point $P=(x_0,y_0)$ with the direction of vector $\mathbf{u}=(a,b)$) the intersection is a curve, and the directional derivative of $f$ at $P$ along the direction of $\mathbf{u}$ is the slope of the tangent line to that curve at point $P$.&lt;/p>
&lt;p>&lt;img src="/media/logo-geogebra.png#left" /> &lt;strong>&lt;a href="https://ggbm.at/Bx8nFMNc" target="_blank">Interactive Example&lt;/a>&lt;/strong>&lt;/p>
&lt;h3 id="growth-of-scalar-field-along-the-gradient">Growth of scalar field along the gradient&lt;/h3>
&lt;p>We have seen that for any vector $\mathbf{u}$&lt;/p>
&lt;p>$$f^\prime_{\mathbf{u}}(P) = \nabla f(P)\cdot \mathbf{u} = |\nabla f(P)|\cos \theta,$$&lt;/p>
&lt;p>where $\theta$ is the angle between $\mathbf{u}$ and the gradient $\nabla f(P)$.&lt;/p>
&lt;p>Taking into account that $-1\leq \cos\theta\leq 1$, for any vector $\mathbf{u}$ it is satisfied that&lt;/p>
&lt;p>$$-|\nabla f(P)|\leq f'_{\mathbf{u}}(P)\leq |\nabla f(P)| .$$&lt;/p>
&lt;p>Furthermore, if $\mathbf{u}$ has the same direction and sense than the gradient, we have $f'_{\mathbf{u}}(P)=\vert\nabla f(P)\vert\cos 0=\vert\nabla f(P)\vert$.
Therefore, &lt;em>the maximum increase of a scalar field at a point $P$ is along the direction of the gradient at that point&lt;/em>.&lt;/p>
&lt;p>In the same manner, if $\mathbf{u}$ has the same direction but opposite sense than the gradient, we have $f_{\mathbf{u}}'(P)=\vert\nabla f(P)\vert\cos \pi=-\vert\nabla f(P)\vert$.
Therefore, &lt;em>the maximum decrease of a scalar field at a point $P$ is along the opposite direction of the gradient at that point&lt;/em>.&lt;/p>
&lt;h2 id="implicit-derivation">Implicit derivation&lt;/h2>
&lt;p>When we have a relation $f(x,y)=0$, sometimes we can consider $y$ as an &lt;em>implicit function&lt;/em> of $x$, at least in a neighbourhood of a point $(x_0,y_0)$.&lt;/p>
&lt;p>The equation $x^2+y^2=25$, whose graph is the circle of radius 5 centred at the origin of coordinates, its not a function, because if we solve the equation for $y$, we have two images for some values of $x$,&lt;/p>
&lt;p>$$y=\pm \sqrt{25-x^2}$$&lt;/p>
&lt;p>However, near the point $(3,4)$ we can represent the relation as the function $y=\sqrt{25-x^2}$, and near the point $(3,-4)$ we can represent the relation as the function $y=-\sqrt{25-x^2}$.&lt;/p>
&lt;p>If an equation $f(x,y)=0$ defines $y$ as a implicit function of $x$, $y=h(x)$, in a neighbourhood of $(x_0,y_0)$, then we can compute de derivative of $y$, $h'(x)$, even if we do not know the explicit formula for $h$.&lt;/p>
&lt;div class="alert alert-theo">
&lt;div>
&lt;p>&lt;strong>Theorem - Implicit derivation&lt;/strong>. Let $f(x,y):\mathbb{R}^2\longrightarrow \mathbb{R}$ a two-variable function and let $(x_0,y_0)$ be a point in $\mathbb{R}^2$ such that $f(x_0,y_0)=0$. If $f$ has partial derivatives continuous at $(x_0,y_0)$ and $\frac{\partial f}{\partial y}(x_0,y_0)\neq 0$, then there is an open interval $I\subset \mathbb{R}$ with $x_0\in I$ and a function $h(x): I\longrightarrow \mathbb{R}$ such that&lt;/p>
&lt;ol>
&lt;li>$y_0=h(x_0)$.&lt;/li>
&lt;li>$f(x,h(x))=0$ for all $x\in I$.&lt;/li>
&lt;li>$h$ is differentiable on $I$, and $y'=h'(x)=\frac{-\dfrac{\partial f}{\partial x}}{\dfrac{\partial f}{\partial y}}$&lt;/li>
&lt;/ol>
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Proof&lt;/strong>. To prove the last result, take the trajectory $g(x)=(x,h(x))$ on the interval $I$. Then&lt;/p>
&lt;p>$$(f\circ g)(x) = f(g(x)) = f(x,h(x))=0.$$&lt;/p>
&lt;p>Thus, using the chain rule we have&lt;/p>
&lt;p>$$
\begin{aligned}
(f\circ g)'(x) &amp;amp;= \nabla f(g(x))\cdot g'(x) = \left(\frac{\partial f}{\partial x}, \frac{\partial f}{\partial y}\right)\cdot (1,h'(x)) = \newline
&amp;amp;= \frac{\partial f}{\partial x}+\frac{\partial f}{\partial y}h'(x) = 0,
\end{aligned}
$$&lt;/p>
&lt;p>from where we can deduce&lt;/p>
&lt;p>$$y'=h'(x)=\frac{-\dfrac{\partial f}{\partial x}}{\dfrac{\partial f}{\partial y}}.$$&lt;/p>
&lt;p>This technique that allows us to compute $y'$ in a neighbourhood of $x_0$ without the explicit formula of $y=h(x)$, it is known as &lt;em>implicit derivation&lt;/em>.&lt;/p>
&lt;p>&lt;strong>Example&lt;/strong>. Consider the equation of the circle of radius 5 centred at the origin $x^2+y^2=25$. It can also be written as&lt;/p>
&lt;p>$$f(x,y) = x^2+y^2-25 = 0.$$
Take the point $(3,4)$ that satisfies the equation, $f(3,4)=0$.&lt;/p>
&lt;p>As $f$ have partial derivatives $\frac{\partial f}{\partial x}=2x$ and $\frac{\partial f}{\partial y}=2y$, that are continuous at $(3,4)$, and $\frac{\partial f}{\partial y}(3,4)=8\neq 0$, then $y$ can be expressed as a function of $x$ in a neighbourhood of $(3,4)$ and its derivative is&lt;/p>
&lt;p>$$y'=\frac{-\frac{\partial f}{\partial x}}{\frac{\partial f}{\partial y}} = \frac{-2x}{2y}=\frac{-x}{y} \quad \mbox{and} \quad y'(3)=\frac{-3}{4}.$$&lt;/p>
&lt;p>In this particular case, that we know the explicit formula of $y=\sqrt{1-x^2}$, we can get the same result computing the derivative as usual&lt;/p>
&lt;p>$$y' = \frac{1}{2\sqrt{1-x^2}}(-2x) = \frac{-x}{\sqrt{1-x^2}}.$$&lt;/p>
&lt;p>The implicit function theorem can be generalized to functions with several variables.&lt;/p>
&lt;div class="alert alert-theo">
&lt;div>
&lt;p>&lt;strong>Theorem - Implicit derivation&lt;/strong>. Let $f(x_1,\ldots,x_n,y):\mathbb{R}^{n+1}\longrightarrow \mathbb{R}$ a $n+1$-variables function and let $(a_1,\ldots, a_n,b)$ be a point in $\mathbb{R}^{n+1}$ such that $f(a_1,\ldots,a_n,b)=0$. If $f$ has partial derivatives continuous at $(a_1,\ldots,a_n,b)$ and $\frac{\partial f}{\partial y}(a_1,\ldots,a_n,b)\neq 0$, then there is a region $I\subset \mathbb{R}^n$ with $(x_1,\ldots,x_n)\in I$ and a function $h(x_1,\ldots, x_n): I\longrightarrow \mathbb{R}$ such that&lt;/p>
&lt;ol>
&lt;li>$b=h(a_1,\ldots,a_n)$.&lt;/li>
&lt;li>$f(x_1,\ldots,x_n,h(x_1,\ldots,x_n))=0$ for all $(x_1,\ldots,x_n)\in I$.&lt;/li>
&lt;li>$h$ is differentiable on $I$, and $\dfrac{\partial y}{\partial x_i}=\frac{-\dfrac{\partial f}{\partial x_i}}{\dfrac{\partial f}{\partial y}}$&lt;/li>
&lt;/ol>
&lt;/div>
&lt;/div>
&lt;h2 id="second-order-partial-derivatives">Second order partial derivatives&lt;/h2>
&lt;p>As the partial derivatives of a function are also functions of several variables we can differentiate partially each of them.&lt;/p>
&lt;p>If a function $f(x_1,\ldots,x_n)$ has a partial derivative $f^\prime_{x_i}(x_1,\ldots,x_n)$ with respect to the variable $x_i$ in a set $A$, then we can differentiate partially again $f_{x_i}^\prime$ with respect to the variable $x_j$. This second derivative, when exists, is known as &lt;em>second order partial derivative&lt;/em> of $f$ with respect to the variables $x_i$ and $x_j$; it is written as&lt;/p>
&lt;p>$$\frac{\partial ^2 f}{\partial x_j \partial x_i}= \frac{\partial}{\partial x_j}\left(\frac{\partial f}{\partial x_i}\right).$$&lt;/p>
&lt;p>In the same way we can define higher order partial derivatives.&lt;/p>
&lt;p>&lt;strong>Example&lt;/strong>. The two-variables function
$$f(x,y)=x^y$$
has 4 second order partial derivatives:&lt;/p>
&lt;p>$$
\begin{aligned}
\frac{\partial^2 f}{\partial x^2}(x,y) &amp;amp;=
\frac{\partial}{\partial x}\left(\frac{\partial f}{\partial x}(x,y)\right) =
\frac{\partial}{\partial x}\left(yx^{y-1}\right) =
y(y-1)x^{y-2},\newline
\frac{\partial^2 f}{\partial y \partial x}(x,y) &amp;amp;=
\frac{\partial}{\partial y}\left(\frac{\partial f}{\partial x}(x,y)\right) =
\frac{\partial}{\partial y}\left(yx^{y-1}\right) =
x^{y-1}+yx^{y-1}\log x,\newline
\frac{\partial^2 f}{\partial x \partial y}(x,y) &amp;amp;=
\frac{\partial}{\partial x}\left(\frac{\partial f}{\partial y}(x,y)\right) =
\frac{\partial}{\partial x}\left(x^y\log x \right) =
yx^{y-1}\log x+x^y\frac{1}{x},\newline
\frac{\partial^2 f}{\partial y^2}(x,y) &amp;amp;=
\frac{\partial}{\partial y}\left(\frac{\partial f}{\partial y}(x,y)\right) =
\frac{\partial}{\partial y}\left(x^y\log x \right) =
x^y(\log x)^2.
\end{aligned}
$$&lt;/p>
&lt;h2 id="hessian-matrix-and-hessian">Hessian matrix and Hessian&lt;/h2>
&lt;div class="alert alert-def">
&lt;div>
&lt;p>&lt;strong>Definition - Hessian matrix&lt;/strong>. Given a scalar field $f(x_1,\ldots,x_n)$, with second order partial derivatives at the point $a=(a_1,\ldots,a_n)$, the &lt;em>Hessian matrix&lt;/em> of $f$ at $a$, denoted by $\nabla^2f(a)$, is the matrix&lt;/p>
&lt;p>$$
\nabla^2f(a)=\left(
\begin{array}{cccc}
\dfrac{\partial^2 f}{\partial x_1^2}(a) &amp;amp;
\dfrac{\partial^2 f}{\partial x_1 \partial x_2}(a) &amp;amp;
\cdots &amp;amp;
\dfrac{\partial^2 f}{\partial x_1 \partial x_n}(a)\newline
\dfrac{\partial^2 f}{\partial x_2 \partial x_1}(a) &amp;amp;
\dfrac{\partial^2 f}{\partial x_2^2}(a) &amp;amp;
\cdots &amp;amp;
\dfrac{\partial^2 f}{\partial x_2 \partial x_n}(a)\newline
\vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \newline
\dfrac{\partial^2 f}{\partial x_n \partial x_1}(a) &amp;amp;
\dfrac{\partial^2 f}{\partial x_n \partial x_2}(a) &amp;amp;
\cdots &amp;amp;
\dfrac{\partial^2 f}{\partial x_n^2}(a)
\end{array}
\right)
$$&lt;/p>
&lt;p>The determinant of this matrix is known as &lt;em>Hessian&lt;/em> of $f$ at $a$; it is denoted $Hf(a)=\vert\nabla^2f(a)\vert$.&lt;/p>
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Example&lt;/strong>. Consider again the two-variables function&lt;/p>
&lt;p>$$f(x,y)=x^y.$$&lt;/p>
&lt;p>Its Hessian matrix is&lt;/p>
&lt;p>$$
\nabla^2f(x,y) =
\left(
\begin{array}{cc}
\dfrac{\partial^2 f}{\partial x^2} &amp;amp; \dfrac{\partial^2 f}{\partial x \partial y}\newline
\dfrac{\partial^2 f}{\partial y \partial x} &amp;amp; \dfrac{\partial^2 f}{\partial y^2}
\end{array}
\right) =
\left(\begin{array}{cc}
y(y-1)x^{y-2} &amp;amp; x^{y-1}(y\log x+1) \newline
x^{y-1}(y\log x+1) &amp;amp; x^y(\log x)^2
\end{array}
\right).
$$&lt;/p>
&lt;p>At point $(1,2)$ is&lt;/p>
&lt;p>$$
\nabla^2 f(1,2) =
\left(
\begin{array}{cc}
2(2-1)1^{2-2} &amp;amp; 1^{2-1}(2\log 1+1) \newline
1^{2-1}(2\log 1+1) &amp;amp; 1^2(\log 1)^2
\end{array}
\right) =
\left(
\begin{array}{cc}
2 &amp;amp; 1 \newline
1 &amp;amp; 0
\end{array}
\right).
$$&lt;/p>
&lt;p>And its Hessian is&lt;/p>
&lt;p>$$
Hf(1,2)=\left|
\begin{array}{cc}
2 &amp;amp; 1 \newline
1 &amp;amp; 0
\end{array}
\right|=
2\cdot 0-1\cdot1= -1.
$$&lt;/p>
&lt;h3 id="symmetry-of-second-partial-derivatives">Symmetry of second partial derivatives&lt;/h3>
&lt;p>In the previous example we can observe that the &lt;em>mixed derivatives&lt;/em> of second order $\frac{\partial^2 f}{\partial y\partial x}$ and $\frac{\partial^2 f}{\partial x\partial y}$ are the same. This fact is due to the following result.&lt;/p>
&lt;div class="alert alert-theo">
&lt;div>
&lt;p>&lt;strong>Theorem - Symmetry of second partial derivatives&lt;/strong>. If $f(x_1,\ldots,x_n)$ is a scalar field with second order partial derivatives $\frac{\partial^2 f}{\partial x_i\partial x_j}$ and $\frac{\partial^2 f}{\partial x_j\partial x_i}$ continuous at a point $(a_1,\ldots,a_n)$, then&lt;/p>
&lt;p>$$\frac{\partial^2 f}{\partial x_i\partial x_j}(a_1,\ldots,a_n)=\frac{\partial^2 f}{\partial x_j\partial x_i}(a_1,\ldots,a_n).$$&lt;/p>
&lt;/div>
&lt;/div>
&lt;p>This means that when computing a second partial derivative.&lt;/p>
&lt;p>As a consequence, if the function satisfies the requirements of the theorem for all the second order partial derivatives, the Hessian matrix is symmetric.&lt;/p>
&lt;h2 id="taylor-polynomials">Taylor polynomials&lt;/h2>
&lt;h3 id="linear-approximation-of-a-scalar-field">Linear approximation of a scalar field&lt;/h3>
&lt;p>In a previous chapter we saw how to approximate a one-variable function with a Taylor polynomial. This can be generalized to several-variables functions.&lt;/p>
&lt;p>If $P$ is a point in the domain of a scalar field $f$ and $\mathbf{v}$ is a vector, the first degree &lt;em>Taylor formula&lt;/em> of $f$ around $P$ is&lt;/p>
&lt;p>$$f(P+\mathbf{v}) = f(P) + \nabla f(P)\cdot \mathbf{v} +R^1_{f,P}(\mathbf{v}),$$&lt;/p>
&lt;p>where&lt;/p>
&lt;p>$$P^1_{f,P}(\mathbf{v}) = f(P)+\nabla f(P)\mathbf{v}$$&lt;/p>
&lt;p>is the first degree &lt;em>Taylor polynomial&lt;/em> of $f$ at $P$, and $R^1_{f,P}(\mathbf{v})$ is the &lt;em>Taylor remainder&lt;/em> for the vector $\mathbf{v}$, that is the error in the approximation.&lt;/p>
&lt;p>The remainder satisfies&lt;/p>
&lt;p>$$\lim_{|\mathbf{v}|\rightarrow 0} \frac{R^1_{f,P}(\mathbf{v})}{|\mathbf{v}|} = 0$$&lt;/p>
&lt;div class="alert alert-warning">
&lt;div>
The first degree Taylor polynomial for a function of two variables is the tangent plane to the graph of $f$ at $P$.
&lt;/div>
&lt;/div>
&lt;h3 id="linear-approximation-of-a-two-variable-function">Linear approximation of a two-variable function&lt;/h3>
&lt;p>If $f$ is a scalar field of two variables $f(x,y)$ and $P=(x_0,y_0)$, as for any point $Q=(x,y)$ we can take the vector $\mathbf{v}=\vec{PQ}=(x-x_0,y-y_0)$, then the first degree Taylor polynomial of $f$ at $P$, can be written as&lt;/p>
&lt;p>$$
\begin{aligned}
P^1_{f,P}(x,y) &amp;amp;= f(x_0,y_0)+\nabla f(x_0,y_0)(x-x_0,y-y_0) =\newline
&amp;amp;= f(x_0,y_0)+\frac{\partial f}{\partial x}(x_0,y_0)(x-x_0)+\frac{\partial f}{\partial y}(x_0,y_0)(y-y_0).
\end{aligned}
$$&lt;/p>
&lt;p>&lt;strong>Example&lt;/strong>. Given the scalar field $f(x,y)=\log(xy)$, its gradient is&lt;/p>
&lt;p>$$\nabla f(x,y) = \left(\frac{1}{x},\frac{1}{y}\right),$$&lt;/p>
&lt;p>and the first degree Taylor polynomial at the point $P=(1,1)$ is&lt;/p>
&lt;p>$$\begin{aligned}
P^1_{f,P}(x,y) &amp;amp;= f(1,1) +\nabla f(1,1)\cdot (x-1,y-1) = \newline
&amp;amp;= \log 1+(1,1)\cdot(x-1,y-1) = x-1+y-1 = x+y-2.
\end{aligned}$$&lt;/p>
&lt;p>This polynomial approximates $f$ near the point $P$. For instance,&lt;/p>
&lt;p>$$f(1.01,1.01) \approx P^1_{f,P}(1.01,1.01) = 1.01+1.01-2 = 0.02.$$&lt;/p>
&lt;p>The graph of the scalar field $f(x,y)=\log(xy)$ and the first degree Taylor polynomial of $f$ at the point $P=(1,1)$ is below.&lt;/p>
&lt;img src="../img/derivativesn/first_degree_taylor_polynomial.svg" alt="First degree Taylor polynomial" width="350">
&lt;h3 id="quadratic-approximation-of-a-scalar-field">Quadratic approximation of a scalar field&lt;/h3>
&lt;p>If $P$ is a point in the domain of a scalar field $f$ and $\mathbf{v}$ is a vector, the second degree &lt;em>Taylor formula&lt;/em> of $f$ around $P$ is&lt;/p>
&lt;p>$$f(P+\mathbf{v}) = f(P) + \nabla f(P)\cdot \mathbf{v} + \frac{1}{2}\left(\mathbf{v}\nabla^2f(P)\mathbf{v}\right) + R^2_{f,P}(\mathbf{v}),$$&lt;/p>
&lt;p>where&lt;/p>
&lt;p>$$P^2_{f,P}(\mathbf{v})f(P)+\nabla f(P)\mathbf{v}+\frac{1}{2}\left(\mathbf{v}\nabla^2f(P)\mathbf{v}\right)$$&lt;/p>
&lt;p>is the second degree &lt;em>Taylor polynomial&lt;/em> of $f$ at the point $P$, and $R^2_{f,P}(\mathbf{v})$ is the &lt;em>Taylor remainder&lt;/em> for the vector $\mathbf{v}$, that is the error in the approximation.&lt;/p>
&lt;p>The remainder satisfies&lt;/p>
&lt;p>$$\lim_{|\mathbf{v}\rightarrow 0|} \frac{R^2_{f,P}(\mathbf{v})}{|\mathbf{v}|^2} = 0.$$&lt;/p>
&lt;p>This means that the remainder is smaller than the square of the module of $\mathbf{v}$.&lt;/p>
&lt;h3 id="quadratic-approximation-of-a-two-variable-function">Quadratic approximation of a two-variable function&lt;/h3>
&lt;p>If $f$ is a scalar field of two variables $f(x,y)$ and $P=(x_0,y_0)$, then the second degree Taylor polynomial of $f$ at $P$, can be written as&lt;/p>
&lt;p>$$
\begin{aligned}
P^2_{f,P}(x,y) &amp;amp;= f(x_0,y_0)+\nabla f(x_0,y_0)(x-x_0,y-y_0) + \newline &amp;amp; +
\frac{1}{2}(x-x_0,y-y_0)\nabla^2f(x_0,y_0)(x-x_0,y-y_0)= \newline
&amp;amp; = f(x_0,y_0)+\frac{\partial f}{\partial x}(x_0,y_0)(x-x_0)+\frac{\partial f}{\partial y}(x_0,y_0)(y-y_0)+ \newline &amp;amp; +
\frac{1}{2}(\frac{\partial^2 f}{\partial x^2}(x_0,y_0) (x-x_0)^2 + 2\frac{\partial^2 f}{\partial y\partial x}(x_0,y_0)(x-x_0)(y-y_0) + \newline &amp;amp; + \frac{\partial^2 f}{\partial y^2}(x_0,y_0)(y-y_0^2))
\end{aligned}
$$&lt;/p>
&lt;p>&lt;strong>Example&lt;/strong>. Given the scalar field $f(x,y)=\log(xy)$, its gradient is&lt;/p>
&lt;p>$$\nabla f(x,y) = \left(\frac{1}{x},\frac{1}{y}\right),$$&lt;/p>
&lt;p>its Hessian matrix is&lt;/p>
&lt;p>$$Hf(x,y) = \left(
\begin{array}{cc}
\frac{-1}{x^2} &amp;amp; 0\newline
0 &amp;amp; \frac{-1}{y^2}
\end{array}
\right)$$&lt;/p>
&lt;p>and the second degree Taylor polynomial of $f$ at the point $P=(1,1)$ is&lt;/p>
&lt;p>$$\begin{aligned}
P^2_{f,P}(x,y) &amp;amp;= f(1,1) +\nabla f(1,1)\cdot (x-1,y-1) +\newline
&amp;amp;+ \frac{1}{2}(x-1,y-1)\nabla^2f(1,1)\cdot(x-1,y-1)=\newline
&amp;amp;= \log 1+(1,1)\cdot(x-1,y-1) +\newline
&amp;amp;+ \frac{1}{2}(x-1,y-1)
\left(
\begin{array}{cc}
-1 &amp;amp; 0\newline
0 &amp;amp; -1
\end{array}
\right)
\left(
\begin{array}{c}
x-1\newline
y-1
\end{array}
\right)
= \newline
&amp;amp;= x-1+y-1+\frac{-x^2-y^2+2x+2y-2}{2} =\newline
&amp;amp;= \frac{-x^2-y^2+4x+4y-6}{2}.
\end{aligned}$$&lt;/p>
&lt;p>Thus,
$$
\begin{aligned}
f(1.01,1.01) \approx P^1_{f,P}(1.01,1.01) &amp;amp;= \frac{-1.01^2-1.01^2+4\cdot 1.01+4\cdot 1.01-6}{2} \newline
&amp;amp;= 0.0199.
\end{aligned}
$$&lt;/p>
&lt;p>The graph of the scalar field $f(x,y)=\log(xy)$ and the second degree Taylor polynomial of $f$ at the point $P=(1,1)$ is below.&lt;/p>
&lt;img src="../img/derivativesn/second_degree_taylor_polynomial.svg" alt="Second degree Taylor polynomial" width="350">
&lt;p>&lt;img src="/media/logo-geogebra.png#left" /> &lt;strong>&lt;a href="https://ggbm.at/Ehnz3hGb" target="_blank">Interactive Example&lt;/a>&lt;/strong>&lt;/p>
&lt;h2 id="relative-extrema">Relative extrema&lt;/h2>
&lt;div class="alert alert-def">
&lt;div>
&lt;p>&lt;strong>Definition - Relative extrema&lt;/strong>.
A scalar field $f$ in $\mathbb{R}^n$ has a &lt;em>relative maximum&lt;/em> at a point $P$ if there is a value $\epsilon&amp;gt;0$ such that&lt;/p>
&lt;p>$$f(P)\geq f(X)\ \forall X, |\vec{PX}|&amp;lt;\epsilon.$$&lt;/p>
&lt;p>$f$ has a &lt;em>relative minimum&lt;/em> at $f$ if there is a value $\epsilon&amp;gt;0$ such that&lt;/p>
&lt;p>$$f(P)\leq f(X)\ \forall X, |\vec{PX}|&amp;lt;\epsilon.$$&lt;/p>
&lt;p>Both relative maxima and minima are known as &lt;em>relative extrema&lt;/em> of $f$.&lt;/p>
&lt;/div>
&lt;/div>
&lt;h3 id="critical-points">Critical points&lt;/h3>
&lt;div class="alert alert-theo">
&lt;div>
&lt;p>&lt;strong>Theorem - Critical points&lt;/strong>. If a scalar field $f$ in $\mathbb{R}^n$ has a relative maximum or minimum at a point $P$, then $P$ is a &lt;em>critical or stationary point&lt;/em> of $f$, that is, a point where the gradient vanishes&lt;/p>
&lt;p>$$\nabla f(P) = 0.$$&lt;/p>
&lt;/div>
&lt;/div>
&lt;div class="spoiler " >
&lt;p>
&lt;a class="btn btn-primary" data-toggle="collapse" href="#spoiler-31" role="button" aria-expanded="false" aria-controls="spoiler-31">
Proof
&lt;/a>
&lt;/p>
&lt;div class="collapse card " id="spoiler-31">
&lt;div class="card-body">
&lt;p>Taking the trajectory that passes through $P$ with the direction of the gradient at that point $$g(t)=P+t\nabla f(P),$$ the function $h=(f\circ g)(t)$ does not decrease at $t=0$ since&lt;/p>
&lt;p>$$h'(0)= (f\circ g)'(0) = \nabla f(g(0))\cdot g'(0) = \nabla f(P)\cdot \nabla f(P) = |\nabla f(P)|^2\geq 0,$$&lt;/p>
&lt;p>and it only vanishes if $\nabla f(P)=0$.&lt;/p>
&lt;p>Thus, if $\nabla f(P)\neq 0$, $f$ can not have a relative maximum at $P$ since following the trajectory of $g$ from $P$ there are points where $f$ has an image greater than the image at $P$. In the same way, following the trajectory of $g$ in the opposite direction there are points where $f$ has an image less than the image at $P$, so $f$ can not have relative minimum at $P$.&lt;/p>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Example&lt;/strong>. Given the scalar field $f(x,y)=x^2+y^2$, it is obvious that $f$ only has a relative minimum at $(0,0)$ since&lt;/p>
&lt;p>$$f(0,0)=0 \leq f(x,y)=x^2+y^2,\ \forall x,y\in \mathbb{R}.$$&lt;/p>
&lt;p>Is easy to check that $f$ has a critical point at $(0,0)$, that is $\nabla f(0,0) = 0$.&lt;/p>
&lt;img src="../img/derivativesn/paraboloid_minimum.svg" alt="Relative minimum of a two-variable function" width="300">
&lt;h3 id="saddle-points">Saddle points&lt;/h3>
&lt;p>Not all the critical points of a scalar field are points where the scalar field has relative extrema. If we take, for instance, the scalar field $f(x,y)=x^2-y^2$, its gradient is&lt;/p>
&lt;p>$$\nabla f(x,y) = (2x,-2y),$$&lt;/p>
&lt;p>that only vanishes at $(0,0)$. However, this point is not a relative maximum since the points $(x,0)$ in the $x$-axis have images $f(x,0)=x^2\geq
0=f(0,0)$, nor a relative minimum since the points $(0,y)$ in the $y$-axis have images $f(0,y)=-y^2\leq
0=f(0,0)$. This type of critical points that are not relative extrema are known as &lt;em>saddle points&lt;/em>.&lt;/p>
&lt;img src="../img/derivativesn/saddle_point.svg" alt="Saddle point of a two-variable function" width="300">
&lt;h3 id="analysis-of-the-relative-extrema">Analysis of the relative extrema&lt;/h3>
&lt;p>From the second degree Taylor’s formula of a scalar field $f$ at a point $P$ we have&lt;/p>
&lt;p>$$f(P+\mathbf{v})-f(P)\approx \nabla f(P)\mathbf{v}+\frac{1}{2}\nabla^2f(P)\mathbf{v}\cdot\mathbf{v}.$$&lt;/p>
&lt;p>Thus, if $P$ is a critical point of $f$, as $\nabla f(P)=0$, we have&lt;/p>
&lt;p>$$f(P+\mathbf{v})-f(P)\approx \frac{1}{2}\nabla^2f(P)\mathbf{v}\cdot\mathbf{v}.$$&lt;/p>
&lt;p>Therefore, the sign of the $f(P+\mathbf{v})-f(P)$ is the sign of the second degree term $\nabla^2f(P)\mathbf{v}\cdot\mathbf{v}$.&lt;/p>
&lt;p>There are four possibilities:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Definite positive: $\nabla^2f(P)\mathbf{v}\cdot\mathbf{v}&amp;gt;0$ $\forall \mathbf{v}\neq 0$.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Definite negative: $\nabla^2f(P)\mathbf{v}\cdot\mathbf{v}&amp;lt;0$ $\forall \mathbf{v}\neq 0$.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Indefinite: $\nabla^2f(P)\mathbf{v}\cdot\mathbf{v}&amp;gt;0$ for some $\mathbf{v}\neq 0$ and $\nabla^2f(P)\mathbf{u}\cdot\mathbf{u}&amp;lt;0$ for some $\mathbf{u}\neq 0$.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Semidefinite: In any other case.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>Thus, depending on de sign of $\nabla^2 f(P)\mathbf{v}\cdot\mathbf{v}$, we have&lt;/p>
&lt;div class="alert alert-theo">
&lt;div>
&lt;p>&lt;strong>Theorem&lt;/strong>.
Given a critical point $P$ of a scalar field $f$, it holds that&lt;/p>
&lt;ul>
&lt;li>If $\nabla^2f(P)$ is definite positive then $f$ has a relative minimum at $P$.&lt;/li>
&lt;li>If $\nabla^2f(P)$ is definite negative then $f$ has a relative maximum at $P$.&lt;/li>
&lt;li>If $\nabla^2f(P)$ is indefinite then $f$ has a saddle point at $P$.&lt;/li>
&lt;/ul>
&lt;p>When $\nabla^2f(P)$ is semidefinite we can not draw any conclusion and we need higher order partial derivatives to classify the critical point.&lt;/p>
&lt;/div>
&lt;/div>
&lt;h3 id="analysis-of-the-relative-extrema-of-a-scalar-field-in-mathbbr2">Analysis of the relative extrema of a scalar field in $\mathbb{R}^2$&lt;/h3>
&lt;p>In the particular case of a scalar field of two variables, we have&lt;/p>
&lt;div class="alert alert-theo">
&lt;div>
&lt;p>&lt;strong>Theorem&lt;/strong>.
Given a critical point $P=(x_0,y_0)$ of a scalar field $f(x,y)$, it holds that&lt;/p>
&lt;ul>
&lt;li>If $Hf(P)&amp;gt;0$ and $\dfrac{\partial^2 f}{\partial x^2}(x_0,y_0)&amp;gt;0$ then $f$ has a relative minimum at $P$.&lt;/li>
&lt;li>If $Hf(P)&amp;gt;0$ and $\dfrac{\partial^2 f}{\partial x^2}(x_0,y_0)&amp;lt;0$ then $f$ has a relative maximum at $P$.&lt;/li>
&lt;li>IF $Hf(P)&amp;lt;0$ then $f$ has a saddle point at $P$.&lt;/li>
&lt;/ul>
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Example&lt;/strong>. Given the scalar field $f(x,y)=\dfrac{x^3}{3}-\dfrac{y^3}{3}-x+y$, its gradient is&lt;/p>
&lt;p>$$\nabla f(x,y)= (x^2-1,-y^2+1),$$&lt;/p>
&lt;p>and it has critical points at $(1,1)$, $(1,-1)$, $(-1,1)$ and $(-1,-1)$.&lt;/p>
&lt;p>The hessian matrix is&lt;/p>
&lt;p>$$\nabla^2f(x,y) = \left(
\begin{array}{cc}
2x &amp;amp; 0\newline
0 &amp;amp; -2y
\end{array}
\right)$$&lt;/p>
&lt;p>and the hessian is&lt;/p>
&lt;p>$$Hf(x,y) = -4xy.$$&lt;/p>
&lt;p>Thus, we have&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Point $(1,1)$: $Hf(1,1)=-4&amp;lt;0 \Rightarrow$ Saddle point.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Point $(1,-1)$: $Hf(1,-1)=4&amp;gt;0$ and $\frac{\partial^2}{\partial x^2}(1,-1)=2&amp;gt;0 \Rightarrow$ Relative min.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Point $(-1,1)$: $Hf(-1,1)=4&amp;gt;0$ and $\frac{\partial^2}{\partial x^2}(-1,1)=-2&amp;lt;0 \Rightarrow$ Relative max.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Point $(-1,-1)$: $Hf(-1,-1)=-4&amp;lt;0 \Rightarrow$ Saddle point.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>The graph of the function $f(x,y)=\dfrac{x^3}{3}-\dfrac{y^3}{3}-x+y$ and their relative extrema and saddle points are shown below.&lt;/p>
&lt;img src="../img/derivativesn/extrema_analysis.svg" alt="Relative extrema and saddle points of a two-variable function" width="350"></description></item></channel></rss>
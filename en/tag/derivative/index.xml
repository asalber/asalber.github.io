<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Derivative | Aprende con Alf</title><link>/en/tag/derivative/</link><atom:link href="/en/tag/derivative/index.xml" rel="self" type="application/rss+xml"/><description>Derivative</description><generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><image><url>/images/logo_hude38443eeb2faa5fa84365aba7d86a77_3514_300x300_fit_lanczos_3.png</url><title>Derivative</title><link>/en/tag/derivative/</link></image><item><title>One variable differential calculus</title><link>/en/teaching/calculus/manual/derivatives-one-variable/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/en/teaching/calculus/manual/derivatives-one-variable/</guid><description>&lt;h2 id="concept-of-derivative">Concept of derivative&lt;/h2>
&lt;h3 id="increment">Increment&lt;/h3>
&lt;div class="alert alert-def">
&lt;div>
&lt;strong>Definition - Increment of a variable&lt;/strong>. An &lt;em>increment of a variable&lt;/em> $x$ is a change in the value of the variable; it is denoted $\Delta x$. The increment of a variable $x$ along an interval $[a,b]$ is given by $$\Delta x = b-a.$$
&lt;/div>
&lt;/div>
&lt;div class="alert alert-def">
&lt;div>
&lt;strong>Definition - Increment of a function&lt;/strong>. The &lt;em>increment of a function&lt;/em> $y=f(x)$ along an interval $[a,b]\subseteq Dom(f)$ is given by $$\Delta y = f(b)-f(a).$$
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Example&lt;/strong>. The increment of $x$ along the interval $[2,5]$ is $\Delta x=5-2=3$, and the increment of the function $y=x^2$ along the same interval is $\Delta y=5^2-2^2=21$.&lt;/p>
&lt;h3 id="average-rate-of-change">Average rate of change&lt;/h3>
&lt;p>The study of a function $y=f(x)$ requires to understand how the function changes, that is, how the dependent variable $y$ changes when we change the independent variable $x$.&lt;/p>
&lt;div class="alert alert-def">
&lt;div>
&lt;strong>Definition - Average rate of change&lt;/strong>. The &lt;em>average rate of change&lt;/em> of a function $y=f(x)$ in an interval $[a,a+\Delta x]\subseteq Dom(f)$, is the quotient between the increment of $y$ and the increment of $x$ in that interval; it is denoted by $$\mbox{ARC}\;f[a,a+\Delta x]=\frac{\Delta y}{\Delta x}=\frac{f(a+\Delta x)-f(a)}{\Delta x}.$$
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Example - Area of a square&lt;/strong>. Let $y=x^2$ be the function that measures the area of a metallic square of side length $x$.&lt;/p>
&lt;p>If at any given time the side of the square is $a$, and we heat the square uniformly increasing the side by dilatation a quantity $\Delta x$, how much will increase the area of the square?&lt;/p>
&lt;p>$$
\Delta y = f(a+\Delta x)-f(a)=(a+\Delta x)^2-a^2=
a^2+2a\Delta x+\Delta x^2-a^2=2a\Delta x+\Delta x^2.
$$&lt;/p>
&lt;p>What is the average rate of change in the interval $[a,a+\Delta x]$? $$\mbox{ARC}\;f[a,a+\Delta x]=\frac{\Delta y}{\Delta x}=\frac{2a\Delta x+\Delta x^2}{\Delta x}=2a+\Delta x.$$&lt;/p>
&lt;img src="../img/derivatives1/square_area_variation.svg" alt="Variation of the area of a square" width="300">
&lt;h3 id="geometric-interpretation-of-the-average-rate-of-change">Geometric interpretation of the average rate of change&lt;/h3>
&lt;p>The average rate of change of a function $y=f(x)$ in an interval $[a,a+\Delta x]$ is the slope of the &lt;em>secant&lt;/em> line to the graph of $f$ through the points $(a,f(a))$ and $(a+\Delta x,f(a+\Delta x))$.&lt;/p>
&lt;img src="../img/derivatives1/secant_line.svg" alt="Secant line to a function" width="500">
&lt;h3 id="instantaneous-rate-of-change">Instantaneous rate of change&lt;/h3>
&lt;p>Often it is interesting to study the rate of change of a function, not in an interval, but in a point.&lt;/p>
&lt;p>Knowing the tendency of change of a function in an instant can be used to predict the value of the function in nearby instants.&lt;/p>
&lt;div class="alert alert-def">
&lt;div>
&lt;p>&lt;strong>Definition - Instantaneous rate of change and derivative&lt;/strong>. The &lt;em>instantaneous rate of change&lt;/em> of a function $f$ in a point $a$, is the limit of the average rate of change of $f$ in the interval $[a,a+\Delta x]$, when $\Delta x$ approaches 0; it is denoted by&lt;/p>
&lt;p>$$
\begin{aligned}
\textrm{IRC}\;f (a) &amp;amp;= \lim_{\Delta x\rightarrow 0} \textrm{ARC}\; f[a,a+\Delta x]=\lim_{\Delta x\rightarrow 0}\frac{\Delta y}{\Delta x}=\newline
&amp;amp;= \lim_{\Delta x\rightarrow 0}\frac{f(a+\Delta x)-f(a)}{\Delta x}.
\end{aligned}
$$&lt;/p>
&lt;p>When this limit exists, the function $f$ is said to be &lt;em>differentiable&lt;/em> at the point $a$, and its value is called the &lt;em>derivative&lt;/em> of $f$ at $a$, and it is denoted $f&amp;rsquo;(a)$ (Lagrange’s notation) or $\frac{df}{dx}(a)$ (Leibniz’s notation).&lt;/p>
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Example - Area of a square&lt;/strong>. Let us take again the function $y=x^2$ that measures the area of a metallic square of side $x$.&lt;/p>
&lt;p>If at any given time the side of the square is $a$, and we heat the square uniformly increasing the side, what is the tendency of change of the area in that moment?&lt;/p>
&lt;p>$$\begin{aligned}
\textrm{IRC}\;f(a)&amp;amp;=\lim_{\Delta x\rightarrow 0}\frac{\Delta y}{\Delta x} = \lim_{\Delta x\rightarrow 0}\frac{f(a+\Delta x)-f(a)}{\Delta x} =\newline
&amp;amp;= \lim_{\Delta x\rightarrow 0}\frac{2a\Delta x+\Delta x^2}{\Delta x}=\lim_{\Delta x\rightarrow 0} 2a+\Delta x= 2a.
\end{aligned}
$$&lt;/p>
&lt;p>Thus, $$f&amp;rsquo;(a)=\frac{df}{dx}(a)=2a,$$ indicating that the area of the square tends to increase the double of the side.&lt;/p>
&lt;h3 id="interpretation-of-the-derivative">Interpretation of the derivative&lt;/h3>
&lt;div class="alert alert-int">
&lt;div>
&lt;p>The derivative of a function $f&amp;rsquo;(a)$ shows the growth rate of $f$ at point $a$:&lt;/p>
&lt;ul>
&lt;li>$f&amp;rsquo;(a)&amp;gt;0$ indicates an increasing tendency ($y$ increases as $x$ increases).&lt;/li>
&lt;li>$f&amp;rsquo;(a)&amp;lt;0$ indicates a decreasing tendency ($y$ decreases as $x$ increases).&lt;/li>
&lt;/ul>
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Example&lt;/strong>. A derivative $f&amp;rsquo;(a)=3$ indicates that $y$ tends to increase triple of $x$ at point $a$. A derivative $f&amp;rsquo;(a)=-0.5$ indicates that $y$ tends to decrease half of $x$ at point $a$.&lt;/p>
&lt;h3 id="geometric-interpretation-of-the-derivative">Geometric interpretation of the derivative&lt;/h3>
&lt;p>We have seen that the average rate of change of a function $y=f(x)$ in an interval $[a,a+\Delta x]$ is the slope of the secant line, but when $\Delta x$ approaches $0$, the secant line becomes the tangent line.&lt;/p>
&lt;p>The instantaneous rate of change or derivative of a function $y=f(x)$ at $x=a$ is the slope of the &lt;em>tangent line&lt;/em> to the graph of $f$ at point $(a,f(a))$. Thus, the equation of the tangent line to the graph of $f$ at the point $(a,f(a))$ is $$y-f(a) = f&amp;rsquo;(a)(x-a) \Leftrightarrow y = f(a)+f&amp;rsquo;(a)(x-a)$$&lt;/p>
&lt;img src="../img/derivatives1/tangent_line.svg" alt="Tangent line to a function" width="450">
&lt;h3 id="kinematic-applications-linear-motion">Kinematic applications: Linear motion&lt;/h3>
&lt;p>Assume that the function $y=f(t)$ describes the position of an object moving in the real line at time $t$. Taking as reference the coordinates origin $O$ and the unitary vector $\mathbf{i}=(1)$, we can represent the position of the moving object $P$ at every moment $t$ with a vector $\vec{OP}=x\mathbf{i}$ where $x=f(t)$.&lt;/p>
&lt;img src="../img/derivatives1/linear_motion.svg" alt="Linear motion" width="500">
&lt;p>&lt;strong>Remark&lt;/strong>. It also makes sense when $f$ measures other magnitudes as the temperature of a body, the concentration of a gas, or the quantity of substance in a chemical reaction at every moment $t$.&lt;/p>
&lt;h3 id="kinematic-interpretation-of-the-average-rate-of-change">Kinematic interpretation of the average rate of change&lt;/h3>
&lt;p>In this context, if we take the instants $t=t_0$ and $t=t_0+\Delta t$, both in $\mbox{Dom}(f)$, the vector $$\mathbf{v}_m=\frac{f(t_0+\Delta t)-f(t_0)}{\Delta t}$$ is known as the &lt;em>average velocity&lt;/em> of the trajectory $f$ in the interval $[t_0, t_0+\Delta t]$.&lt;/p>
&lt;p>&lt;strong>Example&lt;/strong>. A vehicle makes a trip from Madrid to Barcelona. Let $f(t)$ be the function that determine the position of the vehicle at every moment $t$. If the vehicle departs from Madrid (km 0) at 8:00 and arrives at Barcelona (km 600) at 14:00, then the average velocity of the vehicle in the path is $$\mathbf{v}_m=\frac{f(14)-f(8)}{14-8}=\frac{600-0}{6} = 100 km/h.$$&lt;/p>
&lt;h3 id="kinematic-interpretation-of-the-derivative">Kinematic interpretation of the derivative&lt;/h3>
&lt;p>In the same context of the linear motion, the derivative of the function $f(t)$ at the moment $t_0$ is the vector&lt;/p>
&lt;p>$$\mathbf{v}=f&amp;rsquo;(t_0)=\lim_{\Delta t\rightarrow 0}\frac{f(t_0+\Delta t)-f(t_0)}{\Delta t},$$&lt;/p>
&lt;p>that is known, as long as the limit exists, as the &lt;em>instantaneous velocity&lt;/em> or simply &lt;em>velocity&lt;/em> of the trajectory $f$ at moment $t_0$.&lt;/p>
&lt;p>That is, the derivative of the object position with respect to time is a vector field that is called &lt;em>velocity along the trajectory $f$&lt;/em>.&lt;/p>
&lt;p>&lt;strong>Example&lt;/strong>. Following with the previous example, what indicates the speedometer at any instant is the modulus of the instantaneous velocity vector at that moment.&lt;/p>
&lt;h2 id="algebra-of-derivatives">Algebra of derivatives&lt;/h2>
&lt;h3 id="properties-of-the-derivative">Properties of the derivative&lt;/h3>
&lt;p>If $y=c$, is a constant function, then $y&amp;rsquo;=0$ at any point.&lt;/p>
&lt;p>If $y=x$, is the identity function, then $y&amp;rsquo;=1$ at any point.&lt;/p>
&lt;p>If $u=f(x)$ and $v=g(x)$ are two differentiable functions, then&lt;/p>
&lt;ul>
&lt;li>$(u+v)&amp;rsquo;=u&amp;rsquo;+v'$&lt;/li>
&lt;li>$(u-v)&amp;rsquo;=u&amp;rsquo;-v'$&lt;/li>
&lt;li>$(u\cdot v)&amp;rsquo;=u&amp;rsquo;\cdot v+ u\cdot v'$&lt;/li>
&lt;li>$\left(\dfrac{u}{v}\right)&amp;rsquo;=\dfrac{u&amp;rsquo;\cdot v-u\cdot v&amp;rsquo;}{v^2}$&lt;/li>
&lt;/ul>
&lt;h3 id="derivative-of-a-composite-function">Derivative of a composite function&lt;/h3>
&lt;div class="alert alert-def">
&lt;div>
&lt;strong>Theorem - Chain rule&lt;/strong>. If the function $y=f\circ g$ is the composition of two functions $y=f(z)$ and $z=g(x)$, then $$(f\circ g)&amp;rsquo;(x)=f&amp;rsquo;(g(x))g&amp;rsquo;(x).$$
&lt;/div>
&lt;/div>
&lt;div class="spoiler " >
&lt;p>
&lt;a class="btn btn-primary" data-toggle="collapse" href="#spoiler-6" role="button" aria-expanded="false" aria-controls="spoiler-6">
Proof
&lt;/a>
&lt;/p>
&lt;div class="collapse card " id="spoiler-6">
&lt;div class="card-body">
It is easy to proof this fact using the Leibniz notation $$\frac{dy}{dx}=\frac{dy}{dz}\frac{dz}{dx}=f&amp;rsquo;(z)g&amp;rsquo;(x)=f&amp;rsquo;(g(x))g&amp;rsquo;(x).$$
&lt;/div>
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Example&lt;/strong>. If $f(z)=\sin z$ and $g(x)=x^2$, then $f\circ g(x)=\sin(x^2)$. Applying the chain rule the derivative of the composite function is
$$(f\circ g)&amp;rsquo;(x)=f&amp;rsquo;(g(x))g&amp;rsquo;(x) = \cos(g(x)) 2x = \cos(x^2)2x.$$&lt;/p>
&lt;p>On the other hand, $g\circ f(z)= (\sin z)^2$, and applying the chain rule again, its derivative is
$$(g\circ f)&amp;rsquo;(z)=g&amp;rsquo;(f(z))f&amp;rsquo;(z) = 2f(z)\cos z = 2\sin z\cos z.$$&lt;/p>
&lt;h3 id="derivative-of-the-inverse-of-a-function">Derivative of the inverse of a function&lt;/h3>
&lt;div class="alert alert-def">
&lt;div>
&lt;strong>Theorem - Derivative of the inverse function&lt;/strong>. Given a function $y=f(x)$ with inverse $x=f^{-1}(y)$, then $$\left(f^{-1}\right)&amp;rsquo;(y)=\frac{1}{f&amp;rsquo;(x)}=\frac{1}{f&amp;rsquo;(f^{-1}(y))},$$
provided that $f$ is differentiable at $f^{-1}(y)$ and $f&amp;rsquo;(f^{-1}(y))\neq 0$.
&lt;/div>
&lt;/div>
&lt;div class="spoiler " >
&lt;p>
&lt;a class="btn btn-primary" data-toggle="collapse" href="#spoiler-8" role="button" aria-expanded="false" aria-controls="spoiler-8">
Proof
&lt;/a>
&lt;/p>
&lt;div class="collapse card " id="spoiler-8">
&lt;div class="card-body">
It is easy to prove this equality using the Leibniz notation $$\frac{dx}{dy}=\frac{1}{dy/dx}=\frac{1}{f&amp;rsquo;(x)}=\frac{1}{f&amp;rsquo;(f^{-1}(y))}$$
&lt;/div>
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Example&lt;/strong>. The inverse of the exponential function $y=f(x)=e^x$ is the natural logarithm $x=f^{-1}(y)=\ln y$, so we can compute the derivative of the natural logarithm using the previous theorem and we get $$\left(f^{-1}\right)&amp;rsquo;(y)=\frac{1}{f&amp;rsquo;(x)}=\frac{1}{e^x}=\frac{1}{e^{\ln y}}=\frac{1}{y}.$$&lt;/p>
&lt;p>Sometimes it is easier to apply the chain rule to compute the derivative of the inverse of a function. In this example, as $\ln x$ is the inverse of $e^x$, we know that $e^{\ln x}=x$, so differentiating both sides and applying the chain rule to the left side we get $$(e^{\ln x})&amp;rsquo;=x&amp;rsquo; \Leftrightarrow e^{\ln x}(\ln(x))&amp;rsquo; = 1 \Leftrightarrow (\ln(x))&amp;rsquo;=\frac{1}{e^{\ln x}}=\frac{1}{x}.$$&lt;/p>
&lt;h2 id="analysis-of-functions">Analysis of functions&lt;/h2>
&lt;h3 id="analysis-of-functions-increase-and-decrease">Analysis of functions: increase and decrease&lt;/h3>
&lt;p>The main application of derivatives is to determine the variation (increase or decrease) of functions. For that we use the sign of the first derivative.&lt;/p>
&lt;div class="alert alert-def">
&lt;div>
&lt;p>&lt;strong>Theorem&lt;/strong>. Let $f(x)$ be a function with first derivative in an interval $I\subseteq \mathbb{R}$.&lt;/p>
&lt;ul>
&lt;li>If $\forall x\in I\ f&amp;rsquo;(x)&amp;gt; 0$ then $f$ is increasing on $I$.&lt;/li>
&lt;li>If $\forall x\in I\ f&amp;rsquo;(x)&amp;lt; 0$ then $f$ is decreasing on $I$.&lt;/li>
&lt;/ul>
&lt;/div>
&lt;/div>
&lt;p>If $f&amp;rsquo;(x_0)=0$ then $x_0$ is known as a &lt;em>critical point&lt;/em> or &lt;em>stationary point&lt;/em>. At this point the function can be increasing, decreasing or neither increasing nor decreasing.&lt;/p>
&lt;p>&lt;strong>Example&lt;/strong> The function $f(x)=x^2$ has derivative $f&amp;rsquo;(x)=2x$; it is decreasing on $\mathbb{R}^-$ as $f&amp;rsquo;(x)&amp;lt; 0$ $\forall x\in \mathbb{R}^-$ and increasing on $\mathbb{R}^+$ as $f&amp;rsquo;(x)&amp;gt; 0$ $\forall x\in \mathbb{R}^+$.
It has a critical point at $x=0$, as $f&amp;rsquo;(0)=0$; at this point the function is neither increasing nor decreasing.&lt;/p>
&lt;div class="alert alert-warning">
&lt;div>
A function can be increasing or decreasing on an interval and not have first derivative.
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Example&lt;/strong>. Let us analyze the increase and decrease of the function $f(x)=x^4-2x^2+1$. Its first derivative is $f&amp;rsquo;(x)=4x^3-4x$.&lt;/p>
&lt;img src="../img/derivatives1/increase_analysis.svg" alt="Inrease or decrease analysis of a function" width="550">
&lt;h3 id="analysis-of-functions-relative-extrema">Analysis of functions: relative extrema&lt;/h3>
&lt;p>As a consequence of the previous result we can also use the first derivative to determine the relative extrema of a function.&lt;/p>
&lt;div class="alert alert-def">
&lt;div>
&lt;p>&lt;strong>Theorem - First derivative test&lt;/strong>. Let $f(x)$ be a function with first derivative in an interval $I\subseteq \mathbb{R}$ and let $x_0\in I$ be a critical point of $f$ ($f&amp;rsquo;(x_0)=0$).&lt;/p>
&lt;ul>
&lt;li>If $f&amp;rsquo;(x)&amp;gt;0$ on an open interval extending left from $x_0$ and $f&amp;rsquo;(x)&amp;lt;0$ on an open interval extending right from $x_0$, then $f$ has a &lt;em>relative maximum&lt;/em> at $x_0$.&lt;/li>
&lt;li>If $f&amp;rsquo;(x)&amp;lt;0$ on an open interval extending left from $x_0$ and $f&amp;rsquo;(x)&amp;gt;0$ on an open interval extending right from $x_0$, then $f$ has a &lt;em>relative minimum&lt;/em> at $x_0$.&lt;/li>
&lt;li>If $f&amp;rsquo;(x)$ has the same sign on both an open interval extending left from $x_0$ and an open interval extending right from $x_0$, then $f$ has an &lt;em>inflection point&lt;/em> at $x_0$.&lt;/li>
&lt;/ul>
&lt;/div>
&lt;/div>
&lt;div class="alert alert-warning">
&lt;div>
A vanishing derivative is a necessary but not sufficient condition for the function to have a relative extrema at a point.
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Example&lt;/strong>. The function $f(x)=x^3$ has derivative $f&amp;rsquo;(x)=3x^2$; it has a critical point at $x=0$. However it does not have a relative extrema at that point, but an inflection point.&lt;/p>
&lt;p>&lt;strong>Example&lt;/strong>. Consider again the function $f(x)=x^4-2x^2+1$ and let us analyze its relative extrema now. Its first derivative is $f&amp;rsquo;(x)=4x^3-4x$.&lt;/p>
&lt;img src="../img/derivatives1/extrema_analysis.svg" alt="Extrema analysis of a function" width="550">
&lt;h3 id="analysis-of-functions-concavity">Analysis of functions: concavity&lt;/h3>
&lt;p>The concavity of a function can be determined by de second derivative.&lt;/p>
&lt;div class="alert alert-def">
&lt;div>
&lt;p>&lt;strong>Theorem&lt;/strong>. Let $f(x)$ be a function with second derivative in an interval $I\subseteq \mathbb{R}$.&lt;/p>
&lt;ul>
&lt;li>If $\forall x\in I\ f&amp;rsquo;&amp;rsquo;(x)&amp;gt; 0$ then $f$ is concave up (convex) on $I$.&lt;/li>
&lt;li>If $\forall x\in I\ f&amp;rsquo;&amp;rsquo;(x)&amp;lt; 0$ then $f$ is concave down (concave) on $I$.&lt;/li>
&lt;/ul>
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Example&lt;/strong>. The function $f(x)=x^2$ has second derivative $f&amp;rsquo;&amp;rsquo;(x)=2&amp;gt;0$ $\forall x\in \mathbb{R}$, so it is concave up in all $\mathbb{R}$.&lt;/p>
&lt;div class="alert alert-warning">
&lt;div>
A function can be concave up or down and not have second derivative.
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Example&lt;/strong>. Let us analyze the concavity of the same function of previous examples $f(x)=x^4-2x^2+1$. Its second derivative is $f&amp;rsquo;&amp;rsquo;(x)=12x^2-4$.&lt;/p>
&lt;img src="../img/derivatives1/concavity_analysis.svg" alt="Concavity analysis of a function" width="550">
&lt;h2 id="function-approximation">Function approximation&lt;/h2>
&lt;h3 id="approximating-a-function-with-the-derivative">Approximating a function with the derivative&lt;/h3>
&lt;p>The tangent line to the graph of a function $f(x)$ at $x=a$ can be used to approximate $f$ in a neighbourhood of $a$.&lt;/p>
&lt;p>Thus, the increment of a function $f(x)$ in an interval $[a,a+\Delta x]$ can be approximated multiplying the derivative of $f$ at $a$ by the increment of $x$ $$\Delta y \approx f&amp;rsquo;(a)\Delta x$$&lt;/p>
&lt;img src="../img/derivatives1/tangent_line_approximation.gif" alt="Approximation of a function with the tangent line" width="700">
&lt;p>&lt;strong>Example - Area of a square&lt;/strong>. In the previous example of the function $y=x^2$ that measures the area of a metallic square of side $x$, if the side of the square is $a$ and we increment it by a quantity $\Delta x$, then the increment on the area will be approximately $$\Delta y \approx f&amp;rsquo;(a)\Delta x = 2a\Delta x.$$
In the figure below we can see that the error of this approximation is $\Delta x^2$, which is smaller than $\Delta x$ when $\Delta x$ approaches to 0.&lt;/p>
&lt;img src="../img/derivatives1/square_area_variation_approximation.svg" alt="Approximation of the variation of a square area" width="300">
&lt;h3 id="approximating-a-function-by-a-polynomial">Approximating a function by a polynomial&lt;/h3>
&lt;p>Another useful application of the derivative is the approximation of functions by polynomials.&lt;/p>
&lt;p>Polynomials are functions easy to calculate (sums and products) with very good properties:&lt;/p>
&lt;ul>
&lt;li>Defined in all the real numbers.&lt;/li>
&lt;li>Continuous.&lt;/li>
&lt;li>Differentiable of all orders with continuous derivatives.&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Goal&lt;/strong> Approximate a function $f(x)$ by a polynomial $p(x)$ near a point $x=a$.&lt;/p>
&lt;h3 id="approximating-a-function-by-a-polynomial-of-order-0">Approximating a function by a polynomial of order 0&lt;/h3>
&lt;p>A polynomial of degree 0 has equation
$$p(x) = c_0,$$
where $c_0$ is a constant.&lt;/p>
&lt;p>As the polynomial should coincide with the function at $a$, it must satisfy
$$p(a) = c_0 = f(a).$$&lt;/p>
&lt;p>Therefore, the polynomial of degree 0 that best approximate $f$ near $a$ is
$$p(x) = f(a).$$&lt;/p>
&lt;img src="../img/derivatives1/approximation_polynomial_0.gif" alt="Approximation of a function by a polynomial of order 0" width="550">
&lt;h3 id="approximating-a-function-by-a-polynomial-of-order-1">Approximating a function by a polynomial of order 1&lt;/h3>
&lt;p>A polynomial of order 1 has equation
$$p(x) = c_0+c_1x,$$ but it can also be written as $$p(x) = c_0+c_1(x-a).$$&lt;/p>
&lt;p>Among all the polynomials of degree 1, the one that best approximates $f(x)$ near $a$ is that which meets the following conditions&lt;/p>
&lt;ol>
&lt;li>$p$ and $f$ coincide at $a$: $p(a) = f(a)$,&lt;/li>
&lt;li>$p$ and $f$ have the same rate of change at $a$: $p&amp;rsquo;(a) = f&amp;rsquo;(a)$.&lt;/li>
&lt;/ol>
&lt;p>The last condition guarantees that $p$ and $f$ have approximately the same tendency, but it requires the function $f$ to be differentiable at $a$.&lt;/p>
&lt;p>Imposing the previous conditions we have&lt;/p>
&lt;ol>
&lt;li>$p(x)=c_0+c_1(x-a) \Rightarrow p(a)=c_0+c_1(a-a)=c_0=f(a)$,&lt;/li>
&lt;li>$p&amp;rsquo;(x)=c_1 \Rightarrow p&amp;rsquo;(a)=c_1=f&amp;rsquo;(a)$.&lt;/li>
&lt;/ol>
&lt;p>Therefore, the polynomial of degree 1 that best approximates $f$ near $a$ is
$$p(x) = f(a)+f &amp;lsquo;(a)(x-a),$$
which turns out to be the tangent line to $f$ at $(a,f(a))$.&lt;/p>
&lt;img src="../img/derivatives1/approximation_polynomial_1.gif" alt="Approximation of a function by a polynomial of order 1" width="650">
&lt;h3 id="approximating-a-function-by-a-polynomial-of-order-2">Approximating a function by a polynomial of order 2&lt;/h3>
&lt;p>A polynomial of order 2 is a parabola with equation $$p(x) = c_0+c_1x+c_2x^2,$$ but it can also be written as
$$p(x) = c_0+c_1(x-a)+c_2(x-a)^2.$$&lt;/p>
&lt;p>Among all the polynomials of degree 2, the one that best approximate $f(x)$ near $a$ is that which meets the following conditions&lt;/p>
&lt;ol>
&lt;li>$p$ and $f$ coincide at $a$: $p(a) = f(a)$,&lt;/li>
&lt;li>$p$ and $f$ have the same rate of change at $a$: $p&amp;rsquo;(a) = f&amp;rsquo;(a)$.&lt;/li>
&lt;li>$p$ and $f$ have the same concavity at $a$: $p&amp;rsquo;&amp;rsquo;(a)=f&amp;rsquo;&amp;rsquo;(a)$.&lt;/li>
&lt;/ol>
&lt;p>The last condition requires the function $f$ to be differentiable twice at $a$.&lt;/p>
&lt;p>Imposing the previous conditions we have&lt;/p>
&lt;ol>
&lt;li>$p(x)=c_0+c_1(x-a) \Rightarrow p(a)=c_0+c_1(a-a)=c_0=f(a)$,&lt;/li>
&lt;li>$p&amp;rsquo;(x)=c_1 \Rightarrow p&amp;rsquo;(a)=c_1=f&amp;rsquo;(a)$.&lt;/li>
&lt;li>$p&amp;rsquo;&amp;rsquo;(x)=2c_2 \Rightarrow p&amp;rsquo;&amp;rsquo;(a)=2c_2=f&amp;rsquo;&amp;rsquo;(a) \Rightarrow c_2=\frac{f&amp;rsquo;&amp;rsquo;(a)}{2}$.&lt;/li>
&lt;/ol>
&lt;p>Therefore, the polynomial of degree 2 that best approximates $f$ near $a$ is
$$p(x) = f(a)+f&amp;rsquo;(a)(x-a)+\frac{f&amp;rsquo;&amp;rsquo;(a)}{2}(x-a)^2.$$&lt;/p>
&lt;img src="../img/derivatives1/approximation_polynomial_2.gif" alt="Approximation of a function by a polynomial of order 2" width="750">
&lt;h3 id="approximating-a-function-by-a-polynomial-of-order-n">Approximating a function by a polynomial of order $n$&lt;/h3>
&lt;p>A polynomial of order $n$ has equation
$$p(x) = c_0+c_1x+c_2x^2+\cdots +c_nx^n,$$ but it can also be written as $$p(x) = c_0+c_1(x-a)+c_2(x-a)^2+\cdots +c_n(x-a)^n.$$&lt;/p>
&lt;p>Among all the polynomials of degree $n$, the one that best approximate $f(x)$ near $a$ is that which meets the following $n+1$ conditions:&lt;/p>
&lt;ol>
&lt;li>$p(a) = f(a)$,&lt;/li>
&lt;li>$p&amp;rsquo;(a) = f&amp;rsquo;(a)$,&lt;/li>
&lt;li>$p&amp;rsquo;&amp;rsquo;(a)=f&amp;rsquo;&amp;rsquo;(a)$,&lt;/li>
&lt;li>$\cdots$&lt;/li>
&lt;li>$p^{(n)}(a)=f^{(n)}(a)$.&lt;/li>
&lt;/ol>
&lt;p>The successive derivatives of $p$ are&lt;/p>
&lt;p>$$
\begin{aligned}
p(x) &amp;amp;= c_0+c_1(x-a)+c_2(x-a)^2+\cdots +c_n(x-a)^n,\newline
p&amp;rsquo;(x)&amp;amp; = c_1+2c_2(x-a)+\cdots +nc_n(x-a)^{n-1},\newline
p&amp;rsquo;&amp;rsquo;(x)&amp;amp; = 2c_2+\cdots +n(n-1)c_n(x-a)^{n-2},\newline
\vdots
\newline
p^{(n)}(x)&amp;amp;= n(n-1)(n-2)\cdots 1 c_n=n!c_n.
\end{aligned}
$$&lt;/p>
&lt;p>Imposing the previous conditions we have&lt;/p>
&lt;ol>
&lt;li>$p(a) = c_0+c_1(a-a)+c_2(a-a)^2+\cdots +c_n(a-a)^n=c_0=f(a)$,&lt;/li>
&lt;li>$p&amp;rsquo;(a) = c_1+2c_2(a-a)+\cdots +nc_n(a-a)^{n-1}=c_1=f&amp;rsquo;(a)$,&lt;/li>
&lt;li>$p&amp;rsquo;&amp;rsquo;(a) = 2c_2+\cdots +n(n-1)c_n(a-a)^{n-2}=2c_2=f&amp;rsquo;&amp;rsquo;(a)\Rightarrow c_2=f&amp;rsquo;&amp;rsquo;(a)/2$,&lt;/li>
&lt;li>$\cdots$&lt;/li>
&lt;li>$p^{(n)}(a)=n!c_n=f^{(n)}(a)=c_n=\frac{f^{(n)}(a)}{n!}$.&lt;/li>
&lt;/ol>
&lt;h3 id="taylor-polynomial-of-order-n">Taylor polynomial of order $n$&lt;/h3>
&lt;div class="alert alert-def">
&lt;div>
&lt;p>&lt;strong>Definition - Taylor polynomial&lt;/strong>. Given a function $f(x)$ differentiable $n$ times at $x=a$, the &lt;em>Taylor polynomial&lt;/em> of order $n$ of $f$ at $a$ is the polynomial with equation&lt;/p>
&lt;p>$$
\begin{aligned}
p_{f,a}^n(x) &amp;amp;= f(a) + f&amp;rsquo;(a)(x-a) + \frac{f&amp;rsquo;&amp;rsquo;(a)}{2}(x-a)^2 + \cdots + \frac{f^{(n)}(a)}{n!}(x-a)^n = \newline
&amp;amp;= \sum_{i=0}^{n}\frac{f^{(i)}(a)}{i!}(x-a)^i.
\end{aligned}
$$&lt;/p>
&lt;/div>
&lt;/div>
&lt;div class="alert alert-int">
&lt;div>
The Taylor polynomial of order $n$ of $f$ at $a$ is the $n$th degree polynomial that best approximates $f$ near $a$, as is the only one that meets the previous conditions.
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Example&lt;/strong>. Let us approximate the function $f(x)=\log x$ near the value $1$ by a polynomial of order $3$.&lt;/p>
&lt;p>The equation of the Taylor polynomial of order $3$ of $f$ at $a=1$ is $$p_{f,1}^3(x)=f(1)+f&amp;rsquo;(1)(x-1)+\frac{f&amp;rsquo;&amp;rsquo;(1)}{2}(x-1)^2+\frac{f&amp;rsquo;&amp;rsquo;&amp;rsquo;(1)}{3!}(x-1)^3.$$ The derivatives of $f$ at $1$ up to order $3$ are&lt;/p>
&lt;p>$$
\begin{array}{lll}
f(x)=\log x &amp;amp; \quad &amp;amp; f(1)=\log 1 =0,\newline
f&amp;rsquo;(x)=1/x &amp;amp; &amp;amp; f&amp;rsquo;(1)=1/1=1,\newline
f&amp;rsquo;&amp;rsquo;(x)=-1/x^2 &amp;amp; &amp;amp; f&amp;rsquo;&amp;rsquo;(1)=-1/1^2=-1,\newline
f&amp;rsquo;&amp;rsquo;&amp;rsquo;(x)=2/x^3 &amp;amp; &amp;amp; f&amp;rsquo;&amp;rsquo;&amp;rsquo;(1)=2/1^3=2.
\end{array}
$$&lt;/p>
&lt;p>And substituting into the polynomial equation we get $$p_{f,1}^3(x)=0+1(x-1)+\frac{-1}{2}(x-1)^2+\frac{2}{3!}(x-1)^3= \frac{2}{3}x^3-\frac{3}{2}x^2+3x-\frac{11}{6}.$$&lt;/p>
&lt;img src="../img/derivatives1/taylor_polynomials_logarithm.gif" alt="Taylor polynomials of the logarithm function" width="650">
&lt;h3 id="maclaurin-polynomial-of-order-n">Maclaurin polynomial of order $n$&lt;/h3>
&lt;p>The Taylor polynomial equation has a simpler form when the polynomial is calculated at $0$. This special case of Taylor polynomial at $0$ is known as the &lt;em>Maclaurin polynomial&lt;/em>.&lt;/p>
&lt;div class="alert alert-def">
&lt;div>
&lt;p>&lt;strong>Definition - Maclaurin polynomial&lt;/strong>. Given a function $f(x)$ differentiable $n$ times at $0$, the &lt;em>Maclaurin polynomial&lt;/em> of order $n$ of $f$ is the polynomial with equation&lt;/p>
&lt;p>$$
\begin{aligned}
p_{f,0}^n(x)&amp;amp;=f(0)+f&amp;rsquo;(0)x+\frac{f&amp;rsquo;&amp;rsquo;(0)}{2}x^2+\cdots +\frac{f^{(n)}(0)}{n!}x^n = \newline
&amp;amp;=\sum_{i=0}^{n}\frac{f^{(i)}(0)}{i!}x^i.
\end{aligned}
$$&lt;/p>
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Example&lt;/strong>. Let us approximate the function $f(x)=\sin x$ near the value $0$ by a polynomial of order $3$.&lt;/p>
&lt;p>The Maclaurin polynomial equation of order $3$ of $f$ is $$p_{f,0}^3(x)=f(0)+f&amp;rsquo;(0)x+\frac{f&amp;rsquo;&amp;rsquo;(0)}{2}x^2+\frac{f&amp;rsquo;&amp;rsquo;&amp;rsquo;(0)}{3!}x^3.$$
The derivatives of $f$ at $0$ up to order $3$ are&lt;/p>
&lt;p>$$\begin{array}{lll}
f(x)=\sin x &amp;amp; \quad &amp;amp; f(0)=\sin 0 =0,\newline
f&amp;rsquo;(x)=\cos x &amp;amp; &amp;amp; f&amp;rsquo;(0)=\cos 0=1,\newline
f&amp;rsquo;&amp;rsquo;(x)=-\sin x &amp;amp; &amp;amp; f&amp;rsquo;&amp;rsquo;(0)=-\sin 0=0,\newline
f&amp;rsquo;&amp;rsquo;&amp;rsquo;(x)=-\cos x &amp;amp; &amp;amp; f&amp;rsquo;&amp;rsquo;&amp;rsquo;(0)=-\cos 0=-1.
\end{array}
$$&lt;/p>
&lt;p>And substituting into the polynomial equation we get
$$p_{f,0}^3(x)=0+1\cdot x+\frac{0}{2}x^2+\frac{-1}{3!}x^3= x-\frac{x^3}{6}.$$&lt;/p>
&lt;img src="../img/derivatives1/maclaurin_polynomials_sine.gif" alt="Macalurin polynomials of the sine function" width="650">
&lt;h3 id="maclaurin-polynomials-of-elementary-functions">Maclaurin polynomials of elementary functions&lt;/h3>
&lt;p>$$
\renewcommand{\arraystretch}{2.5}
\begin{array}{cc}
\hline
f(x) &amp;amp; p_{f,0}^n(x) \newline
\hline
\sin x &amp;amp; \displaystyle x - \frac{x^3}{3!} + \frac{x^5}{5!} - \cdots + (-1)^k\frac{x^{2k-1}}{(2k-1)!} \mbox{ if $n=2k$ or $n=2k-1$}\newline
\cos x &amp;amp; \displaystyle 1 - \frac{x^2}{2!} + \frac{x^4}{4!} - \cdots + (-1)^k\frac{x^{2k}}{(2k)!} \mbox{ if $n=2k$ or $n=2k+1$}\newline
\arctan x &amp;amp; \displaystyle x - \frac{x^3}{3} + \frac{x^5}{5} - \cdots + (-1)^k\frac{x^{2k-1}}{(2k-1)} \mbox{ if $n=2k$ or $n=2k-1$}\newline
e^x &amp;amp; \displaystyle 1 + x + \frac{x^2}{2!} + \frac{x^3}{3!} + \cdots + \frac{x^n}{n!}\newline
\log(1+x) &amp;amp; \displaystyle x - \frac{x^2}{2} + \frac{x^3}{3} - \cdots + (-1)^{n-1}\frac{x^n}{n}\newline
\hline
\end{array}
$$&lt;/p>
&lt;h3 id="taylor-remainder-and-taylor-formula">Taylor remainder and Taylor formula&lt;/h3>
&lt;p>Taylor polynomials allow to approximate a function in a neighborhood of a value $a$, but most of the times there is an error in the approximation.&lt;/p>
&lt;div class="alert alert-def">
&lt;div>
&lt;p>&lt;strong>Definition - Taylor remainder&lt;/strong>. Given a function $f(x)$ and its Taylor polynomial of order $n$ at $a$, $p_{f,a}^n(x)$, the &lt;em>Taylor remainder&lt;/em> of order $n$ of $f$ at $a$ is the difference between the function and the polynomial,&lt;/p>
&lt;p>$$r_{f,a}^n(x)=f(x)-p_{f,a}^n(x).$$&lt;/p>
&lt;/div>
&lt;/div>
&lt;p>The Taylor remainder measures the error int the approximation of $f(x)$ by the Taylor polynomial and allow us to express the function as the Taylor polynomial plus the Taylor remainder&lt;/p>
&lt;p>$$f(x)=p_{f,a}^n(x) + r_{f,a}^n(x).$$&lt;/p>
&lt;p>This expression is known as the &lt;em>Taylor formula&lt;/em> of order $n$ or $f$ at $a$.&lt;/p>
&lt;p>It can be proved that&lt;/p>
&lt;p>$$\lim_{h\rightarrow 0}\frac{r_{f,a}^n(a+h)}{h^n}=0,$$&lt;/p>
&lt;p>which means that the remainder $r_{f,a}^n(a+h)$ is much smaller than $h^n$.&lt;/p></description></item></channel></rss>
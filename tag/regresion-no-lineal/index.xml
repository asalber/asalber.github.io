<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Regresión No Lineal | Aprende con Alf</title><link>/tag/regresion-no-lineal/</link><atom:link href="/tag/regresion-no-lineal/index.xml" rel="self" type="application/rss+xml"/><description>Regresión No Lineal</description><generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>es-es</language><image><url>/images/logo_hude38443eeb2faa5fa84365aba7d86a77_3514_300x300_fit_lanczos_3.png</url><title>Regresión No Lineal</title><link>/tag/regresion-no-lineal/</link></image><item><title>Regresión</title><link>/docencia/estadistica/manual/regresion/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docencia/estadistica/manual/regresion/</guid><description>&lt;p>Hasta ahora se ha visto como describir el comportamiento de una variable, pero en los fenómenos naturales normalmente aparecen más de una variable que suelen estar relacionadas. Por ejemplo, en un estudio sobre el peso de las personas, deberíamos incluir todas las variables con las que podría tener relación: altura, edad, sexo, dieta, tabaco,
ejercicio físico, etc.&lt;/p>
&lt;p>Para comprender el fenómeno no basta con estudiar cada variable por separado y es preciso un estudio conjunto de todas las variables para ver cómo interactúan y qué relaciones se dan entre ellas. El objetivo de la estadística en este caso es dar medidas del grado y del tipo de relación entre dichas variables.&lt;/p>
&lt;p>Generalmente, en un &lt;em>estudio de dependencia&lt;/em> se considera una &lt;strong>variable dependiente&lt;/strong> $Y$ que se supone relacionada con otras variables $X_1,\ldots,X_n$ llamadas &lt;strong>variables independientes&lt;/strong>.&lt;/p>
&lt;p>El caso más simple es el de una sola variable independiente, y en tal caso se habla de &lt;em>estudio de dependencia simple&lt;/em>. Para más de una
variable independiente se habla de &lt;em>estudio de dependencia múltiple&lt;/em>.&lt;/p>
&lt;p>En este capítulo se verán los estudios de dependencia simple que son más sencillos.&lt;/p>
&lt;h2 id="distribución-de-frecuencias-conjunta">Distribución de frecuencias conjunta&lt;/h2>
&lt;h3 id="frecuencias-conjuntas">Frecuencias conjuntas&lt;/h3>
&lt;p>Al estudiar la dependencia simple entre dos variables $X$ e $Y$, no se pueden estudiar sus distribuciones por separado, sino que hay que estudiar la distribución conjunta de la &lt;strong>variable bidimensional&lt;/strong> $(X,Y)$, cuyos valores son los pares $(x_i,y_j)$ donde el primer elemento es un valor $X$ y el segundo uno de $Y$.&lt;/p>
&lt;div class="alert alert-def">
&lt;div>
&lt;p>&lt;strong>Definición - Frecuencias muestrales conjuntas&lt;/strong>. Dada una muestra de tamaño $n$ de una variable bidimensional $(X,Y)$, para cada valor de la variable $(x_i,y_j)$ observado en la muestra se define&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Frecuencia absoluta&lt;/strong> $n_{ij}$: Es el número de veces que el par $(x_i,y_j)$ aparece en la muestra.&lt;/li>
&lt;li>&lt;strong>Frecuencia relativa&lt;/strong> $f_{ij}$: Es la proporción de veces que el par $(x_i,y_j)$ aparece en la muestra.&lt;/li>
&lt;/ul>
&lt;p>$$f_{ij}=\frac{n_{ij}}{n}$$&lt;/p>
&lt;/div>
&lt;/div>
&lt;div class="alert alert-warning">
&lt;div>
Para las variables bidimensionales no tienen sentido las frecuencias acumuladas.
&lt;/div>
&lt;/div>
&lt;h3 id="distribución-de-frecuencias-bidimensional">Distribución de frecuencias bidimensional&lt;/h3>
&lt;p>Al conjunto de valores de la variable bidimensional y sus respectivas frecuencias muestrales se le denomina &lt;strong>distribución de frecuencias bidimensional&lt;/strong>, y se representa mediante una &lt;strong>tabla de frecuencias bidimensional&lt;/strong>.&lt;/p>
&lt;p>$$\begin{array}{|c|ccccc|}
\hline
X\backslash Y &amp;amp; y_1 &amp;amp; \cdots &amp;amp; y_j &amp;amp; \cdots &amp;amp; y_q\newline
\hline
x_1 &amp;amp; n_{11} &amp;amp; \cdots &amp;amp; n_{1j} &amp;amp; \cdots &amp;amp; n_{1q}\newline
\vdots &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \vdots\newline
x_i &amp;amp; n_{i1} &amp;amp; \cdots &amp;amp; n_{ij} &amp;amp; \cdots &amp;amp; n_{iq}\newline
\vdots &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \vdots\newline
x_p &amp;amp; n_{p1} &amp;amp; \cdots &amp;amp; n_{pj} &amp;amp; \cdots &amp;amp; n_{pq}\newline
\hline
\end{array}$$&lt;/p>
&lt;p>&lt;strong>Ejemplo (datos agrupados)&lt;/strong>. La estatura (en cm) y el peso (en Kg) de una muestra de 30 estudiantes es:&lt;/p>
&lt;div style="text-align:center">
(179,85), (173,65), (181,71), (170,65), (158,51), (174,66),&lt;br/>
(172,62), (166,60), (194,90), (185,75), (162,55), (187,78),&lt;br/>
(198,109), (177,61), (178,70), (165,58), (154,50), (183,93),&lt;br/>
(166,51), (171,65), (175,70), (182,60), (167,59), (169,62),&lt;br/>
(172,70), (186,71), (172,54), (176,68),(168,67), (187,80).
&lt;/div>
&lt;p>La tabla de frecuencias bidimensional es&lt;/p>
&lt;p>$$\begin{array}{|c||c|c|c|c|c|c|}
\hline
X/Y &amp;amp; [50,60) &amp;amp; [60,70) &amp;amp; [70,80) &amp;amp; [80,90) &amp;amp; [90,100) &amp;amp; [100,110) \newline
\hline\hline
(150,160] &amp;amp; 2 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 \newline
\hline
(160,170] &amp;amp; 4 &amp;amp; 4 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 \newline
\hline
(170,180] &amp;amp; 1 &amp;amp; 6 &amp;amp; 3 &amp;amp; 1 &amp;amp; 0 &amp;amp; 0 \newline
\hline
(180,190] &amp;amp; 0 &amp;amp; 1 &amp;amp; 4 &amp;amp; 1 &amp;amp; 1 &amp;amp; 0 \newline
\hline
(190,200] &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 1 &amp;amp; 1 \newline
\hline
\end{array}$$&lt;/p>
&lt;h3 id="diagrama-de-dispersión">Diagrama de dispersión&lt;/h3>
&lt;p>La distribución de frecuencias conjunta de una variable bidimensional puede representarse gráficamente mediante un &lt;strong>diagrama de dispersión&lt;/strong>, donde los datos se representan como una colección de puntos en un plano cartesiano.&lt;/p>
&lt;p>Habitualmente la variable independiente se representa en el eje $X$ y la variable dependiente en el eje $Y$. Por cada par de valores $(x_i,y_j)$ en la muestra se dibuja un punto en el plano con esas coordenadas.&lt;/p>
&lt;img src="../img/regresion/diagrama_dispersion.svg" alt="Diagrama de dispersión" width="300">
&lt;p>El resultado es un conjunto de puntos que se conoce como &lt;em>nube de puntos&lt;/em>.&lt;/p>
&lt;p>&lt;strong>Ejemplo&lt;/strong>. El siguiente diagrama de dispersión representa la distribución conjunta de estaturas y pesos de la muestra anterior.&lt;/p>
&lt;img src="../img/regresion/diagrama_dispersion_estatura_peso.svg" alt="Diagrama de dispersión de estaturas y pesos" width="600">
&lt;div class="alert alert-int">
&lt;div>
El diagrama de dispersión da información visual sobre el tipo de relación entre las variables.&lt;/p>
&lt;img src="../img/regresion/diagrama_dispersion_tipos_relaciones.svg" alt="Diagramas de dispersión de diferentes tipos de relaciones" width="700">
&lt;/div>
&lt;/div>
&lt;h3 id="distribuciones-marginales">Distribuciones marginales&lt;/h3>
&lt;p>A cada una de las distribuciones de las variables que conforman la
variable bidimensional se les llama .&lt;/p>
&lt;p>Las distribuciones marginales se pueden obtener a partir de la tabla de
frecuencias bidimensional, sumando las frecuencias por filas y columnas.&lt;/p>
&lt;p>$$
\begin{array}{|c|ccccc|c|}
\hline
X\backslash Y &amp;amp; y_1 &amp;amp; \cdots &amp;amp; y_j &amp;amp; \cdots &amp;amp; y_q &amp;amp; \color{red}{n_x}\newline
\hline
x_1 &amp;amp; n_{11} &amp;amp; \cdots &amp;amp; n_{1j} &amp;amp; \cdots &amp;amp; n_{1q} &amp;amp; \color{red}{n_{x_1}}\newline
\vdots &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \downarrow + &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \color{red}{\vdots} \newline
x_i &amp;amp; n_{i1} &amp;amp; \stackrel{+}{\rightarrow} &amp;amp; n_{ij} &amp;amp; \stackrel{+}{\rightarrow} &amp;amp; n_{iq} &amp;amp; \color{red}{n_{x_i}}\newline
\vdots &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \downarrow + &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \color{red}{\vdots}\newline
x_p &amp;amp; n_{p1} &amp;amp; \cdots &amp;amp; n_{pj} &amp;amp; \cdots &amp;amp; n_{pq} &amp;amp; \color{red}{n_{x_p}} \newline
\hline
\color{red}{n_y} &amp;amp; \color{red}{n_{y_1}} &amp;amp; \color{red}{\cdots} &amp;amp; \color{red}{n_{y_j}} &amp;amp; \color{red}{\cdots} &amp;amp; \color{red}{n_{y_q}} &amp;amp; n\newline
\hline
\end{array}
$$&lt;/p>
&lt;p>&lt;strong>Ejemplo&lt;/strong>. En el ejemplo anterior de las estaturas y los pesos, las distribuciones marginales son&lt;/p>
&lt;p>$$
\begin{array}{|c||c|c|c|c|c|c|c|}
\hline
X/Y &amp;amp; [50,60) &amp;amp; [60,70) &amp;amp; [70,80) &amp;amp; [80,90) &amp;amp; [90,100) &amp;amp; [100,110) &amp;amp; \color{red}{n_x}\newline
\hline\hline
(150,160] &amp;amp; 2 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; \color{red}{2}\newline
\hline
(160,170] &amp;amp; 4 &amp;amp; 4 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; \color{red}{8}\newline
\hline
(170,180] &amp;amp; 1 &amp;amp; 6 &amp;amp; 3 &amp;amp; 1 &amp;amp; 0 &amp;amp; 0 &amp;amp; \color{red}{11} \newline
\hline
(180,190] &amp;amp; 0 &amp;amp; 1 &amp;amp; 4 &amp;amp; 1 &amp;amp; 1 &amp;amp; 0 &amp;amp; \color{red}{7} \newline
\hline
(190,200] &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 1 &amp;amp; 1 &amp;amp; \color{red}{2}\newline
\hline
\color{red}{n_y} &amp;amp; \color{red}{7} &amp;amp; \color{red}{11} &amp;amp; \color{red}{7} &amp;amp; \color{red}{2} &amp;amp; \color{red}{2} &amp;amp; \color{red}{1} &amp;amp; 30\newline
\hline
\end{array}
$$&lt;/p>
&lt;p>y los estadísticos correspondientes son&lt;/p>
&lt;p>$$
\begin{array}{lllll}
\bar x = 174.67 \mbox{ cm} &amp;amp; \quad &amp;amp; s^2_x = 102.06 \mbox{ cm}^2 &amp;amp; \quad &amp;amp; s_x = 10.1 \mbox{ cm}\newline
\bar y = 69.67 \mbox{ Kg} &amp;amp; &amp;amp; s^2_y = 164.42 \mbox{ Kg}^2 &amp;amp; &amp;amp; s_y = 12.82 \mbox{ Kg}
\end{array}
$$&lt;/p>
&lt;h2 id="covarianza">Covarianza&lt;/h2>
&lt;p>Para analizar la relación entre dos variables cuantitativas es importante hacer un estudio conjunto de las desviaciones respecto de la media de cada variable.&lt;/p>
&lt;img src="../img/regresion/desviaciones_media.svg" alt="Desviaciones de las medias en un diagrama de dispersión" width="600">
&lt;p>Si dividimos la nube de puntos del diagrama de dispersión en 4 cuadrantes centrados en el punto de medias $(\bar x, \bar y)$, el signo de las desviaciones será:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align:center">&lt;strong>Cuadrante&lt;/strong>&lt;/th>
&lt;th style="text-align:center">$(x_i-\bar x)$&lt;/th>
&lt;th style="text-align:center">$(y_j-\bar y)$&lt;/th>
&lt;th style="text-align:center">$(x_i-\bar x)(y_j-\bar y)$&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:center">1&lt;/td>
&lt;td style="text-align:center">$+$&lt;/td>
&lt;td style="text-align:center">$+$&lt;/td>
&lt;td style="text-align:center">$+$&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">2&lt;/td>
&lt;td style="text-align:center">$-$&lt;/td>
&lt;td style="text-align:center">$+$&lt;/td>
&lt;td style="text-align:center">$-$&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">3&lt;/td>
&lt;td style="text-align:center">$-$&lt;/td>
&lt;td style="text-align:center">$-$&lt;/td>
&lt;td style="text-align:center">$+$&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">4&lt;/td>
&lt;td style="text-align:center">$+$&lt;/td>
&lt;td style="text-align:center">$-$&lt;/td>
&lt;td style="text-align:center">$-$&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;img src="../img/regresion/cuadrantes_diagrama_dispersion.svg" alt="Cuadrantes de un diagrama de dispersión" width="400">
&lt;p>Si la relación entre las variables es &lt;em>lineal y creciente&lt;/em>, entonces la mayor parte de los puntos estarán en los cuadrantes 1 y 3 y la suma de los productos de desviaciones será positiva.&lt;/p>
&lt;p>$$\sum(x_i-\bar x)(y_j-\bar y) &amp;gt; 0$$&lt;/p>
&lt;img src="../img/regresion/diagrama_dispersion_lineal_creciente.svg" alt="Diagrama de dispersión de una relación lineal creciente" width="500">
&lt;p>Si la relación entre las variables es &lt;em>lineal y decreciente&lt;/em>, entonces la mayor parte de los puntos estarán en los cuadrantes 2 y 4 y la suma de los productos de desviaciones será negativa.&lt;/p>
&lt;p>$$\sum(x_i-\bar x)(y_j-\bar y) = -$$&lt;/p>
&lt;img src="../img/regresion/diagrama_dispersion_lineal_decreciente.svg" alt="Diagrama de dispersión de una relación lineal decreciente" width="500">
&lt;p>Usando el producto de las desviaciones respecto de las medias surge el
siguiente estadístico.&lt;/p>
&lt;div class="alert alert-def">
&lt;div>
&lt;p>&lt;strong>Definición - Covarianza muestral&lt;/strong>. La &lt;em>covarianza muestral&lt;/em> de una variable aleatoria bidimensional $(X,Y)$ se define como el promedio de los productos de las respectivas desviaciones respecto de las medias de $X$ e $Y$.&lt;/p>
&lt;p>$$s_{xy}=\frac{\sum (x_i-\bar x)(y_j-\bar y)n_{ij}}{n}$$&lt;/p>
&lt;/div>
&lt;/div>
&lt;p>También puede calcularse de manera más sencilla mediante la fórmula&lt;/p>
&lt;p>$$s_{xy}=\frac{\sum x_iy_jn_{ij}}{n}-\bar x\bar y.$$&lt;/p>
&lt;div class="alert alert-int">
&lt;div>
&lt;p>La covarianza sirve para estudiar la relación lineal entre dos variables:&lt;/p>
&lt;ul>
&lt;li>Si $s_{xy}&amp;gt;0$ existe una relación lineal creciente.&lt;/li>
&lt;li>Si $s_{xy}&amp;lt;0$ existe una relación lineal decreciente.&lt;/li>
&lt;li>Si $s_{xy}=0$ no existe relación lineal.&lt;/li>
&lt;/ul>
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Ejemplo&lt;/strong>. Utilizando la tabla de frecuencias bidimensional de la muestra de estaturas y pesos&lt;/p>
&lt;p>$$
\begin{array}{|c||c|c|c|c|c|c|c|}
\hline
X/Y &amp;amp; [50,60) &amp;amp; [60,70) &amp;amp; [70,80) &amp;amp; [80,90) &amp;amp; [90,100) &amp;amp; [100,110) &amp;amp; n_x\newline
\hline\hline
(150,160] &amp;amp; 2 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 2\newline
\hline
(160,170] &amp;amp; 4 &amp;amp; 4 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 8\newline
\hline
(170,180] &amp;amp; 1 &amp;amp; 6 &amp;amp; 3 &amp;amp; 1 &amp;amp; 0 &amp;amp; 0 &amp;amp; 11 \newline
\hline
(180,190] &amp;amp; 0 &amp;amp; 1 &amp;amp; 4 &amp;amp; 1 &amp;amp; 1 &amp;amp; 0 &amp;amp; 7 \newline
\hline
(190,200] &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 1 &amp;amp; 1 &amp;amp; 2\newline
\hline
n_y &amp;amp; 7 &amp;amp; 11 &amp;amp; 7 &amp;amp; 2 &amp;amp; 2 &amp;amp; 1 &amp;amp; 30\newline
\hline
\end{array}
$$&lt;/p>
&lt;p>$$\bar x = 174.67 \mbox{ cm} \qquad \bar y = 69.67 \mbox{ Kg}$$&lt;/p>
&lt;p>la covarianza vale&lt;/p>
&lt;p>$$
\begin{aligned}
s_{xy} &amp;amp;=\frac{\sum x_iy_jn_{ij}}{n}-\bar x\bar y = \frac{155\cdot 55\cdot 2 + 165\cdot 55\cdot 4 + \cdots + 195\cdot 105\cdot 1}{30}-174.67\cdot 69.67 =\newline
&amp;amp; = \frac{368200}{30}-12169.26 = 104.07 \mbox{ cm$\cdot$ Kg}.
\end{aligned}
$$&lt;/p>
&lt;p>Esto indica que existe una relación lineal creciente entre la estatura y el peso.&lt;/p>
&lt;h2 id="regresión">Regresión&lt;/h2>
&lt;p>En muchos casos el objetivo de un estudio no es solo detectar una relación entre dos variables, sino explicarla mediante alguna función matemática $$y=f(x)$$ que permita predecir la variable dependiente para cada valor de la independiente.&lt;/p>
&lt;p>La &lt;strong>regresión&lt;/strong> es la parte de la Estadística encargada de construir esta función, que se conoce como &lt;strong>función de regresión&lt;/strong> o &lt;strong>modelo de regresión&lt;/strong>.&lt;/p>
&lt;h3 id="modelos-de-regresión-simple">Modelos de regresión simple&lt;/h3>
&lt;p>Dependiendo de la forma de función de regresión, existen muchos tipos de
regresión simple. Los más habituales son los que aparecen en la
siguiente tabla:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align:left">&lt;strong>Modelo&lt;/strong>&lt;/th>
&lt;th style="text-align:center">&lt;strong>Ecuación&lt;/strong>&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:left">Lineal&lt;/td>
&lt;td style="text-align:center">$y=a+bx$&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">Cuadrático&lt;/td>
&lt;td style="text-align:center">$y=a+bx+cx^2$&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">Cúbico&lt;/td>
&lt;td style="text-align:center">$y=a+bx+cx^2+dx^3$&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">Potencial&lt;/td>
&lt;td style="text-align:center">$y=a\cdot x^b$&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">Exponencial&lt;/td>
&lt;td style="text-align:center">$y=e^{a+bx}$&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">Logarítmico&lt;/td>
&lt;td style="text-align:center">$y=a+b\log x$&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">Inverso&lt;/td>
&lt;td style="text-align:center">$y=a+\frac{b}{x}$&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">Sigmoidal&lt;/td>
&lt;td style="text-align:center">$y=e^{a+\frac{b}{x}}$&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>La elección de un tipo u otro depende de la forma que tenga la nube de puntos del diagrama de dispersión.&lt;/p>
&lt;h3 id="residuos-o-errores-predictivos">Residuos o errores predictivos&lt;/h3>
&lt;p>Una vez elegida la familia de curvas que mejor se adapta a la nube de
puntos, se determina, dentro de dicha familia, la curva que mejor se
ajusta a la distribución, es decir, la función que mejor predice la variable dependiente.&lt;/p>
&lt;p>El objetivo es encontrar la función de regresión que haga mínimas las
distancias entre los valores de la variable dependiente observados en la
muestra, y los predichos por la función de regresión. Estas distancias
se conocen como &lt;em>residuos&lt;/em> o &lt;em>errores predictivos&lt;/em>.&lt;/p>
&lt;div class="alert alert-def">
&lt;div>
&lt;p>&lt;strong>Definición - Residuos o errores predictivos&lt;/strong>. Dado el modelo de regresión $y=f(x)$ para una variable bidimensional $(X,Y)$, el &lt;em>residuo&lt;/em> o &lt;em>error predictivo&lt;/em> de un valor $(x_i,y_j)$ observado en la muestra, es la diferencia entre el valor observado de la variable dependiente $y_j$ y el predicho por la función de regresión para $x_i$,&lt;/p>
&lt;p>$$e_{ij} = y_j-f(x_i).$$&lt;/p>
&lt;/div>
&lt;/div>
&lt;img src="../img/regresion/residuos_y.svg" alt="Residuos de un modelo de regresión" width="600">
&lt;h3 id="ajuste-de-mínimos-cuadrados">Ajuste de mínimos cuadrados&lt;/h3>
&lt;p>Una forma posible de obtener la función de regresión es mediante el método de &lt;em>mínimos cuadrados&lt;/em> que consiste en calcular la función que haga mínima la suma de los cuadrados de los residuos&lt;/p>
&lt;p>$$\sum e_{ij}^2.$$&lt;/p>
&lt;p>En el caso de un modelo de regresión lineal $f(x) = a + bx$, como la recta depende de dos parámetros (el término independiente $a$ y la pendiente $b$), la suma también dependerá de estos parámetros&lt;/p>
&lt;p>$$\theta(a,b) = \sum e_{ij}^2 =\sum (y_j - f(x_i))^2 =\sum (y_j-a-bx_i)^2.$$&lt;/p>
&lt;p>Así pues, todo se reduce a buscar los valores $a$ y $b$ que hacen mínima esta suma.&lt;/p>
&lt;p>Considerando la suma de los cuadrados de los residuos como una función de dos variables $\theta(a,b)$, se pueden calcular los valores de los parámetros del modelo que hacen mínima esta suma derivando e igualando a 0 las derivadas con respecto a $a$ y $b$.&lt;/p>
&lt;p>$$
\begin{aligned}
\frac{\partial \theta(a,b)}{\partial a} &amp;amp;= \frac{\partial \sum (y_j-a-bx_i)^2 }{\partial a} =0\newline
\frac{\partial \theta(a,b)}{\partial b} &amp;amp;= \frac{\partial \sum (y_j-a-bx_i)^2 }{\partial b} =0
\end{aligned}
$$&lt;/p>
&lt;p>Tras resolver el sistema se obtienen los valores&lt;/p>
&lt;p>$$a= \bar y - \frac{s_{xy}}{s_x^2}\bar x \qquad b=\frac{s_{xy}}{s_x^2}$$&lt;/p>
&lt;p>Estos valores hacen mínimos los residuos en $Y$ y por tanto dan la recta
de regresión óptima.&lt;/p>
&lt;h2 id="recta-de-regresión">Recta de regresión&lt;/h2>
&lt;div class="alert alert-def">
&lt;div>
&lt;p>&lt;strong>Definición - Recta de regresión&lt;/strong>. Dada una variable bidimensional $(X,Y)$, la &lt;em>recta de regresión&lt;/em> de $Y$ sobre $X$ es&lt;/p>
&lt;p>$$y = \bar y +\frac{s_{xy}}{s_x^2}(x-\bar x).$$&lt;/p>
&lt;/div>
&lt;/div>
&lt;div class="alert alert-int">
&lt;div>
La recta de regresión de $Y$ sobre $X$ es la recta que hace mínimos los errores predictivos en $Y$, y por tanto es la recta que hará mejores predicciones de $Y$ para cualquier valor de $X$.
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Ejemplo&lt;/strong>. Utilizando la muestra anterior de estaturas ($X$) y pesos ($Y$) con los siguientes estadísticos&lt;/p>
&lt;p>$$
\begin{array}{lllll}
\bar x = 174.67 \mbox{ cm} &amp;amp; \quad &amp;amp; s^2_x = 102.06 \mbox{ cm}^2 &amp;amp; \quad &amp;amp; s_x = 10.1 \mbox{ cm}\newline
\bar y = 69.67 \mbox{ Kg} &amp;amp; &amp;amp; s^2_y = 164.42 \mbox{ Kg}^2 &amp;amp; &amp;amp; s_y = 12.82 \mbox{ Kg}\newline
&amp;amp; &amp;amp; s_{xy} = 104.07 \mbox{ cm$\cdot$ Kg} &amp;amp; &amp;amp;
\end{array}
$$&lt;/p>
&lt;p>la recta de regresión del peso sobre la estatura es&lt;/p>
&lt;p>$$y = \bar y +\frac{s_{xy}}{s_x^2}(x-\bar x) = 69.67+\frac{104.07}{102.06}(x-174.67) = -108.49 + 1.02 x.$$&lt;/p>
&lt;p>De igual modo, si tomamos la estatura como variable dependiente, la
recta de regresión de la estatura sobre el peso es&lt;/p>
&lt;p>$$x = \bar x +\frac{s_{xy}}{s_y^2}(y-\bar y) = 174.67+\frac{104.07}{164.42}(y-69.67) = +130.78 + 0.63 y.$$&lt;/p>
&lt;div class="alert alert-warning">
&lt;div>
¡Obsérvese que ambas rectas de regresión son diferentes!
&lt;/div>
&lt;/div>
&lt;img src="../img/regresion/rectas_regresion.svg" alt="Rectas de regresión de estaturas y pesos" width="600">
&lt;h3 id="posición-relativa-de-las-rectas-de-regresión">Posición relativa de las rectas de regresión&lt;/h3>
&lt;p>Habitualmente, las rectas de regresión $Y$ sobre $X$ y de $X$ sobre $Y$ no coinciden, pero siempre se cortan en el punto de medias $(\bar x,\bar y)$.&lt;/p>
&lt;p>Si entre las variables la relación lineal es perfecta, entonces ambas rectas coinciden ya que esa recta hace tanto los residuos en $X$ como los residuos en $Y$ nulos.&lt;/p>
&lt;img src="../img/regresion/regresion_lineal_perfecta.svg" alt="Recta de regresión de una relación lineal perfecta" width="500">
&lt;p>Si no hay relación lineal, entonces las ecuaciones de las rectas son constantes e iguales a las respectivas medias,&lt;/p>
&lt;p>$$y = \bar y,\quad x = \bar x,$$&lt;/p>
&lt;p>y se cortan perpendicularmente.&lt;/p>
&lt;img src="../img/regresion/rectas_independencia_lineal.svg" alt="Rectas de regresión de dos variables linealmente independientes" width="500">
&lt;h3 id="coeficiente-de-regresión">Coeficiente de regresión&lt;/h3>
&lt;p>El parámetro más importante de una recta de regresión es su pendiente.&lt;/p>
&lt;div class="alert alert-def">
&lt;div>
&lt;p>&lt;strong>Definición - Coeficiente de regresión&lt;/strong> $b_{yx}$. Dada una variable bidimensional $(X,Y)$, el _coeficiente de regresión_ de la recta de regresión de $Y$ sobre $X$ es su pendiente,&lt;/p>
&lt;p>$$b_{yx} = \frac{s_{xy}}{s_x^2}$$&lt;/p>
&lt;/div>
&lt;/div>
&lt;div class="alert alert-warning">
&lt;div>
El coeficiente de regresión siempre tiene el mismo signo que la covarianza.
&lt;/div>
&lt;/div>
&lt;div class="alert alert-int">
&lt;div>
Refleja el crecimiento de la variable dependiente en relación a la independiente según la recta de regresión. En concreto da el número de unidades que aumenta o disminuye la variable dependiente por cada unidad que aumenta la variable independiente.
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Ejemplo&lt;/strong>. En el ejemplo de las estaturas y los pesos, la recta de regresión del
peso sobre la estatura era&lt;/p>
&lt;p>$$y=-108.49 +1.02 x,$$&lt;/p>
&lt;p>de manera que el coeficiente de regresión del peso sobre la estatura es&lt;/p>
&lt;p>$$b_{yx}= 1.02 \mbox{Kg/cm.}$$&lt;/p>
&lt;p>Esto significa que, según la recta de regresión del peso sobre la estatura, por cada cm más de estatura, la persona pesará $1.02$ Kg más.&lt;/p>
&lt;h3 id="predicciones-con-las-rectas-de-regresión">Predicciones con las rectas de regresión&lt;/h3>
&lt;p>Las rectas de regresión, y en general cualquier modelo de regresión, suele utilizarse con fines predictivos.&lt;/p>
&lt;p>&lt;strong>Ejemplo&lt;/strong>. En la muestra de las estaturas y los pesos, si se quiere predecir
el peso de una persona que mide 180 cm, se debe utilizar la recta de regresión del peso sobre la estatura,&lt;/p>
&lt;p>$$y = 1.02 \cdot 180 -108.49 = 75.11 \mbox{ Kg}.$$&lt;/p>
&lt;p>Y si se quiere predecir la estatura de una persona que pesa 79 Kg, se debe utilizar la recta de regresión de la estatura sobre el peso,&lt;/p>
&lt;p>$$x = 0.63\cdot 79+ 130.78 = 180.55 \mbox{ cm}.$$&lt;/p>
&lt;p>&lt;em>Ahora bien, ¿qué fiabilidad tienen estas predicciones?&lt;/em>&lt;/p>
&lt;h2 id="correlación">Correlación&lt;/h2>
&lt;p>Una vez construido un modelo de regresión, para saber si se trata de un buen modelo predictivo, se tiene que analizar el grado de dependencia entre las variables según el tipo de dependencia planteada en el modelo.
De ello se encarga la parte de la estadística conocida como &lt;strong>correlación&lt;/strong>.&lt;/p>
&lt;p>La correlación se basa en el estudio de los residuos: cuanto menores sean éstos, más se ajustará la curva de regresión a los puntos, y más intensa será la correlación.&lt;/p>
&lt;h3 id="varianza-residual-muestral">Varianza residual muestral&lt;/h3>
&lt;p>Una medida de la bondad del ajuste del modelo de regresión es la
&lt;em>varianza residual&lt;/em>.&lt;/p>
&lt;div class="alert alert-def">
&lt;div>
&lt;p>&lt;strong>Definición - Varianza residual muestral&lt;/strong> $s_{ry}^2$. Dado un modelo de regresión simple $y=f(x)$ de una variable bidimensional $(X,Y)$, su _varianza residual muestral_ es el promedio de los cuadrados de los residuos para los valores de la muestra,&lt;/p>
&lt;p>$$s_{ry}^2 = \frac{\sum e_{ij}^2n_{ij}}{n} = \frac{\sum (y_j - f(x_i))^2n_{ij}}{n}.$$&lt;/p>
&lt;/div>
&lt;/div>
&lt;div class="alert alert-int">
&lt;div>
&lt;p>Cuanto más alejados estén los puntos de la curva de regresión, mayor será la varianza residual y menor la dependencia.&lt;/p>
&lt;p>Cuando la relación lineal es perfecta los residuos se anulan y la varianza residual vale cero. Por contra, cuando no existe relación, los residuos coinciden con las desviaciones de la media, y la varianza residual es igual a la varianza de la variable dependiente.&lt;/p>
&lt;p>$$0\leq s_{ry}^2\leq s_y^2$$&lt;/p>
&lt;/div>
&lt;/div>
&lt;h3 id="descomposición-de-la-variabilidad-total-variabilidad-explicada-y-no-explicada">Descomposición de la variabilidad total: Variabilidad explicada y no explicada&lt;/h3>
&lt;img src="../img/regresion/variation_decomposition.gif" alt="Descomposición de la variabilidad de un modelo de regresión" width="600">
&lt;h3 id="coeficiente-de-determinación">Coeficiente de determinación&lt;/h3>
&lt;p>A partir de la varianza residual se puede definir otro estadístico más sencillo de interpretar.&lt;/p>
&lt;div class="alert alert-def">
&lt;div>
&lt;p>&lt;strong>Definición - Coeficiente de determinación muestral $r^2$&lt;/strong>. Dado un modelo de regresión simple $y=f(x)$ de una variable bidimensional $(X,Y)$, su &lt;em>coeficiente de determinación muestral&lt;/em> es&lt;/p>
&lt;p>$$r^2 = 1- \frac{s_{ry}^2}{s_y^2}$$&lt;/p>
&lt;/div>
&lt;/div>
&lt;div class="alert alert-warning">
&lt;div>
&lt;p>Como la varianza residual puede tomar valores entre 0 y $s_y^2$, se tiene que&lt;/p>
&lt;p>$$0\leq r^2\leq 1$$&lt;/p>
&lt;/div>
&lt;/div>
&lt;div class="alert alert-int">
&lt;div>
&lt;p>Cuanto mayor sea $r^2$, mejor explicará el modelo de regresión la relación entre las variables, en particular:&lt;/p>
&lt;ul>
&lt;li>Si $r^2 =0$ entonces no existe relación del tipo planteado por el modelo.&lt;/li>
&lt;li>Si $r^2=1$ entonces la relación que plantea el modelo es perfecta.&lt;/li>
&lt;/ul>
&lt;/div>
&lt;/div>
&lt;div class="alert alert-warning">
&lt;div>
&lt;p>En el caso de las rectas de regresión, el coeficiente de determinación puede calcularse con esta fórmula&lt;/p>
&lt;p>$$ r^2 = \frac{s_{xy}^2}{s_x^2s_y^2}.$$&lt;/p>
&lt;/div>
&lt;/div>
&lt;div class="spoiler " >
&lt;p>
&lt;a class="btn btn-primary" data-toggle="collapse" href="#spoiler-18" role="button" aria-expanded="false" aria-controls="spoiler-18">
Demostración
&lt;/a>
&lt;/p>
&lt;div class="collapse card " id="spoiler-18">
&lt;div class="card-body">
&lt;p>Cuando el modelo ajustado es la recta de regresión la varianza residual vale&lt;/p>
&lt;p>$$
\begin{aligned}
s_{ry}^2 &amp;amp; = \sum e_{ij}^2f_{ij} = \sum (y_j - f(x_i))^2f_{ij} = \sum \left(y_j - \bar y -\frac{s_{xy}}{s_x^2}(x_i-\bar x) \right)^2f_{ij}=\newline
&amp;amp; = \sum \left((y_j - \bar y)^2 +\frac{s_{xy}^2}{s_x^4}(x_i-\bar x)^2 - 2\frac{s_{xy}}{s_x^2}(x_i-\bar x)(y_j -\bar y)\right)f_{ij} =\newline
&amp;amp; = \sum (y_j - \bar y)^2f_{ij} +\frac{s_{xy}^2}{s_x^4}\sum (x_i-\bar x)^2f_{ij}- 2\frac{s_{xy}}{s_x^2}\sum (x_i-\bar x)(y_j -\bar y)f_{ij}=\newline
&amp;amp; = s_y^2 + \frac{s_{xy}^2}{s_x^4}s_x^2 - 2 \frac{s_{xy}}{s_x^2}s_{xy} = s_y^2 - \frac{s_{xy}^2}{s_x^2}.
\end{aligned}
$$&lt;/p>
&lt;p>y, por tanto, el coeficiente de determinación lineal vale&lt;/p>
&lt;p>$$
\begin{aligned}
r^2 &amp;amp;= 1- \frac{s_{ry}^2}{s_y^2} = 1- \frac{s_y^2 - \frac{s_{xy}^2}{s_x^2}}{s_y^2} = 1 - 1 + \frac{s_{xy}^2}{s_x^2s_y^2} = \frac{s_{xy}^2}{s_x^2s_y^2}.
\end{aligned}
$$&lt;/p>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Ejemplo&lt;/strong>. En el ejemplo de las estaturas y pesos se tenía&lt;/p>
&lt;p>$$
\begin{array}{lll}
\bar x = 174.67 \mbox{ cm} &amp;amp; \quad &amp;amp; s^2_x = 102.06 \mbox{ cm}^2\newline
\bar y = 69.67 \mbox{ Kg} &amp;amp; &amp;amp; s^2_y = 164.42 \mbox{ Kg}^2\newline
s_{xy} = 104.07 \mbox{ cm$\cdot$ Kg}
\end{array}
$$&lt;/p>
&lt;p>De modo que el coeficiente de determinación lineal vale&lt;/p>
&lt;p>$$r^2 = \frac{s_{xy}^2}{s_x^2s_y^2} = \frac{(104.07 \mbox{ cm\cdot Kg})^2}{102.06 \mbox{ cm}^2 \cdot 164.42 \mbox{ Kg}^2} = 0.65.$$&lt;/p>
&lt;p>Esto indica que la recta de regresión del peso sobre la estatura explica el 65% de la variabilidad del peso, y de igual modo, la recta de regresión de la estatura sobre el peso explica el 65% de la variabilidad de la estatura.&lt;/p>
&lt;h3 id="coeficiente-de-correlación-lineal">Coeficiente de correlación lineal&lt;/h3>
&lt;div class="alert alert-def">
&lt;div>
&lt;p>&lt;strong>Definición - Coeficiente de correlación lineal muestral&lt;/strong>. Dada una variable bidimensional $(X,Y)$, el &lt;em>coeficiente de correlación lineal muestral&lt;/em> es la raíz cuadrada de su coeficiente de determinación lineal, con signo el de la covarianza&lt;/p>
&lt;p>$$r = \sqrt{r^2} = \dfrac{s_{xy}}{s_xs_y}.$$&lt;/p>
&lt;/div>
&lt;/div>
&lt;div class="alert alert-warning">
&lt;div>
&lt;p>Como $r^2$ toma valores entre 0 y 1, $r$ tomará valores entre -1 y 1,&lt;/p>
&lt;p>$$-1\leq r\leq 1$$&lt;/p>
&lt;/div>
&lt;/div>
&lt;div class="alert alert-int">
&lt;div>
&lt;p>El coeficiente de correlación lineal no sólo mide mide el grado de dependencia
lineal sino también su dirección (creciente o decreciente):&lt;/p>
&lt;ul>
&lt;li>Si $r =0$ entonces no existe relación lineal.&lt;/li>
&lt;li>Si $r=1$ entonces existe una relación lineal creciente perfecta.&lt;/li>
&lt;li>Si $r=-1$ entonces existe una relación lineal decreciente perfecta.&lt;/li>
&lt;/ul>
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Ejemplo&lt;/strong>. En el ejemplo de las estaturas y los pesos se tenía&lt;/p>
&lt;p>$$
\begin{array}{lll}
\bar x = 174.67 \mbox{ cm} &amp;amp; \quad &amp;amp; s^2_x = 102.06 \mbox{ cm}^2\newline
\bar y = 69.67 \mbox{ Kg} &amp;amp; &amp;amp; s^2_y = 164.42 \mbox{ Kg}^2\newline
s_{xy} = 104.07 \mbox{ cm$\cdot$ Kg}
\end{array}
$$&lt;/p>
&lt;p>De manera que el coeficiente de correlación lineal es&lt;/p>
&lt;p>$$r = \frac{s_{xy}}{s_xs_y} = \frac{104.07 \mbox{ cm\cdot Kg}}{10.1 \mbox{ cm} \cdot 12.82 \mbox{ Kg}} = +0.8.$$&lt;/p>
&lt;p>Esto indica que la relación lineal entre el peso y la estatura es fuerte, y además creciente.&lt;/p>
&lt;h3 id="distintos-grados-de-correlación">Distintos grados de correlación&lt;/h3>
&lt;p>Los siguientes diagramas de dispersión muestran modelos de regresión lineales con diferentes grados de correlación.&lt;/p>
&lt;img src="../img/regresion/grados_correlacion.svg" alt="Modelos de regresión lineales con diferentes grados de correlación" width="700">
&lt;h3 id="fiabilidad-de-las-predicciones-de-un-modelo-de-regresión">Fiabilidad de las predicciones de un modelo de regresión&lt;/h3>
&lt;p>Aunque el coeficiente de determinación o el de correlación determinan la bondad de ajuste de un modelo de regresión, existen otros factores que influyen en la fiabilidad de las predicciones de un modelo de regresión:&lt;/p>
&lt;ul>
&lt;li>El coeficiente de determinación: Cuanto mayor sea, menores serán los errores predictivos y mayor la fiabilidad de las predicciones.&lt;/li>
&lt;li>La variabilidad de la población: Cuanto más variable es una población, más difícil es predecir y por tanto menos fiables serán las predicciones.&lt;/li>
&lt;li>El tamaño muestral: Cuanto mayor sea, más información tendremos y, en consecuencia, más fiables serán las predicciones.&lt;/li>
&lt;/ul>
&lt;div class="alert alert-warning">
&lt;div>
Además, hay que tener en cuenta que un modelo de regresión es válido únicamente para el rango de valores observados en la muestra. Fuera de ese rango no hay información del tipo de relación entre las variables, por lo que no deben hacerse predicciones para valores lejos de los observados en la muestra.
&lt;/div>
&lt;/div>
&lt;h2 id="regresión-no-lineal">Regresión no lineal&lt;/h2>
&lt;p>El ajuste de un modelo de regresión no lineal es similar al del modelo lineal y también puede realizarse mediante la técnica de mínimos cuadrados.&lt;/p>
&lt;p>No obstante, en determinados casos un ajuste no lineal puede convertirse en un ajuste lineal mediante una sencilla transformación de alguna de las variables del modelo.&lt;/p>
&lt;h3 id="transformación-de-modelos-de-regresión-no-lineales">Transformación de modelos de regresión no lineales&lt;/h3>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>Logarítmico&lt;/strong>: Un modelo logarítmico $y = a+b \log x$ se convierte en un modelo lineal haciendo el cambio $t=\log x$:&lt;/p>
&lt;p>$$y=a+b\log x = a+bt.$$&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Exponencial&lt;/strong>: Un modelo exponencial $y = ae^{bx}$ se convierte en un modelo
lineal haciendo el cambio $z = \log y$:&lt;/p>
&lt;p>$$z = \log y = \log(ae^{bx}) = \log a + \log e^{bx} = a^\prime +bx.$$&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Potencial&lt;/strong>: Un modelo potencial $y = ax^b$ se convierte en un modelo lineal
haciendo los cambios $t=\log x$ y $z=\log y$:&lt;/p>
&lt;p>$$z = \log y = \log(ax^b) = \log a + b \log x = a^\prime+bt.$$&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Inverso&lt;/strong>: Un modelo inverso $y = a+b/x$ se convierte en un modelo lineal
haciendo el cambio $t=1/x$:&lt;/p>
&lt;p>$$y = a + b(1/x) = a+bt.$$&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Sigmoidal&lt;/strong>: Un modelo curva S $y = e^{a+b/x}$ se convierte en un modelo lineal haciendo los cambios $t=1/x$ y $z=\log y$:&lt;/p>
&lt;p>$$z = \log y = \log (e^{a+b/x}) = a+b(1/x) = a+bt.$$&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h3 id="relación-exponencial">Relación exponencial&lt;/h3>
&lt;p>&lt;strong>Ejemplo&lt;/strong> El número de bacterias de un cultivo evoluciona con el tiempo según la
siguiente tabla:&lt;/p>
&lt;p>$$\begin{array}{c|c}
\mbox{Horas} &amp;amp; \mbox{Bacterias}\newline
\hline
0 &amp;amp; 25 \newline
1 &amp;amp; 28 \newline
2 &amp;amp; 47\newline
3 &amp;amp; 65 \newline
4 &amp;amp; 86\newline
5 &amp;amp; 121\newline
6 &amp;amp; 190\newline
7 &amp;amp; 290\newline
8 &amp;amp; 362
\end{array}
$$&lt;/p>
&lt;p>El diagrama de dispersión asociado es&lt;/p>
&lt;img src="../img/regresion/evolucion_bacterias.svg" alt="Diagrama de dispersión de la evolución de bacterias" width="500">
&lt;p>Si realizamos un ajuste lineal, obtenemos la siguiente recta de regresión&lt;/p>
&lt;p>$$\mbox{Bacterias} = -30.18+41,27,\mbox{Horas, with } r^2=0.85.$$&lt;/p>
&lt;img src="../img/regresion/regresion_lineal_bacterias.svg" alt="Regresión lineal de la evolución de un cultivo de bacterias" width="500">
&lt;p>&lt;em>¿Es un buen modelo?&lt;/em>&lt;/p>
&lt;p>Aunque el modelo lineal no es malo, de acuerdo al diagrama de dispersión es más lógico construir un modelo exponencial o cuadrático.&lt;/p>
&lt;p>Para construir el modelo exponencial $y = ae^{bx}$ hay que realizar la
transformación $z=\log y$, es decir, aplicar el logaritmo a la variable dependiente.&lt;/p>
&lt;p>$$\begin{array}{c|c|c}
\mbox{Horas} &amp;amp; \mbox{Bacterias} &amp;amp; \mbox{$\log$(Bacterias)}\newline
\hline
0 &amp;amp; 25 &amp;amp; 3.22\newline
1 &amp;amp; 28 &amp;amp; 3.33\newline
2 &amp;amp; 47 &amp;amp; 3.85\newline
3 &amp;amp; 65 &amp;amp; 4.17\newline
4 &amp;amp; 86 &amp;amp; 4.45\newline
5 &amp;amp; 121 &amp;amp; 4.80\newline
6 &amp;amp; 190 &amp;amp; 5.25\newline
7 &amp;amp; 290 &amp;amp; 5.67\newline
8 &amp;amp; 362 &amp;amp; 5.89
\end{array}
$$&lt;/p>
&lt;img src="../img/regresion/evolucion_log_bacterias.svg" alt="Diagrama de dispersión de la evolución del logarítmo de las bacterias de un cultivo" width="500">
&lt;p>Ahora sólo queda calcular la recta de regresión del logaritmo de Bacterias sobre Horas&lt;/p>
&lt;p>$$\mbox{Log Bacterias} = 3.107 + 0.352, \mbox{Horas}.$$&lt;/p>
&lt;p>Y, deshaciendo el cambio de variable, se obtiene el modelo exponencial&lt;/p>
&lt;p>$$\mbox{Bacterias} = e^{3.107+0.352,\textrm{Horas}}, \mbox{ con } r^2=0.99.$$&lt;/p>
&lt;img src="../img/regresion/regresion_exponencial_bacterias.svg" alt="Regresión exponencial de la evolución de las bacterias de un cultivo" width="500">
&lt;p>Como se puede apreciar, el modelo exponencial se ajusta mucho mejor que el modelo lineal.&lt;/p>
&lt;div class="alert alert-warning">
&lt;div>
Es importante señalar que cada modelo de regresión tiene su propio coeficiente de determinación. Así, un coeficiente de determinación cercano a cero significa que no existe relación entre las variables del tipo planteado por el modelo, pero &lt;em>eso no quiere decir que las variables sean independientes&lt;/em>, ya que puede existir relación de otro tipo.
&lt;/div>
&lt;/div>
&lt;img src="../img/regresion/regresion_lineal_relacion_cuadratica.svg" alt="Modelo de regresión lineal en una relación cuadrática" width="400">
&lt;img src="../img/regresion/regresion_cuadratica.svg" alt="Modelo de regresión cuadrático en una relación cuadrática" width="400">
&lt;h2 id="datos-atípicos-en-regresión">Datos atípicos en regresión&lt;/h2>
&lt;p>Los &lt;em>datos atípicos&lt;/em> en un estudio de regresión son los puntos que claramente no siguen la tendencia del resto de los puntos en el diagrama de dispersión, incluso si los valores del par no se pueden considerar atípicos para cada variable por separado.&lt;/p>
&lt;div class="alert alert-warning">
&lt;div>
Los datos atípicos en regresión suelen provocar cambios drásticos en el ajuste de los modelos de regresión, y por tanto, habrá que tener mucho cuidado con ellos.
&lt;/div>
&lt;/div>
&lt;div class="center">
&lt;img src="../img/regresion/regresion_lineal_con_datos_atipicos.svg" alt="Modelo de regresión lineal con datos atípicos" width="400"> &lt;img src="../img/regresion/regresion_lineal_sin_datos_atipicos.svg" alt="Modelo de regresión lineal sin datos atípicos" width="400">
&lt;/div></description></item><item><title>Ejercicios de Regresión No Lineal</title><link>/docencia/estadistica/ejercicios/regresion-no-lineal/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docencia/estadistica/ejercicios/regresion-no-lineal/</guid><description>&lt;h2 id="ejercicio-1">Ejercicio 1&lt;/h2>
&lt;p>Titulación: Química, Biotecnología&lt;/p>
&lt;p>Se sometió a una persona a unas sesiones de entrenamiento para el manejo de una máquina de análisis químicos y se valoró la destreza en el manejo en diversas ocasiones, valorandola en una escala de 0 a 100.
Los resultados obtenidos aparecen en la siguiente tabla&lt;/p>
&lt;p>Sesiones | 2 | 5 | 7 | 10 | 12 | 16
Destreza | 15 | 40 | 62 | 86 | 92 | 95&lt;/p>
&lt;p>Se pide:&lt;/p>
&lt;ol>
&lt;li>Calcular la destreza alcanzada al cabo de 8 sesiones empleando el modelo logarítmico.&lt;/li>
&lt;li>Calcular el número de sesiones necesarias para alcanzar una destreza de 80 empleando el modelo exponencial.&lt;/li>
&lt;li>Justificar razonadamente cuál de las predicciones anteriores es más fiable.&lt;/li>
&lt;/ol>
&lt;p>&lt;strong>SOLUCIÓN&lt;/strong>&lt;/p>
&lt;iframe src="http://www.slideshare.net/slideshow/embed_code/35215217" width="640" height="449" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC; border-width:1px 1px 0; margin-bottom:5px; max-width: 100%;" allowfullscreen> &lt;/iframe>
&lt;iframe src="//www.youtube.com/embed/Jx8R4fTFjoE" width="640" height="360" frameborder="0"> &lt;/iframe></description></item></channel></rss>